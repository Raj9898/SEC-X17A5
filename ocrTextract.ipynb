{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.3.3)\n",
      "Collecting pip\n",
      "  Using cached pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3.3\n",
      "    Uninstalling pip-20.3.3:\n",
      "      Successfully uninstalled pip-20.3.3\n",
      "Successfully installed pip-21.0.1\n",
      "Collecting smart_open\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Collecting minecart\n",
      "  Downloading minecart-0.3.0-py3-none-any.whl (23 kB)\n",
      "Collecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109630 sha256=bf1c7e8f00c07dec226c69dd161576c122fe80edad780598528577e2bf21e5df\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/05/12/87/d479d6a8f92130cd8b27e331cc433bb28dda9c20e57f0b1ab2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: pdfminer3k, smart-open, minecart\n",
      "Successfully installed minecart-0.3.0 pdfminer3k-1.3.4 smart-open-4.2.0\n",
      "Collecting textract-trp\n",
      "  Downloading textract_trp-0.1.3-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: textract-trp\n",
      "Successfully installed textract-trp-0.1.3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install smart_open minecart\n",
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5-Subsets/\"\n",
    "\n",
    "# script to perform OCR (using Textract) for X-17A-5 subsets\n",
    "out_folder = 'Output/X-17A-5-BS/'\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Asynchronous Textract Script (requesting Job)\n",
    "**Content modified from Amazon AWS Textract repository (refer to [URL](https://github.com/aws-samples/amazon-textract-code-samples/blob/master/python/12-pdf-text.py) below)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startJob(s3BucketName:str, objectName:str) -> str:\n",
    "    \"\"\"\n",
    "    Starts a Textract job on AWS server \n",
    "    \"\"\"\n",
    "    response = None\n",
    "    client = boto3.client('textract')\n",
    "    \n",
    "    # issue response to AWS to start Textract job for table analysis \n",
    "    response = client.start_document_analysis(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3BucketName,\n",
    "                'Name': objectName\n",
    "            }\n",
    "        },\n",
    "        FeatureTypes=['FORMS', 'TABLES']    # selecting forms and tables from the OCR\n",
    "    )\n",
    "    \n",
    "    # return response job ID for service\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isJobComplete(jobId:str) -> str:\n",
    "    \"\"\"\n",
    "    Tracks the completion status of the Textract job when qued\n",
    "    \"\"\"\n",
    "    time.sleep(1)\n",
    "    client = boto3.client('textract')\n",
    "    response = client.get_document_analysis(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "    \n",
    "    # check current status of AWS job (ask server every 5 seconds for data)\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)                   # lag before reporting status\n",
    "        response = client.get_document_analysis(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobResults(jobId:str) -> list:\n",
    "    \"\"\"\n",
    "    Returns the contents of the Textract job, after completion status met\n",
    "    \"\"\"\n",
    "    pages = []          # initialize list object to track pages\n",
    "\n",
    "    client = boto3.client('textract')\n",
    "    response = client.get_document_analysis(JobId=jobId)\n",
    "    \n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    \n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "    \n",
    "    # iterate through the pages and append to response figure\n",
    "    while(nextToken):\n",
    "        response = client.get_document_analysis(JobId=jobId, NextToken=nextToken)\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runJob(bucket:str, key:str):\n",
    "    \"\"\"\n",
    "    Function designed to call an AWS Textract \n",
    "    \"\"\"\n",
    "    # S3 storage for files on AWS site   \n",
    "    jobId = startJob(bucket, key)   # intialize Textract job \n",
    "    print(\"Started job with id: {}\".format(jobId))\n",
    "\n",
    "    # if job is complete from AWS return response object \n",
    "    if(isJobComplete(jobId)):\n",
    "        response = getJobResults(jobId)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Extraction Scripts (Key-Value Pairs)\n",
    "**The content was modified from AWS to extract key-value pairs in form documents from Block objects that are stored in a map. (refer to [URL](https://docs.aws.amazon.com/textract/latest/dg/examples-extract-kvp.html))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kv_relationship(key_map, value_map, block_map):\n",
    "    \"\"\"\n",
    "    Retreaving the Key-Value relationship from FORM \n",
    "    \"\"\"\n",
    "    key_value_map = {}\n",
    "    \n",
    "    # unpack the key_map to retrieve the block id and key names\n",
    "    for block_id, key_block in key_map.items():\n",
    "        # retrieve value block given job id\n",
    "        value_block = find_value_block(key_block, value_map)\n",
    "\n",
    "        # get text value from key and value blocks\n",
    "        key = get_text(key_block, block_map)\n",
    "        val = get_text(value_block, block_map)\n",
    "        \n",
    "        # map the key and value pairs\n",
    "        key_value_map[key] = val\n",
    "        \n",
    "    return key_value_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_block(key_block, value_map):\n",
    "    \"\"\"\n",
    "    Retrieving value block from AWS textract job \n",
    "    \"\"\"\n",
    "    for relationship in key_block['Relationships']:\n",
    "        if relationship['Type'] == 'VALUE':\n",
    "            for value_id in relationship['Ids']:\n",
    "                value_block = value_map[value_id]\n",
    "    return value_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(result, blocks_map):\n",
    "    \"\"\"\n",
    "    Retrieving text values from given work\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    if 'Relationships' in result:\n",
    "        for relationship in result['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for child_id in relationship['Ids']:\n",
    "                    word = blocks_map[child_id]\n",
    "                    \n",
    "                    # if word then we append to space\n",
    "                    if word['BlockType'] == 'WORD':\n",
    "                        text += word['Text'] + ' '\n",
    "                    if word['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                        if word['SelectionStatus'] == 'SELECTED':\n",
    "                            text += 'X '    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Wrapper Functions\n",
    "**The scripts perform an OCR job from AWS Textract, and returning well formated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trp2df(table:trp.Table) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to convert a trp table into a dataframe\n",
    "    :param table: a trp table object parsed from a pdf  \n",
    "    :return: a DataFrame object housing a textracted trp table\n",
    "    \n",
    "    Complexity -> O(n^2) approx.\n",
    "    \"\"\"\n",
    "    N = len(table.rows)               # number of rows in table\n",
    "    M = len(table.rows[0].cells)      # number of columns in table\n",
    "    arr = [0]*N\n",
    "    \n",
    "    # iterate through each row within the provided table\n",
    "    for row in np.arange(N):\n",
    "        \n",
    "        # strip the text from the cell references to construct (N X M) matrix\n",
    "        arr[row] = [table.rows[row].cells[col].text.strip() for col in np.arange(M)]\n",
    "        \n",
    "    return pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTable(response:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dataframe, by searching for tables\n",
    "    :param response: AWS Textract response object\n",
    "    \"\"\"\n",
    "    # in the event multiple tables detected on one page (concat them)\n",
    "    catDF = []\n",
    "    \n",
    "    # format the Textract response type \n",
    "    doc = trp.Document(response)\n",
    "    \n",
    "    # iterate through document pages\n",
    "    for page in doc.pages:\n",
    "        \n",
    "        # itterate through page tables\n",
    "        for table in page.tables: \n",
    "            \n",
    "            # convert trp-table into dataframe object\n",
    "            df = trp2df(table)\n",
    "            \n",
    "            # remove columns that are completly empty\n",
    "            empty_cols = [col for col in df.columns if (df[col] == '').all()]\n",
    "            df = df.drop(empty_cols, axis=1)\n",
    "  \n",
    "            # number of columns in dataframe\n",
    "            n = df.columns.size\n",
    "            \n",
    "            # reset the column names (avoid the column names)\n",
    "            df.columns = np.arange(n)\n",
    "            \n",
    "            ##############################################################\n",
    "            #                           NOTES\n",
    "            #          a good dataframe should have 2-3 columns\n",
    "            #      anything more or less is a reading error we ignore\n",
    "            ##############################################################\n",
    "            \n",
    "            # if the dataframe has more than 3 columns then we most likley have an issue in parsing\n",
    "            if n > 3:\n",
    "                return None\n",
    "            \n",
    "            elif n > 1:\n",
    "                \n",
    "                ##############################\n",
    "                # Balance Sheet Assummptions\n",
    "                ##############################\n",
    "                \n",
    "                lineIndex = df.columns[0]\n",
    "\n",
    "                # check for the word \"cash\" in a string at the begining, ignoring case sensitivity (asset check)\n",
    "                assetCheck = df[lineIndex].str.contains('^Cash', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "                # check for the word \"Liabilities\" in a string at the end, ignoring case sensitivity (liability check)\n",
    "                debtCheck1 = df[lineIndex].str.contains('Liabilities$|^Liabilities', regex=True, flags=re.IGNORECASE)\n",
    "                debtCheck2 = df[lineIndex].str.contains('Liability$|^Liability', regex=True, flags=re.IGNORECASE)\n",
    "                \n",
    "                # check for the presence of $ sign, we assume the balance sheet items should have presence of $ sign\n",
    "                # used to handle table of contents error caused from prior reads\n",
    "                dollarCheck = df[df.columns[1]].str.contains('\\$[^\\]]+', regex=True, flags=re.IGNORECASE)\n",
    "                \n",
    "                ##############################\n",
    "                # Balance Sheet Determination\n",
    "                ##############################\n",
    "                \n",
    "                # check if the key words have been found \n",
    "                check1 = df[assetCheck | debtCheck1 | debtCheck2].empty      # check for terms, and $ presence\n",
    "                check2 = df[dollarCheck == True].empty                       # check for presence of '$' sign  \n",
    "                check3 = df[debtCheck1 == True].empty                        # debt check for Liabilities\n",
    "                check4 = df[debtCheck2 == True].empty                        # debt check for Liability \n",
    "                \n",
    "                # if either asset term or liability term is found, with a $ sign we append the dataframe\n",
    "                if not check1 and not check2:\n",
    "                    catDF.append(df)      # we append since sometimes asset and liablility tables are seperated \n",
    "\n",
    "                    if not check3 or not check4:\n",
    "                        # if liability table was found on the first iteration we simply concat data frames and return \n",
    "                        return pd.concat(catDF)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readForm(response:list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dictionary, by searching for key value pairs\n",
    "    :param response: AWS Textract response object\n",
    "    \"\"\"\n",
    "    # format the Textract response type \n",
    "    doc = trp.Document(response)\n",
    "    \n",
    "    # initializing \n",
    "    key_map = {}\n",
    "    value_map = {}\n",
    "    block_map = {}\n",
    "\n",
    "    # iterate through document pages\n",
    "    for page in doc.pages:\n",
    "\n",
    "        # itterate through page tables\n",
    "        for block in page.blocks: \n",
    "\n",
    "            # store the block id in map to retrive information later\n",
    "            block_id = block['Id']\n",
    "            block_map[block_id] = block\n",
    "\n",
    "            # if Key-value set has been seen \n",
    "            if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "\n",
    "                # if KEY is labeled as entity type then we found Key, else we found VALUE\n",
    "                if 'KEY' in block['EntityTypes']:\n",
    "                    key_map[block_id] = block\n",
    "                else:\n",
    "                    value_map[block_id] = block\n",
    "\n",
    "    return get_kv_relationship(key_map, value_map, block_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Balance Sheet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textractParse(path:str, index:int, csvDirectory:np.ndarray, bucket:str = \"ran-s3-systemic-risk\", \n",
    "                  out_folder:str = 'Output/X-17A-5-BS/') -> dict:\n",
    "    \"\"\"\n",
    "    Function runs a Textract job and saves Balance Sheet information to .csv file in s3 folder \n",
    "    \"\"\"\n",
    "    errors = ''\n",
    "    \n",
    "    # track the presence of json file storing information on forms\n",
    "    if os.path.exists('X17A5-forms.json'):\n",
    "        with open('X17A5-forms.json', 'r') as f: forms = json.loads(f.read())\n",
    "    else:\n",
    "        forms = {}\n",
    "    \n",
    "    # baseFile name to name export .csv file e.g. 782124-2002.csv\n",
    "    baseFile = '-'.join(path.split('/')[-1].split('-')[:2])\n",
    "    fileName = baseFile + '.csv'\n",
    "    print('\\nPerforming OCR for {}'.format(baseFile))\n",
    "\n",
    "    # if file is not found in directory we continue the iteration process\n",
    "    if (out_folder + fileName not in csvDirectory) or (baseFile not in forms):\n",
    "\n",
    "        # temporary data frame object for balance sheet information\n",
    "        res = runJob(\"ran-s3-systemic-risk\", path)\n",
    "        \n",
    "        # if Textract job did not fail we continue extraction\n",
    "        if res[0]['JobStatus'] != 'FAILED':\n",
    "            # retrieve structured data pulls from OCR\n",
    "            forms[baseFile] = readForm(res)\n",
    "            tempDF = readTable(res)\n",
    "            \n",
    "            print(tempDF)\n",
    "            \n",
    "            # checks for type of return, if none then we log an error\n",
    "            if type(tempDF) == pd.DataFrame:\n",
    "                \n",
    "                # writing data frame to .csv file\n",
    "                tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "                # save contents to AWS S3 bucket\n",
    "                with open(fileName, 'rb') as data:\n",
    "                    s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "                # remove local file after it has been created\n",
    "                os.remove(fileName)\n",
    "                \n",
    "                print('-----------------------------------------------------')\n",
    "                print('Saved {} file to s3 bucket'.format(baseFile + '.csv'))\n",
    "            else:\n",
    "                errors = 'No Balance Sheet found, or parsing error'\n",
    "        else:\n",
    "            errors = 'Could not parse, JOB FAILEDs'\n",
    "    else:\n",
    "        print('{} has been downloaded'.format(fileName))\n",
    "        \n",
    "    # storing key-value line items\n",
    "    with open('X17A5-forms.json', 'w') as f: json.dump(forms, f)\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing OCR for 58056-2012\n",
      "58056-2012.csv has been downloaded\n"
     ]
    }
   ],
   "source": [
    "# csv Directory to store balance sheet information \n",
    "csvs = np.array(session.list_s3_files(bucket, out_folder))\n",
    "\n",
    "# discover all of the pdfs that you want to parse\n",
    "paths = np.array(session.list_s3_files(bucket, data_folder))[1:]\n",
    "\n",
    "errorDict = {}\n",
    "\n",
    "# iterate through X-17A-5 subsets stored in s3 \n",
    "for i, key in enumerate(['Input/X-17A-5-Subsets/58056-2012-subset.pdf']):     \n",
    "    val = textractParse(key, i, csvs)\n",
    "    \n",
    "    if val != '':\n",
    "        errorDict[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing unique list of asset items and liability line items\n",
    "with open('textractErrors.json', 'w') as f: json.dump(errorDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
