{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.3)\n",
      "Collecting pip\n",
      "  Using cached pip-20.3.3-py2.py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3\n",
      "    Uninstalling pip-20.3:\n",
      "      Successfully uninstalled pip-20.3\n",
      "Successfully installed pip-20.3.3\n",
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
      "Building wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py): started\n",
      "  Building wheel for PyPDF2 (setup.py): finished with status 'done'\n",
      "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61084 sha256=d9633c98eb76e0d9557778a36505ac608d1d5b371c3ac9070f9056ddad6bc016\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/97/28/4b/142b7d8c98eeeb73534b9c5b6558ddd3bab3c2c8192aa7ab30\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n",
      "Collecting jupyterthemes\n",
      "  Downloading jupyterthemes-0.20.0-py2.py3-none-any.whl (7.0 MB)\n",
      "Requirement already satisfied: ipython>=5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jupyterthemes) (7.12.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jupyterthemes) (3.1.3)\n",
      "Requirement already satisfied: notebook>=5.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jupyterthemes) (6.0.3)\n",
      "Requirement already satisfied: jupyter-core in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jupyterthemes) (4.6.1)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (4.3.3)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (2.5.2)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (4.4.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (50.3.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (0.14.1)\n",
      "Requirement already satisfied: pexpect in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=5.4.1->jupyterthemes) (4.8.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.4.1->jupyterthemes) (0.5.2)\n",
      "Collecting lesscpy>=0.11.2\n",
      "  Downloading lesscpy-0.14.0-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.19.4)\n",
      "Requirement already satisfied: tornado>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (5.6.1)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.1)\n",
      "Requirement already satisfied: nbformat in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (5.0.4)\n",
      "Requirement already satisfied: Send2Trash in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (0.8.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (18.1.1)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.4)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=5.6.0->jupyterthemes) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: testpath in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.2)\n",
      "Requirement already satisfied: bleach in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.2.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.15.7)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (20.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (3.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pexpect->ipython>=5.4.1->jupyterthemes) (0.6.0)\n",
      "Installing collected packages: lesscpy, jupyterthemes\n",
      "Successfully installed jupyterthemes-0.20.0 lesscpy-0.14.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install PyPDF2\n",
    "pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# console and directory access\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# interacting with Amazon AWS\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# data reading and exporting  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# parsing SEC website for data  \n",
    "import requests\n",
    "import time \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# pdf manipulation\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5/\"\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()\n",
    "\n",
    "file_type = 'X-17A-5'       # files looking to extract\n",
    "prior2date = '20201231'     # format YYYY/MM/DD - select data prior to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Dealer Data Import\n",
    "# Parses in dealer information with accompaning CIK code for EDGAR lookup from the SEC dealer registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isfile('CIKandDealers.txt'):\n",
    "#     print('Broker dealer data has been found')\n",
    "    \n",
    "#     # exporting RegisteredDealer information, loading in JSON dictionary \n",
    "#     with open('CIKandDealers.txt') as file:\n",
    "#         cik2brokers = json.load(file)\n",
    "    \n",
    "#     # unpacking the dictionary keys (all broker dealer CIK figures)\n",
    "#     bdNames = [*cik2brokers]\n",
    "# else:\n",
    "#     print('File not found, retrieving information ...')\n",
    "#     start = time.time()\n",
    "    \n",
    "#     # will be used to concat all available broker dealer information \n",
    "#     tempDF = []\n",
    "    \n",
    "#     # send request to SEC website to retrieve broker dealer information \n",
    "#     response = requests.get('https://www.sec.gov/help/foiadocsbdfoiahtm.html', allow_redirects=True)\n",
    "\n",
    "#     # parse the HTML doc string from the response object\n",
    "#     s1Table = BeautifulSoup(response.text, 'html.parser') \n",
    "    \n",
    "#     # parse through links from the SEC filings\n",
    "#     for link in s1Table.find_all('a'):\n",
    "#         documentURL = link.get('href')           # document links for filings\n",
    "        \n",
    "#         try:\n",
    "#             # find .txt file substring greater than 0\n",
    "#             if documentURL.find('.txt') > 0:\n",
    "                \n",
    "#                 # requesting data from document links storing the files\n",
    "#                 pdf_storage = requests.get('https://www.sec.gov'+ documentURL, allow_redirects=True)\n",
    "                \n",
    "#                 # open a file to store files from SEC\n",
    "#                 open('secDealers.txt', 'wb').write(pdf_storage.content)\n",
    "                \n",
    "#                 # convert text file to .csv and store dataframe\n",
    "#                 df = pd.read_csv('secDealers.txt', sep=\"\\t\", header=None)\n",
    "                \n",
    "#                 dateRelease = documentURL.split('/')[-1][2:-4]\n",
    "#                 print('\\tSEC Dealer information has been downloaded ' + dateRelease)\n",
    "                \n",
    "#                 # append new dataframe figures to be concated\n",
    "#                 tempDF.append(df)\n",
    "                \n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "    \n",
    "#     # concat all disjoint dataframes\n",
    "#     df = pd.concat(tempDF)\n",
    "    \n",
    "#     # remove NaN columns and provide column naming conventions\n",
    "#     df = df[df.columns[:-1]]\n",
    "#     df.columns = ['CIK NUMBER', 'COMPANY NAME', 'REPORTING FILE NUMBER', 'ADDRESS1',\n",
    "#                   'ADDRESS2', 'CITY', 'STATE CODE', 'ZIP CODE']\n",
    "    \n",
    "#     # remove duplicate rows from the dataset\n",
    "#     df = df.drop_duplicates(subset='CIK NUMBER')\n",
    "#     df.to_csv('secRegisteredDealers.csv')\n",
    "\n",
    "#     # convert CIK and Company Name to dictionary\n",
    "#     cik2brokers = df[df.columns[:2]].set_index('CIK NUMBER').to_dict(orient='index')\n",
    "\n",
    "#     # unpacking the dictionary keys (all broker dealer CIK figures)\n",
    "#     bdNames = [*cik2brokers]\n",
    "\n",
    "#     # exporting RegisteredDealer information\n",
    "#     with open('CIKandDealers.txt', 'w') as file:\n",
    "#         json.dump(cik2brokers, file)\n",
    "#         file.close()\n",
    "    \n",
    "#     print('Time taken is {} seconds'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF File Extraction\n",
    "Extract URL links per company filing to download accompaning X-17A-5 files from SEC EDGAR site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J.P. MORGAN SECURITIES LLC, GOLDMAN SACHS & CO. LLC, MORGAN STANLEY & CO. LLC, CITIGROUP GLOBAL MARKETS INC.\n",
    "# NOMURA SECURITIES INTERNATIONAL, INC., WELLS FARGO SECURITIES, LLC, BARCLAYS CAPITAL INC.\n",
    "# HSBC SECURITIES (USA) INC., DEUTSCHE BANK SECURITIES INC.\n",
    "big_banks = ['782124', '42352', '68136', '91154', '72267', '1224385', '851376', '853784', '58056']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgarParse(url:str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parses the EDGAR webpage of a provided URL and returns a tuple of arrays/lists\n",
    "    \n",
    "    Input:\n",
    "        url is a string representing a SEC website URL pointing to the \n",
    "        e.g. https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1904&type=X-17A-5&dateb=20201231\n",
    "\n",
    "    Return:\n",
    "        :param: filing_dates\n",
    "            A vector of date strings for all X-17A-5 filings in chronological order\n",
    "        :param: archives\n",
    "            A vector of strings for all sec.gov URL links for each filings in chronological order\n",
    "    \"\"\"\n",
    "    \n",
    "    # requesting HTML data link from the EDGAR search results \n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "\n",
    "    # parse the HTML doc string from the response object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') \n",
    "    \n",
    "    # read in HTML tables from the url link provided \n",
    "    try:\n",
    "        filings = pd.read_html(url)[2]                 # select the filings table from EDGAR search (IndexError Flag)\n",
    "        filing_dates = filings['Filing Date'].values   # select the filing dates columns\n",
    "\n",
    "        # parse the html-doc string for all instance of < a href= > from the URL \n",
    "        href = [link.get('href') for link in soup.find_all('a')]\n",
    "\n",
    "        # search for all links with Archive in handle, these are the search links for the X-17A-5 filings\n",
    "        archives = ['https://www.sec.gov' + link for link in href if str.find(link, 'Archives') > 0]\n",
    "        \n",
    "        # return a tuple of vectors, the filings dates and the corresponding urls\n",
    "        return filing_dates, archives\n",
    "    \n",
    "    # if we can't select the filings table we flag an error\n",
    "    except IndexError:\n",
    "        print('Currently no filings are present for the firm\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePdfs(files:list) -> PdfFileWriter:\n",
    "    \"\"\"\n",
    "    Combines pdfs files iteratively by page for each of the accompanying SEC filings \n",
    "    \"\"\"\n",
    "    # initialize a pdf object to be store pdf pages\n",
    "    pdfWriter = PdfFileWriter()\n",
    "    \n",
    "    for pdf in files:\n",
    "        pdf_file = 'https://www.sec.gov' + pdf \n",
    "        \n",
    "        # request the specific pdf file from the the SEC\n",
    "        pdf_storage = requests.get(pdf_file, allow_redirects=True)\n",
    "\n",
    "        # save PDF contents to local file location \n",
    "        open('temp.pdf', 'wb').write(pdf_storage.content)\n",
    "        \n",
    "        # read pdf file as PyPDF2 object\n",
    "        pdf = PdfFileReader('temp.pdf', strict=False) \n",
    "        nPages = pdf.getNumPages()          # detemine the number of pages in pdf\n",
    "        \n",
    "        # add the pages from the document as specified \n",
    "        _ = [pdfWriter.addPage(pdf.getPage(page_num)) for page_num in np.arange(nPages)]\n",
    "    \n",
    "    os.remove('temp.pdf')\n",
    "    return pdfWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileExtract(cik2brokers:dict, bdNames:list, subFolder:str='Input/X-17A-5/', file_type:str='X-17A-5', \n",
    "                prior2date:str='20201231', bucket:str='ran-s3-systemic-risk', flag=True):\n",
    "    \"\"\"\n",
    "    Parses through the pdf links X-17A-5 pdf files to be saved in an s3 bucket\n",
    "    \n",
    "    NOTE:   This script makes no effort to weed out amended releases, rather it will default to retaining \n",
    "            information on first published releases via iterative retention \n",
    "    \"\"\"\n",
    "    \n",
    "    # discover all of the pdfs that you want to parse\n",
    "    s3_path = session.list_s3_files(bucket, subFolder)\n",
    "    \n",
    "    # initialize time for process to run (track time)\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # the URL links for each SEC company\n",
    "    for index, cik in enumerate(bdNames):\n",
    "        \n",
    "        # forming the SEC search URLs from the select CIK, file type and date range\n",
    "        secFormat = 'https://www.sec.gov/cgi-bin/browse-edgar?'     # SEC base url\n",
    "        dataSelect = 'action=getcompany&CIK={}&type={}&dateb={}'    # select params.\n",
    "\n",
    "        # build lookup URLs for the SEC level data \n",
    "        url = secFormat + dataSelect.format(cik, file_type, prior2date)\n",
    "        \n",
    "        try:\n",
    "            # return the filing dates and archived url's for each SEC company \n",
    "            filing_dates, archives = edgarParse(url)\n",
    "\n",
    "            # company name for broker dealer being downloaded (remove company name handle after readDealerData finishes)\n",
    "            companyName = cik2brokers[cik]['COMPANY NAME']\n",
    "\n",
    "            # logging info for when files are being downloaded\n",
    "            print('{} - Downloading {} files for {} - CIK ({})'.format(index, file_type, companyName, cik))\n",
    "\n",
    "            # itterate through each of the pdf URLs corresponding to archived contents\n",
    "            for i, pdf_url in enumerate(archives):\n",
    "\n",
    "                # filing year in full yyyy-MM-dd format, extracting yyyy portion \n",
    "                year = filing_dates[i][:4]\n",
    "\n",
    "                # data is organized linearly, by most recent issue first\n",
    "                # requesting data from document links storing the files\n",
    "                pdf_storage = requests.get(pdf_url, allow_redirects=True)\n",
    "\n",
    "                # table from filing detail Edgar table \n",
    "                soup = BeautifulSoup(pdf_storage.text, 'html.parser') \n",
    "\n",
    "                # extracts all link within the filing table, filtering for pdfs\n",
    "                extract_link = [file.get('href') for file in soup.find_all('a')]\n",
    "\n",
    "                # filter for all pdf links from the extracted file links  \n",
    "                pdf_files = [string for string in extract_link if str.find(string, 'pdf') > 0]\n",
    "\n",
    "                # check to see if a pdf file exists to extract, otherwise move on \n",
    "                try:\n",
    "                    # our pdf file of interst tends to be the last pdf in the list (we index -1 for last)\n",
    "                    file_name = str(cik) + '-' + year + '.pdf'\n",
    "                    pdf_name = subFolder + file_name\n",
    "                    \n",
    "                    # flag gives users the option to be efficient or thorough in their search or update of data\n",
    "                    # if flag is active and pdf_name exists in s3 bucket simply continue to other company, assume present \n",
    "                    if (pdf_name in s3_path) & (flag == True): \n",
    "                        print('\\tAll files for {} are downloaded'.format(companyName))\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        # concat all pdf files from the pdf_files list, merging all to one large pdf\n",
    "                        concatPdf = mergePdfs(pdf_files)\n",
    "                        \n",
    "                        # open file and save to local instance\n",
    "                        with open(file_name, 'wb') as f:\n",
    "                            concatPdf.write(f)\n",
    "                            f.close()\n",
    "\n",
    "                        # save contents to AWS S3 bucket\n",
    "                        with open(file_name, 'rb') as data:\n",
    "                            s3.upload_fileobj(data, bucket, pdf_name)\n",
    "\n",
    "                        # remove local file after it has been created\n",
    "                        os.remove(file_name)\n",
    "\n",
    "                        print('\\tSaved {} files for {} year {}'.format(file_type, companyName, year))\n",
    "\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "        except TypeError:\n",
    "            pass\n",
    "            \n",
    "        print('\\nTime taken for loop in minutes is {}\\n'.format((time.time() - startTime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Downloading X-17A-5 files for J.P. MORGAN SECURITIES LLC  - CIK (782124)\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2020\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2019\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2018\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2018\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2018\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2017\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2017\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2016\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2015\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2014\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2013\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2012\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2011\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2010\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2010\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2009\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2008\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2007\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2006\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2005\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2004\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2004\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2003\n",
      "\tSaved X-17A-5 files for J.P. MORGAN SECURITIES LLC  year 2002\n",
      "\n",
      "Time taken for loop in minutes is 0.20784639120101928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call function to parse data from the SEC -> port to s3\n",
    "fileExtract(cik2brokers, big_banks[:1], flag=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
