{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.0.1)\n",
      "Collecting smart_open\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Collecting minecart\n",
      "  Downloading minecart-0.3.0-py3-none-any.whl (23 kB)\n",
      "Collecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.14.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109630 sha256=3e8860ade00eb42d0b814ced07005fd0eb90bcf2b8b27f62c0051bd74b15a993\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/05/12/87/d479d6a8f92130cd8b27e331cc433bb28dda9c20e57f0b1ab2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: pdfminer3k, smart-open, minecart\n",
      "Successfully installed minecart-0.3.0 pdfminer3k-1.3.4 smart-open-4.2.0\n",
      "Collecting textract-trp\n",
      "  Downloading textract_trp-0.1.3-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: textract-trp\n",
      "Successfully installed textract-trp-0.1.3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install smart_open minecart\n",
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5-Subsets/\"\n",
    "\n",
    "# script to perform OCR (using Textract) for X-17A-5 subsets\n",
    "out_folder = 'Output/X-17A-5-BS/'\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.array(session.list_s3_files(bucket, out_folder))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We begin by first stripping away NaN terms in the first column and then mapping all the NaN terms to an empty string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .csv files are cleaned of NaN terms\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "\n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "\n",
    "    # first begin by filtering out the NaN rows present in the first column\n",
    "    filterDF = df[np.isin(df[df.columns[0]], df[df.columns[0]].dropna())]\n",
    "    filterDF = filterDF.fillna('')\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    filterDF.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('All .csv files are cleaned of NaN terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly\n",
    "    - Example releases include but are note limited to 1224385-2016, 72267-2003\n",
    "    ----\n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    cleanDF = pd.DataFrame()\n",
    "    \n",
    "    df = df.fillna('')    # fill all NaN values with empty string\n",
    "    \n",
    "    # create first column of new dataframe that corresponds with first column in prior data\n",
    "    cleanDF['0'] = df[df.columns[0]]\n",
    "\n",
    "    # we assume that the second and third columns are filled with figures\n",
    "    cleanDF['1'] = df[df.columns[1]] + df[df.columns[2]]\n",
    "    \n",
    "    return cleanDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on combining columns that are issued seperately\n",
    "s3.download_file(bucket, 'Output/X-17A-5-BS/91154-2009.csv', 'temp.pdf')\n",
    "df = pd.read_csv('temp.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "trans = [['-', np.nan]]*n\n",
    "\n",
    "for i in range(n):\n",
    "    print(df.iloc[i].iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0                  1\n",
      "0                                              Assets                   \n",
      "1                           Cash and cash equivalents          $ 502,248\n",
      "2   Cash and securities segregated pursuant to fed...            203,000\n",
      "3     Collateralized short-term financing agreements:                   \n",
      "4                                 Securities borrowed       $ 14,047,425\n",
      "5     Securities purchased under agreements to resell          1,108,355\n",
      "6                                      Trading assets  15,155,780 12,032\n",
      "7                                        Receivables:                   \n",
      "8         Brokers, dealers and clearing organizations            124,596\n",
      "9                              Interest and dividends             50,254\n",
      "10                                          Customers             13,006\n",
      "11                  Securities received as collateral            147,260\n",
      "12  Furniture, equipment and leasehold improvement...              8,385\n",
      "13                                       Other assets             50,074\n",
      "14                                       Total assets       $ 16,266,635\n",
      "15               Liabilities and stockholder's equity                   \n",
      "16                                       Liabilities:                   \n",
      "17                             Short-term borrowings:                   \n",
      "18                                  Securities loaned       $ 13,932,220\n",
      "19     Securities sold under agreements to repurchase          1,063,767\n",
      "20                                          Payables:        $14,995,987\n",
      "21                          Compensation and benefits             73,123\n",
      "22                             Interest and dividends             65,195\n",
      "23                                          Customers             39,780\n",
      "24        Brokers, dealers and clearing organizations             12,922\n",
      "25                                              Other             58,402\n",
      "26  Obligation to return securities received as co...            147,260\n",
      "27             Commitments and contingent liabilities                   \n",
      "28                            Subordinated borrowings            800,000\n",
      "29                                  Total liabilities         16,192,669\n",
      "30                              Stockholder's equity:                   \n",
      "31  Common stock, without par value, 9,000 shares ...                   \n",
      "32                5,984 shares issued and outstanding                  -\n",
      "33                         Additional paid-in capital          1,050,000\n",
      "34                                Accumulated deficit          (976,034)\n",
      "35                         Total stockholder's equity             73,966\n",
      "36         Total liabilities and stockholder's equity        $16,266,635\n",
      "We merged 72267-2008.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # if columns greater than 2, we have a weird data table\n",
    "    if df.columns.size > 2:\n",
    "        \n",
    "        # two events could occur at this point (either total splits, or year splits)\n",
    "        arr = df[df.columns[2]].values\n",
    "        \n",
    "        # check the scope of the second column \n",
    "        n = arr.size\n",
    "        k = arr.tolist().count(np.nan)\n",
    "        \n",
    "        # if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "        # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "        if k >= int(n/2):\n",
    "            tempDF = singular_merge(df)\n",
    "        else:\n",
    "            tempDF = df[df.columns[:2]]\n",
    "\n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "        \n",
    "        print('We merged {}'.format(fileName))\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes\n",
    "    - Example releases include but are note limited to 42352-2015, 58056-2009, 58056-2013, 58056-2019\n",
    "    \n",
    "    NOTE: Our objective isn't to achieve a perfect split, but rather create labels easy enough for our predictive \n",
    "    model to identify and accurately predict. This is not a perfect method and we make the assumption that a merged \n",
    "    row exists when a space exists in the value column (e.g. [19,345 2,213])\n",
    "    \"\"\"\n",
    "    \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    def find_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            arr = val.split(' ')\n",
    "            \n",
    "            # remove the $ sign if present in the list (this helps avoid false pasitives) \n",
    "            arr = list(filter(lambda x: x != '$', arr))\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(arr) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        \n",
    "        # handle exception for NaN (no attribute to split) \n",
    "        except AttributeError: return False\n",
    "    \n",
    "    def recursive_splits(splits:list, lineName:str, sub=[]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Recursively breaks up merged rows for each split until no merged row is left\n",
    "        \"\"\"\n",
    "        # if our list exceeds 1 in length, we continue to split\n",
    "        if len(splits) > 1:\n",
    "            # index where to cut the string (we assign a 66% cut-off)\n",
    "            sidx = int(len(lineName) * .66)   \n",
    "            \n",
    "            # construct a dataframe row of the first split term to append to sub list\n",
    "            row = pd.DataFrame([lineName[:sidx], splits[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we pass the +1 index splits and line name, appending the first-most layer \n",
    "            return recursive_splits(splits[1:], lineName[sidx:], sub=sub)\n",
    "        else:\n",
    "            row = pd.DataFrame([lineName, splits[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we concatenate all DataFrames vertically to form a large DataFrame \n",
    "            return pd.concat(sub)\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################    \n",
    "    \n",
    "    # select all the rows that match our description, where a space exists \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_splits(x))]\n",
    "    idxs = selections.index\n",
    "    \n",
    "    # iterate through each row that is determined to be conjoined\n",
    "    for i in idxs:\n",
    "\n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = df.loc[:i-1]\n",
    "        bottom = df.loc[i+1:]\n",
    "        \n",
    "        # divide the identified term from the selection e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        values = df[df.columns[1]].loc[i].split(' ')\n",
    "\n",
    "        # filter out the $ sign in the list e.g. [\"$\", \"9,112,943\", \"13,151,663\"] -> [9,112,943\", \"13,151,663\"]\n",
    "        values = list(filter(lambda x: x != '$', values))\n",
    "        lineName = df[df.columns[0]].loc[i]\n",
    "        \n",
    "        # determine the splits for the corresponding row\n",
    "        mid = recursive_splits(values, lineName, sub=[])\n",
    "        mid.columns = ['0', '1']\n",
    "\n",
    "        # reassign the value of df2 to update across each iteration\n",
    "        df = pd.concat([top, mid, bottom])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed the rows for 72267-2008.csv\n",
      "We fixed all conjoined tables in sample\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    tempDF = row_split(df)\n",
    "    \n",
    "    # if difference is found then \n",
    "    if tempDF.shape != df.shape:\n",
    "        print(\"Fixed the rows for {}\".format(fileName))\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We fixed all conjoined tables in sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format\n",
    "    :param: value, string value with hidden numeric quanity  \n",
    "    :return: floating point values\n",
    "    \n",
    "    Complexity -> O(n)\n",
    "    \n",
    "    e.g.\n",
    "        In[0]: $ 19,225     ->   Out[0]: 19255\n",
    "        In[0]: $ 19,225.76  ->   Out[0]: 19255.76\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    def num_strip(number):\n",
    "        numType = type(number)\n",
    "        \n",
    "        # if provided a non-empty string, perform regex operation \n",
    "        if (numType is str) and (len(number) > 0):\n",
    "            \n",
    "            # check for accounting formats that use parenthesis to signal losses \n",
    "            if number[0] == '(': number = '-' + number\n",
    "\n",
    "            # case replacing to handle poor textract reading of numbers\n",
    "            number = number.replace('I', '1').replace('l', '1')\n",
    "            \n",
    "            # --------------------------------------------------------------\n",
    "            # Explanation of the Regex Expression:\n",
    "            #      [^0-9|.|-]     = match all elements that are not numeric 0-9, periods \".\" or hyphens \"-\"\n",
    "            #      (?<!^)-        = match all elements that are hyphens \"-\" not in the first index position\n",
    "            #      \\.(?=[^.]*\\.)  = match all elements that are periods \".\" except the last instance\n",
    "            # --------------------------------------------------------------\n",
    "            \n",
    "            check1 = re.sub(\"[^0-9|.|-]\", \"\", number)\n",
    "            check2 = re.sub(\"(?<!^)-\", \"\", check1)\n",
    "            check3 = re.sub(\"\\.(?=[^.]*\\.)\", \"\", check2)\n",
    "            \n",
    "            # --------------------------------------------------------------\n",
    "            \n",
    "            # we consider weird decimal values that exceed 2 spaces to the right (e.g. 432.2884)\n",
    "            period_check = check3.find('.')\n",
    "            right_tail_length = len(check3) - period_check - 1\n",
    "            \n",
    "            # if more than 2 trailing digits to decimal point we assume incorrect placement\n",
    "            if right_tail_length > 2:\n",
    "                check3 = check3.replace('.', '')\n",
    "            \n",
    "            # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "            if (check3 == '-') or (check3 == '.'):\n",
    "                return 0.0\n",
    "            else:\n",
    "                # try to cast to floating point value, else flat NaN\n",
    "                try: \n",
    "                    return float(check3)\n",
    "                except ValueError: \n",
    "                    return np.nan\n",
    "        \n",
    "        # if operator is an integer or float then simply return the value\n",
    "        elif (numType is int) or (numType is float):\n",
    "            return number\n",
    "    \n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value\n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We converted all tables in the sample to numeric figures\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # pass numeric converter to the column \n",
    "    df[df.columns[1]] = df[df.columns[1]].apply(cleanNumeric)\n",
    "    \n",
    "    # remove NaNs values and reset index to return values\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()[['0', '1']]\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    df.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We converted all tables in the sample to numeric figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
