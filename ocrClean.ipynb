{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.0.1)\n",
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.2.0)\n",
      "Requirement already satisfied: minecart in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: pdfminer3k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.3.4)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Requirement already satisfied: textract-trp in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install smart_open minecart\n",
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5-Subsets/\"\n",
    "\n",
    "# script to perform OCR (using Textract) for X-17A-5 subsets\n",
    "out_folder = 'Output/X-17A-5-BS/'\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.array(session.list_s3_files(bucket, out_folder))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We begin by first stripping away NaN terms in the first column and then mapping all the NaN terms to an empty string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .csv files are cleaned of NaN terms\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "\n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "\n",
    "    # first begin by filtering out the NaN rows present in the first column\n",
    "    filterDF = df[np.isin(df[df.columns[0]], df[df.columns[0]].dropna())]\n",
    "    filterDF = filterDF.fillna('')\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    filterDF.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('All .csv files are cleaned of NaN terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly\n",
    "    - Example releases include but are note limited to 1224385-2016, 72267-2003\n",
    "    ----\n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    cleanDF = pd.DataFrame()\n",
    "    \n",
    "    df = df.fillna('')    # fill all NaN values with empty string\n",
    "    \n",
    "    # create first column of new dataframe that corresponds with first column in prior data\n",
    "    cleanDF['0'] = df[df.columns[0]]\n",
    "\n",
    "    # we assume that the second and third columns are filled with figures\n",
    "    cleanDF['1'] = df[df.columns[1]] + df[df.columns[2]]\n",
    "    \n",
    "    return cleanDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0                  1\n",
      "0                                              Assets                   \n",
      "1                           Cash and cash equivalents          $ 502,248\n",
      "2   Cash and securities segregated pursuant to fed...            203,000\n",
      "3     Collateralized short-term financing agreements:                   \n",
      "4                                 Securities borrowed       $ 14,047,425\n",
      "5     Securities purchased under agreements to resell          1,108,355\n",
      "6                                      Trading assets  15,155,780 12,032\n",
      "7                                        Receivables:                   \n",
      "8         Brokers, dealers and clearing organizations            124,596\n",
      "9                              Interest and dividends             50,254\n",
      "10                                          Customers             13,006\n",
      "11                  Securities received as collateral            147,260\n",
      "12  Furniture, equipment and leasehold improvement...              8,385\n",
      "13                                       Other assets             50,074\n",
      "14                                       Total assets       $ 16,266,635\n",
      "15               Liabilities and stockholder's equity                   \n",
      "16                                       Liabilities:                   \n",
      "17                             Short-term borrowings:                   \n",
      "18                                  Securities loaned       $ 13,932,220\n",
      "19     Securities sold under agreements to repurchase          1,063,767\n",
      "20                                          Payables:        $14,995,987\n",
      "21                          Compensation and benefits             73,123\n",
      "22                             Interest and dividends             65,195\n",
      "23                                          Customers             39,780\n",
      "24        Brokers, dealers and clearing organizations             12,922\n",
      "25                                              Other             58,402\n",
      "26  Obligation to return securities received as co...            147,260\n",
      "27             Commitments and contingent liabilities                   \n",
      "28                            Subordinated borrowings            800,000\n",
      "29                                  Total liabilities         16,192,669\n",
      "30                              Stockholder's equity:                   \n",
      "31  Common stock, without par value, 9,000 shares ...                   \n",
      "32                5,984 shares issued and outstanding                  -\n",
      "33                         Additional paid-in capital          1,050,000\n",
      "34                                Accumulated deficit          (976,034)\n",
      "35                         Total stockholder's equity             73,966\n",
      "36         Total liabilities and stockholder's equity        $16,266,635\n",
      "We merged 72267-2008.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # if columns greater than 2, we have a weird data table\n",
    "    if df.columns.size > 2:\n",
    "        \n",
    "        # two events could occur at this point (either total splits, or year splits)\n",
    "        arr = df[df.columns[2]].values\n",
    "        \n",
    "        # check the scope of the second column \n",
    "        n = arr.size\n",
    "        k = arr.tolist().count(np.nan)\n",
    "        \n",
    "        # if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "        # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "        if k >= int(n/2):\n",
    "            tempDF = singular_merge(df)\n",
    "        else:\n",
    "            tempDF = df[df.columns[:2]]\n",
    "\n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "        \n",
    "        print('We merged {}'.format(fileName))\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes\n",
    "    - Example releases include but are note limited to 42352-2015, 58056-2009, 58056-2013\n",
    "    \n",
    "    NOTE: Our objective isn't to achieve a perfect split, but rather create labels easy enough for our predictive \n",
    "    model to identify and accurately predict. This is not a perfect method and we make assumptions as to the data \n",
    "    \"\"\"\n",
    "    \n",
    "    def find_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            x = val.split(' ')\n",
    "            \n",
    "            # remove the $ sign if present in the list (this helps avoid false pasitives) \n",
    "            try:\n",
    "                x.remove('$')\n",
    "            except ValueError:\n",
    "                # if no $ found we just ignore\n",
    "                pass\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(x) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        except AttributeError: return False\n",
    "    \n",
    "    # select all the rows that match our description, where a space exists \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_splits(x))]\n",
    "    idxs = selections.index\n",
    "\n",
    "    # initialize the reporting dataframe\n",
    "    temp_df = df\n",
    "\n",
    "    for i in idxs:\n",
    "\n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = temp_df.loc[:i-1]\n",
    "        bottom = temp_df.loc[i+1:]\n",
    "\n",
    "        # divide the identified term from the selection piece\n",
    "        # e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        values = temp_df[temp_df.columns[1]].loc[i].split(' ')\n",
    "\n",
    "        # remove the $ sign if present in the list, otherwise pass \n",
    "        try: values.remove('$')\n",
    "        except ValueError: pass\n",
    "        \n",
    "        lineName = temp_df[temp_df.columns[0]].loc[i]\n",
    "        split = int(len(lineName) * .66)   # index where to cut the string (we assign a 66% cut-off)\n",
    "\n",
    "        # forming dataframe from dictionary, we then re-map columns and index values (these are new rows)\n",
    "        # zip restricts bounds to left most split (so always two rows are returned)\n",
    "        # e.g. dict(zip(['A', 'B'], [1, 2, 3, 4])) -> {'A': 1, 'B': 2}\n",
    "        mid = pd.DataFrame.from_dict(dict(zip([lineName[:split], lineName[split:]], values)), \n",
    "                                     orient='index').reset_index()\n",
    "        mid.columns = ['0', '1']\n",
    "        mid.index = [0, 0]\n",
    "\n",
    "        # reassign the value of df2 to update across each iteration\n",
    "        temp_df = pd.concat([top, mid, bottom])\n",
    "        \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed the rows for 72267-2008.csv\n",
      "We fixed all conjoined tables in sample\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    tempDF = row_split(df)\n",
    "    \n",
    "    # if difference is found then \n",
    "    if tempDF.shape != df.shape:\n",
    "        print(\"Fixed the rows for {}\".format(fileName))\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We fixed all conjoined tables in sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format\n",
    "    :param: value, string value with hidden numeric quanity  \n",
    "    :return: floating point values\n",
    "    \n",
    "    Complexity -> O(n)\n",
    "    \n",
    "    e.g.\n",
    "        In[0]: $ 19,225     ->   Out[0]: 19255\n",
    "        In[0]: $ 19,225.76  ->   Out[0]: 19255.76\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # ##############################################################\n",
    "    def num_strip(number):\n",
    "        \"\"\"\n",
    "        Nested function for extracting numerical quantities\n",
    "        \"\"\"\n",
    "        numType = type(number)\n",
    "        \n",
    "        # if provided a string, perform regex operation \n",
    "        if (numType is str) and (len(number) > 0):\n",
    "            \n",
    "            # some accounting formats take () to be negative numbers\n",
    "            if number[0] == '(':\n",
    "                number = '-' + number\n",
    "\n",
    "            # perform regex operation scanning for only numeric quantities/identifiers\n",
    "            cleanValue = re.sub(\"[^0-9|.|-]\", \"\", number)\n",
    "\n",
    "            # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "            if (cleanValue == '-') or (cleanValue == '.'):\n",
    "                return 0.0\n",
    "            else:\n",
    "                # try to convert the stripped value, otherwise return NaN\n",
    "                try: \n",
    "                    return float(cleanValue)\n",
    "                except ValueError: \n",
    "                    return np.nan\n",
    "        \n",
    "        # if operator is integer then simply return the value, no need to modify \n",
    "        elif (numType is int) or (numType is float):\n",
    "            return number\n",
    "    \n",
    "        else:\n",
    "            return np.nan\n",
    "    # ##############################################################\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value\n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We converted all tables in the sample to numeric figures\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # pass numeric converter to the column \n",
    "    df[df.columns[1]] = df[df.columns[1]].apply(cleanNumeric)\n",
    "    \n",
    "    # remove NaNs values and reset index to return values\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()[['0', '1']]\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    df.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We converted all tables in the sample to numeric figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
