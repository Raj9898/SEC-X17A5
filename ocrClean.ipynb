{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.3.3)\n",
      "Collecting pip\n",
      "  Using cached pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3.3\n",
      "    Uninstalling pip-20.3.3:\n",
      "      Successfully uninstalled pip-20.3.3\n",
      "Successfully installed pip-21.0.1\n",
      "Collecting smart_open\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Collecting minecart\n",
      "  Downloading minecart-0.3.0-py3-none-any.whl (23 kB)\n",
      "Collecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109630 sha256=bf55b8cdfac604fbfd77ead72e826b1a391cb2773bbb43358109ca9142678c84\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/05/12/87/d479d6a8f92130cd8b27e331cc433bb28dda9c20e57f0b1ab2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: pdfminer3k, smart-open, minecart\n",
      "Successfully installed minecart-0.3.0 pdfminer3k-1.3.4 smart-open-4.2.0\n",
      "Collecting textract-trp\n",
      "  Downloading textract_trp-0.1.3-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: textract-trp\n",
      "Successfully installed textract-trp-0.1.3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install smart_open minecart\n",
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5-Subsets/\"\n",
    "\n",
    "# script to perform OCR (using Textract) for X-17A-5 subsets\n",
    "out_folder = 'Output/X-17A-5-BS/'\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.array(session.list_s3_files(bucket, out_folder))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We begin by first stripping away NaN terms in the first column and then mapping all the NaN terms to an empty string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .csv files are cleaned of NaN terms\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "\n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "\n",
    "    # first begin by filtering out the NaN rows present in the first column\n",
    "    filterDF = df[np.isin(df[df.columns[0]], df[df.columns[0]].dropna())]\n",
    "    filterDF = filterDF.fillna('')\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    filterDF.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('All .csv files are cleaned of NaN terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly\n",
    "    - Example releases include but are note limited to 1224385-2016, 72267-2003\n",
    "    ----\n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    cleanDF = pd.DataFrame()\n",
    "    \n",
    "    df = df.fillna('')    # fill all NaN values with empty string\n",
    "    \n",
    "    # create first column of new dataframe that corresponds with first column in prior data\n",
    "    cleanDF['0'] = df[df.columns[0]]\n",
    "\n",
    "    # we assume that the second and third columns are filled with figures\n",
    "    cleanDF['1'] = df[df.columns[1]] + df[df.columns[2]]\n",
    "    \n",
    "    return cleanDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # if columns greater than 2, we have a weird data table\n",
    "    if df.columns.size > 2:\n",
    "        \n",
    "        # two events could occur at this point (either total splits, or year splits)\n",
    "        arr = df[df.columns[2]].values\n",
    "        \n",
    "        # check the scope of the second column \n",
    "        n = arr.size\n",
    "        k = arr.tolist().count(np.nan)\n",
    "        \n",
    "        # if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "        # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "        if k >= int(n/2):\n",
    "            tempDF = singular_merge(df)\n",
    "        else:\n",
    "            tempDF = df[df.columns[:2]]\n",
    "\n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "        \n",
    "        print('We merged {}'.format(fileName))\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes\n",
    "    - Example releases include but are note limited to 42352-2015, 58056-2009, 58056-2013\n",
    "    \n",
    "    NOTE: Our objective isn't to achieve a perfect split, but rather create labels easy enough for our predictive \n",
    "    model to identify and accurately predict. This is not a perfect method and we make assumptions as to the data \n",
    "    \"\"\"\n",
    "    \n",
    "    def find_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            x = val.split(' ')\n",
    "            \n",
    "            # remove the $ sign if present in the list (this helps avoid false pasitives) \n",
    "            try:\n",
    "                x.remove('$')\n",
    "            except ValueError:\n",
    "                # if no $ found we just ignore\n",
    "                pass\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(x) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        except AttributeError: return False\n",
    "    \n",
    "    # select all the rows that match our description, where a space exists \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_splits(x))]\n",
    "    idxs = selections.index\n",
    "\n",
    "    # initialize the reporting dataframe\n",
    "    temp_df = df\n",
    "\n",
    "    for i in idxs:\n",
    "\n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = temp_df.loc[:i-1]\n",
    "        bottom = temp_df.loc[i+1:]\n",
    "\n",
    "        # divide the identified term from the selection piece\n",
    "        # e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        values = temp_df[temp_df.columns[1]].loc[i].split(' ')\n",
    "\n",
    "        # remove the $ sign if present in the list, otherwise pass \n",
    "        try: values.remove('$')\n",
    "        except ValueError: pass\n",
    "        \n",
    "        lineName = temp_df[temp_df.columns[0]].loc[i]\n",
    "        split = int(len(lineName) * .66)   # index where to cut the string (we assign a 66% cut-off)\n",
    "\n",
    "        # forming dataframe from dictionary, we then re-map columns and index values (these are new rows)\n",
    "        # zip restricts bounds to left most split (so always two rows are returned)\n",
    "        # e.g. dict(zip(['A', 'B'], [1, 2, 3, 4])) -> {'A': 1, 'B': 2}\n",
    "        mid = pd.DataFrame.from_dict(dict(zip([lineName[:split], lineName[split:]], values)), \n",
    "                                     orient='index').reset_index()\n",
    "        mid.columns = ['0', '1']\n",
    "        mid.index = [0, 0]\n",
    "\n",
    "        # reassign the value of df2 to update across each iteration\n",
    "        temp_df = pd.concat([top, mid, bottom])\n",
    "        \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed the rows for 42352-2007.csv\n",
      "Fixed the rows for 42352-2015.csv\n",
      "Fixed the rows for 42352-2016.csv\n",
      "Fixed the rows for 42352-2017.csv\n",
      "Fixed the rows for 42352-2018.csv\n",
      "Fixed the rows for 42352-2019.csv\n",
      "Fixed the rows for 58056-2009.csv\n",
      "Fixed the rows for 58056-2010.csv\n",
      "Fixed the rows for 58056-2012.csv\n",
      "Fixed the rows for 58056-2013.csv\n",
      "Fixed the rows for 58056-2015.csv\n",
      "Fixed the rows for 58056-2016.csv\n",
      "Fixed the rows for 58056-2018.csv\n",
      "Fixed the rows for 58056-2019.csv\n",
      "Fixed the rows for 58056-2020.csv\n",
      "Fixed the rows for 68136-2002.csv\n",
      "Fixed the rows for 68136-2004.csv\n",
      "Fixed the rows for 68136-2005.csv\n",
      "Fixed the rows for 68136-2008.csv\n",
      "Fixed the rows for 68136-2009.csv\n",
      "Fixed the rows for 72267-2008.csv\n",
      "Fixed the rows for 72267-2009.csv\n",
      "Fixed the rows for 72267-2010.csv\n",
      "Fixed the rows for 72267-2012.csv\n",
      "Fixed the rows for 72267-2014.csv\n",
      "Fixed the rows for 72267-2015.csv\n",
      "Fixed the rows for 72267-2016.csv\n",
      "Fixed the rows for 72267-2017.csv\n",
      "Fixed the rows for 72267-2018.csv\n",
      "Fixed the rows for 72267-2019.csv\n",
      "Fixed the rows for 72267-2020.csv\n",
      "Fixed the rows for 782124-2002.csv\n",
      "Fixed the rows for 782124-2008.csv\n",
      "Fixed the rows for 782124-2011.csv\n",
      "Fixed the rows for 782124-2013.csv\n",
      "Fixed the rows for 851376-2002.csv\n",
      "Fixed the rows for 851376-2014.csv\n",
      "Fixed the rows for 851376-2015.csv\n",
      "Fixed the rows for 851376-2017.csv\n",
      "Fixed the rows for 853784-2018.csv\n",
      "Fixed the rows for 853784-2019.csv\n",
      "Fixed the rows for 853784-2020.csv\n",
      "Fixed the rows for 91154-2003.csv\n",
      "Fixed the rows for 91154-2005.csv\n",
      "Fixed the rows for 91154-2012.csv\n",
      "Fixed the rows for 91154-2013.csv\n",
      "Fixed the rows for 91154-2019.csv\n",
      "\n",
      "We fixed all conjoined tables in sample\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    tempDF = row_split(df)\n",
    "    \n",
    "    # if difference is found then \n",
    "    if tempDF.shape != df.shape:\n",
    "        print(\"Fixed the rows for {}\".format(fileName))\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('\\nWe fixed all conjoined tables in sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format\n",
    "    :param: value, string value with hidden numeric quanity  \n",
    "    :return: floating point values\n",
    "    \n",
    "    Complexity -> O(n)\n",
    "    \n",
    "    e.g.\n",
    "        In[0]: $ 19,225     ->   Out[0]: 19255\n",
    "        In[0]: $ 19,225.76  ->   Out[0]: 19255.76\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # ##############################################################\n",
    "    def num_strip(number):\n",
    "        \"\"\"\n",
    "        Nested function for extracting numerical quantities\n",
    "        \"\"\"\n",
    "        numType = type(number)\n",
    "        \n",
    "        # if provided a string, perform regex operation \n",
    "        if (numType is str) and (len(number) > 0):\n",
    "            \n",
    "            # some accounting formats take () to be negative numbers\n",
    "            if number[0] == '(':\n",
    "                number = '-' + number\n",
    "\n",
    "            # perform regex operation scanning for only numeric quantities/identifiers\n",
    "            cleanValue = re.sub(\"[^0-9|.|-]\", \"\", number)\n",
    "\n",
    "            # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "            if (cleanValue == '-') or (cleanValue == '.'):\n",
    "                return 0.0\n",
    "            else:\n",
    "                # try to convert the stripped value, otherwise return NaN\n",
    "                try: \n",
    "                    return float(cleanValue)\n",
    "                except ValueError: \n",
    "                    return np.nan\n",
    "        \n",
    "        # if operator is integer then simply return the value, no need to modify \n",
    "        elif (numType is int) or (numType is float):\n",
    "            return number\n",
    "    \n",
    "        else:\n",
    "            return np.nan\n",
    "    # ##############################################################\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value\n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We converted all tables in the sample to numeric figures\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # pass numeric converter to the column \n",
    "    df[df.columns[1]] = df[df.columns[1]].apply(cleanNumeric)\n",
    "    \n",
    "    # remove NaNs values and reset index to return values\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()[['0', '1']]\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    df.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We converted all tables in the sample to numeric figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting term matching\n",
    "**Check to see if we report totals, this figure is not need with the exception of the total asset field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totals_check(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks to see if a line row meets the conditon of a total, if true we remove these rows as we make \n",
    "    have checked the terms before have meet our conditions (these include major and minor totals)\n",
    "    \"\"\"\n",
    "    m, n = df.shape\n",
    "    \n",
    "    for i in range(m):\n",
    "        # check the value of at a given index (forward index)\n",
    "        item1 = df.loc[i].values[1]\n",
    "\n",
    "        # compute backward sum (lookback index) \n",
    "        for j in range(i):\n",
    "            # backward sum (index minus j-periods before)\n",
    "            item2 = df.loc[i-j-1:i-1]['1'].sum()\n",
    "\n",
    "            # if we achieve this then we strip totals and break, no need to continue backward sum\n",
    "            if item1 == item2:\n",
    "                df = df.drop(index=i)\n",
    "                break     # help avoid key error flag\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # run an accounting check for numeric figures\n",
    "    tempDF = totals_check(df)\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key='Output/X-17A-5-Clean/' + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We checked for accounting identity and removed accordingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
