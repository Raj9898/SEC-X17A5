{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "asset_folder = \"Output/X-17A-5-Clean/Assets/\"\n",
    "liable_folder = \"Output/X-17A-5-Clean/Liability & Equity/\"\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting term matching\n",
    "**Check to see if we report totals, this figure is not need with the exception of the total asset field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totals_check(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks to see if a line row meets the conditon of a total, if true we remove these rows as we make \n",
    "    have checked the terms before have meet our conditions (these include major and minor totals)\n",
    "    \"\"\"\n",
    "    m, n = df.shape\n",
    "    \n",
    "    # numpy exception for handling invalid log10 RunTime error (do not show)\n",
    "    # switch ignore to warn, if warning is made to be present \n",
    "    np.seterr(invalid = 'ignore') \n",
    "    \n",
    "    def multiple_check(x1:float, x2:float):\n",
    "        \"\"\"\n",
    "        Determine whether the two values are the same number scaled by 10\n",
    "        \"\"\"\n",
    "        # prevent zero division error\n",
    "        if (x1 == 0): return False\n",
    "        else:\n",
    "            # if number is a multiple of 10 we return True (e.g. Total Assets 745.2322 vs Backward Sum 7452322)\n",
    "            check1 = np.log10(x2 / x1).is_integer()\n",
    "            \n",
    "            # if number total is a substring we return True (e.g. Total Assets 174182935 vs Bacward Sum 74182935)\n",
    "            check2 = (str(x2) in str(x1) ) & (len(str(x2)) == len(str(x1)) - 1)\n",
    "            \n",
    "            if check1 or check2: \n",
    "                return True\n",
    "            else: return False\n",
    "    \n",
    "    for i in range(m):\n",
    "        # check the value of at a given index (forward index)\n",
    "        item1 = df.loc[i].values[1]\n",
    "\n",
    "        # compute backward sum (lookback index) \n",
    "        for j in range(i):\n",
    "            # backward sum (index minus j-periods before)\n",
    "            item2 = df.loc[i-j-1:i-1]['1'].sum()\n",
    "\n",
    "            # if we achieve this then we strip totals and break, no need to continue backward sum\n",
    "            if (item1 == item2) or multiple_check(item1, item2):\n",
    "                df = df.drop(index=i)\n",
    "                break     # help avoid key error flag\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accounting_fix(in_folder:str, out_folder:str):\n",
    "    \"\"\"\n",
    "    Wrapper function for executing the parse\n",
    "    \"\"\"\n",
    "    paths = session.list_s3_files(bucket, in_folder)[1:]\n",
    "    \n",
    "    for csv in paths:\n",
    "        fileName = csv.split('/')[-1]\n",
    "        \n",
    "        # work on combining columns that are issued seperately\n",
    "        s3.download_file(bucket, csv, 'temp.pdf')\n",
    "        df = pd.read_csv('temp.pdf')\n",
    "\n",
    "        # run an accounting check for numeric figures\n",
    "        tempDF = totals_check(df)\n",
    "\n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove('temp.pdf')\n",
    "        print('Checked accounting identity for {}'.format(fileName))\n",
    "        \n",
    "    print('\\nOur check has been cleared and removed line items accordingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked accounting identity for 1224385-2004.csv\n",
      "Checked accounting identity for 1224385-2005.csv\n",
      "Checked accounting identity for 1224385-2006.csv\n",
      "Checked accounting identity for 1224385-2007.csv\n",
      "Checked accounting identity for 1224385-2008.csv\n",
      "Checked accounting identity for 1224385-2009.csv\n",
      "Checked accounting identity for 1224385-2010.csv\n",
      "Checked accounting identity for 1224385-2011.csv\n",
      "Checked accounting identity for 1224385-2012.csv\n",
      "Checked accounting identity for 1224385-2013.csv\n",
      "Checked accounting identity for 1224385-2014.csv\n",
      "Checked accounting identity for 1224385-2015.csv\n",
      "Checked accounting identity for 1224385-2016.csv\n",
      "Checked accounting identity for 1224385-2017.csv\n",
      "Checked accounting identity for 1224385-2018.csv\n",
      "Checked accounting identity for 1224385-2019.csv\n",
      "Checked accounting identity for 1224385-2020.csv\n",
      "Checked accounting identity for 42352-2002.csv\n",
      "Checked accounting identity for 42352-2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked accounting identity for 42352-2004.csv\n",
      "Checked accounting identity for 42352-2005.csv\n",
      "Checked accounting identity for 42352-2006.csv\n",
      "Checked accounting identity for 42352-2007.csv\n",
      "Checked accounting identity for 42352-2008.csv\n",
      "Checked accounting identity for 42352-2010.csv\n",
      "Checked accounting identity for 42352-2013.csv\n",
      "Checked accounting identity for 42352-2014.csv\n",
      "Checked accounting identity for 42352-2015.csv\n",
      "Checked accounting identity for 42352-2016.csv\n",
      "Checked accounting identity for 42352-2017.csv\n",
      "Checked accounting identity for 42352-2018.csv\n",
      "Checked accounting identity for 42352-2019.csv\n",
      "Checked accounting identity for 58056-2002.csv\n",
      "Checked accounting identity for 58056-2003.csv\n",
      "Checked accounting identity for 58056-2004.csv\n",
      "Checked accounting identity for 58056-2005.csv\n",
      "Checked accounting identity for 58056-2006.csv\n",
      "Checked accounting identity for 58056-2007.csv\n",
      "Checked accounting identity for 58056-2008.csv\n",
      "Checked accounting identity for 58056-2009.csv\n",
      "Checked accounting identity for 58056-2010.csv\n",
      "Checked accounting identity for 58056-2012.csv\n",
      "Checked accounting identity for 58056-2013.csv\n",
      "Checked accounting identity for 58056-2014.csv\n",
      "Checked accounting identity for 58056-2015.csv\n",
      "Checked accounting identity for 58056-2016.csv\n",
      "Checked accounting identity for 58056-2017.csv\n",
      "Checked accounting identity for 58056-2018.csv\n",
      "Checked accounting identity for 58056-2019.csv\n",
      "Checked accounting identity for 58056-2020.csv\n",
      "Checked accounting identity for 68136-2002.csv\n",
      "Checked accounting identity for 68136-2003.csv\n",
      "Checked accounting identity for 68136-2004.csv\n",
      "Checked accounting identity for 68136-2005.csv\n",
      "Checked accounting identity for 68136-2006.csv\n",
      "Checked accounting identity for 68136-2007.csv\n",
      "Checked accounting identity for 68136-2009.csv\n",
      "Checked accounting identity for 68136-2010.csv\n",
      "Checked accounting identity for 68136-2011.csv\n",
      "Checked accounting identity for 68136-2012.csv\n",
      "Checked accounting identity for 68136-2013.csv\n",
      "Checked accounting identity for 68136-2014.csv\n",
      "Checked accounting identity for 68136-2015.csv\n",
      "Checked accounting identity for 68136-2016.csv\n",
      "Checked accounting identity for 68136-2017.csv\n",
      "Checked accounting identity for 68136-2018.csv\n",
      "Checked accounting identity for 68136-2019.csv\n",
      "Checked accounting identity for 72267-2003.csv\n",
      "Checked accounting identity for 72267-2004.csv\n",
      "Checked accounting identity for 72267-2005.csv\n",
      "Checked accounting identity for 72267-2006.csv\n",
      "Checked accounting identity for 72267-2007.csv\n",
      "Checked accounting identity for 72267-2008.csv\n",
      "Checked accounting identity for 72267-2009.csv\n",
      "Checked accounting identity for 72267-2010.csv\n",
      "Checked accounting identity for 72267-2012.csv\n",
      "Checked accounting identity for 72267-2013.csv\n",
      "Checked accounting identity for 72267-2014.csv\n",
      "Checked accounting identity for 72267-2015.csv\n",
      "Checked accounting identity for 72267-2016.csv\n",
      "Checked accounting identity for 72267-2017.csv\n",
      "Checked accounting identity for 72267-2018.csv\n",
      "Checked accounting identity for 72267-2019.csv\n",
      "Checked accounting identity for 72267-2020.csv\n",
      "Checked accounting identity for 782124-2002.csv\n",
      "Checked accounting identity for 782124-2003.csv\n",
      "Checked accounting identity for 782124-2004.csv\n",
      "Checked accounting identity for 782124-2005.csv\n",
      "Checked accounting identity for 782124-2006.csv\n",
      "Checked accounting identity for 782124-2007.csv\n",
      "Checked accounting identity for 782124-2008.csv\n",
      "Checked accounting identity for 782124-2009.csv\n",
      "Checked accounting identity for 782124-2010.csv\n",
      "Checked accounting identity for 782124-2012.csv\n",
      "Checked accounting identity for 782124-2013.csv\n",
      "Checked accounting identity for 782124-2014.csv\n",
      "Checked accounting identity for 782124-2015.csv\n",
      "Checked accounting identity for 782124-2016.csv\n",
      "Checked accounting identity for 782124-2017.csv\n",
      "Checked accounting identity for 782124-2018.csv\n",
      "Checked accounting identity for 782124-2019.csv\n",
      "Checked accounting identity for 782124-2020.csv\n",
      "Checked accounting identity for 851376-2002.csv\n",
      "Checked accounting identity for 851376-2003.csv\n",
      "Checked accounting identity for 851376-2004.csv\n",
      "Checked accounting identity for 851376-2005.csv\n",
      "Checked accounting identity for 851376-2006.csv\n",
      "Checked accounting identity for 851376-2008.csv\n",
      "Checked accounting identity for 851376-2009.csv\n",
      "Checked accounting identity for 851376-2010.csv\n",
      "Checked accounting identity for 851376-2011.csv\n",
      "Checked accounting identity for 851376-2012.csv\n",
      "Checked accounting identity for 851376-2013.csv\n",
      "Checked accounting identity for 851376-2014.csv\n",
      "Checked accounting identity for 851376-2015.csv\n",
      "Checked accounting identity for 851376-2016.csv\n",
      "Checked accounting identity for 851376-2017.csv\n",
      "Checked accounting identity for 851376-2018.csv\n",
      "Checked accounting identity for 851376-2019.csv\n",
      "Checked accounting identity for 851376-2020.csv\n",
      "Checked accounting identity for 853784-2002.csv\n",
      "Checked accounting identity for 853784-2003.csv\n",
      "Checked accounting identity for 853784-2004.csv\n",
      "Checked accounting identity for 853784-2005.csv\n",
      "Checked accounting identity for 853784-2006.csv\n",
      "Checked accounting identity for 853784-2007.csv\n",
      "Checked accounting identity for 853784-2008.csv\n",
      "Checked accounting identity for 853784-2009.csv\n",
      "Checked accounting identity for 853784-2010.csv\n",
      "Checked accounting identity for 853784-2011.csv\n",
      "Checked accounting identity for 853784-2012.csv\n",
      "Checked accounting identity for 853784-2013.csv\n",
      "Checked accounting identity for 853784-2014.csv\n",
      "Checked accounting identity for 853784-2015.csv\n",
      "Checked accounting identity for 853784-2016.csv\n",
      "Checked accounting identity for 853784-2017.csv\n",
      "Checked accounting identity for 853784-2018.csv\n",
      "Checked accounting identity for 853784-2019.csv\n",
      "Checked accounting identity for 853784-2020.csv\n",
      "Checked accounting identity for 91154-2002.csv\n",
      "Checked accounting identity for 91154-2003.csv\n",
      "Checked accounting identity for 91154-2004.csv\n",
      "Checked accounting identity for 91154-2005.csv\n",
      "Checked accounting identity for 91154-2007.csv\n",
      "Checked accounting identity for 91154-2008.csv\n",
      "Checked accounting identity for 91154-2009.csv\n",
      "Checked accounting identity for 91154-2010.csv\n",
      "Checked accounting identity for 91154-2011.csv\n",
      "Checked accounting identity for 91154-2012.csv\n",
      "Checked accounting identity for 91154-2013.csv\n",
      "Checked accounting identity for 91154-2014.csv\n",
      "Checked accounting identity for 91154-2016.csv\n",
      "Checked accounting identity for 91154-2017.csv\n",
      "Checked accounting identity for 91154-2018.csv\n",
      "Checked accounting identity for 91154-2019.csv\n",
      "Checked accounting identity for 91154-2020.csv\n",
      "\n",
      "Our check has been cleared and removed line items accordingly\n"
     ]
    }
   ],
   "source": [
    "accounting_fix('Output/X-17A-5-Split/Assets/', asset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked accounting identity for 1224385-2004.csv\n",
      "Checked accounting identity for 1224385-2005.csv\n",
      "Checked accounting identity for 1224385-2006.csv\n",
      "Checked accounting identity for 1224385-2007.csv\n",
      "Checked accounting identity for 1224385-2008.csv\n",
      "Checked accounting identity for 1224385-2009.csv\n",
      "Checked accounting identity for 1224385-2010.csv\n",
      "Checked accounting identity for 1224385-2011.csv\n",
      "Checked accounting identity for 1224385-2012.csv\n",
      "Checked accounting identity for 1224385-2013.csv\n",
      "Checked accounting identity for 1224385-2014.csv\n",
      "Checked accounting identity for 1224385-2015.csv\n",
      "Checked accounting identity for 1224385-2016.csv\n",
      "Checked accounting identity for 1224385-2017.csv\n",
      "Checked accounting identity for 1224385-2018.csv\n",
      "Checked accounting identity for 1224385-2019.csv\n",
      "Checked accounting identity for 1224385-2020.csv\n",
      "Checked accounting identity for 42352-2002.csv\n",
      "Checked accounting identity for 42352-2003.csv\n",
      "Checked accounting identity for 42352-2004.csv\n",
      "Checked accounting identity for 42352-2005.csv\n",
      "Checked accounting identity for 42352-2006.csv\n",
      "Checked accounting identity for 42352-2007.csv\n",
      "Checked accounting identity for 42352-2008.csv\n",
      "Checked accounting identity for 42352-2010.csv\n",
      "Checked accounting identity for 42352-2013.csv\n",
      "Checked accounting identity for 42352-2014.csv\n",
      "Checked accounting identity for 42352-2015.csv\n",
      "Checked accounting identity for 42352-2016.csv\n",
      "Checked accounting identity for 42352-2017.csv\n",
      "Checked accounting identity for 42352-2018.csv\n",
      "Checked accounting identity for 42352-2019.csv\n",
      "Checked accounting identity for 58056-2002.csv\n",
      "Checked accounting identity for 58056-2003.csv\n",
      "Checked accounting identity for 58056-2004.csv\n",
      "Checked accounting identity for 58056-2005.csv\n",
      "Checked accounting identity for 58056-2006.csv\n",
      "Checked accounting identity for 58056-2007.csv\n",
      "Checked accounting identity for 58056-2008.csv\n",
      "Checked accounting identity for 58056-2009.csv\n",
      "Checked accounting identity for 58056-2010.csv\n",
      "Checked accounting identity for 58056-2012.csv\n",
      "Checked accounting identity for 58056-2013.csv\n",
      "Checked accounting identity for 58056-2014.csv\n",
      "Checked accounting identity for 58056-2015.csv\n",
      "Checked accounting identity for 58056-2016.csv\n",
      "Checked accounting identity for 58056-2017.csv\n",
      "Checked accounting identity for 58056-2018.csv\n",
      "Checked accounting identity for 58056-2019.csv\n",
      "Checked accounting identity for 58056-2020.csv\n",
      "Checked accounting identity for 68136-2002.csv\n",
      "Checked accounting identity for 68136-2003.csv\n",
      "Checked accounting identity for 68136-2004.csv\n",
      "Checked accounting identity for 68136-2005.csv\n",
      "Checked accounting identity for 68136-2006.csv\n",
      "Checked accounting identity for 68136-2007.csv\n",
      "Checked accounting identity for 68136-2009.csv\n",
      "Checked accounting identity for 68136-2010.csv\n",
      "Checked accounting identity for 68136-2011.csv\n",
      "Checked accounting identity for 68136-2012.csv\n",
      "Checked accounting identity for 68136-2013.csv\n",
      "Checked accounting identity for 68136-2014.csv\n",
      "Checked accounting identity for 68136-2015.csv\n",
      "Checked accounting identity for 68136-2016.csv\n",
      "Checked accounting identity for 68136-2017.csv\n",
      "Checked accounting identity for 68136-2018.csv\n",
      "Checked accounting identity for 68136-2019.csv\n",
      "Checked accounting identity for 72267-2003.csv\n",
      "Checked accounting identity for 72267-2004.csv\n",
      "Checked accounting identity for 72267-2005.csv\n",
      "Checked accounting identity for 72267-2006.csv\n",
      "Checked accounting identity for 72267-2007.csv\n",
      "Checked accounting identity for 72267-2008.csv\n",
      "Checked accounting identity for 72267-2009.csv\n",
      "Checked accounting identity for 72267-2010.csv\n",
      "Checked accounting identity for 72267-2012.csv\n",
      "Checked accounting identity for 72267-2013.csv\n",
      "Checked accounting identity for 72267-2014.csv\n",
      "Checked accounting identity for 72267-2015.csv\n",
      "Checked accounting identity for 72267-2016.csv\n",
      "Checked accounting identity for 72267-2017.csv\n",
      "Checked accounting identity for 72267-2018.csv\n",
      "Checked accounting identity for 72267-2019.csv\n",
      "Checked accounting identity for 72267-2020.csv\n",
      "Checked accounting identity for 782124-2002.csv\n",
      "Checked accounting identity for 782124-2003.csv\n",
      "Checked accounting identity for 782124-2004.csv\n",
      "Checked accounting identity for 782124-2005.csv\n",
      "Checked accounting identity for 782124-2006.csv\n",
      "Checked accounting identity for 782124-2007.csv\n",
      "Checked accounting identity for 782124-2008.csv\n",
      "Checked accounting identity for 782124-2009.csv\n",
      "Checked accounting identity for 782124-2010.csv\n",
      "Checked accounting identity for 782124-2012.csv\n",
      "Checked accounting identity for 782124-2013.csv\n",
      "Checked accounting identity for 782124-2014.csv\n",
      "Checked accounting identity for 782124-2015.csv\n",
      "Checked accounting identity for 782124-2016.csv\n",
      "Checked accounting identity for 782124-2017.csv\n",
      "Checked accounting identity for 782124-2018.csv\n",
      "Checked accounting identity for 782124-2019.csv\n",
      "Checked accounting identity for 782124-2020.csv\n",
      "Checked accounting identity for 851376-2002.csv\n",
      "Checked accounting identity for 851376-2003.csv\n",
      "Checked accounting identity for 851376-2004.csv\n",
      "Checked accounting identity for 851376-2005.csv\n",
      "Checked accounting identity for 851376-2006.csv\n",
      "Checked accounting identity for 851376-2008.csv\n",
      "Checked accounting identity for 851376-2009.csv\n",
      "Checked accounting identity for 851376-2010.csv\n",
      "Checked accounting identity for 851376-2011.csv\n",
      "Checked accounting identity for 851376-2012.csv\n",
      "Checked accounting identity for 851376-2013.csv\n",
      "Checked accounting identity for 851376-2014.csv\n",
      "Checked accounting identity for 851376-2015.csv\n",
      "Checked accounting identity for 851376-2016.csv\n",
      "Checked accounting identity for 851376-2017.csv\n",
      "Checked accounting identity for 851376-2018.csv\n",
      "Checked accounting identity for 851376-2019.csv\n",
      "Checked accounting identity for 851376-2020.csv\n",
      "Checked accounting identity for 853784-2002.csv\n",
      "Checked accounting identity for 853784-2003.csv\n",
      "Checked accounting identity for 853784-2004.csv\n",
      "Checked accounting identity for 853784-2005.csv\n",
      "Checked accounting identity for 853784-2006.csv\n",
      "Checked accounting identity for 853784-2007.csv\n",
      "Checked accounting identity for 853784-2008.csv\n",
      "Checked accounting identity for 853784-2009.csv\n",
      "Checked accounting identity for 853784-2010.csv\n",
      "Checked accounting identity for 853784-2011.csv\n",
      "Checked accounting identity for 853784-2012.csv\n",
      "Checked accounting identity for 853784-2013.csv\n",
      "Checked accounting identity for 853784-2014.csv\n",
      "Checked accounting identity for 853784-2015.csv\n",
      "Checked accounting identity for 853784-2016.csv\n",
      "Checked accounting identity for 853784-2017.csv\n",
      "Checked accounting identity for 853784-2018.csv\n",
      "Checked accounting identity for 853784-2019.csv\n",
      "Checked accounting identity for 853784-2020.csv\n",
      "Checked accounting identity for 91154-2002.csv\n",
      "Checked accounting identity for 91154-2003.csv\n",
      "Checked accounting identity for 91154-2004.csv\n",
      "Checked accounting identity for 91154-2005.csv\n",
      "Checked accounting identity for 91154-2007.csv\n",
      "Checked accounting identity for 91154-2008.csv\n",
      "Checked accounting identity for 91154-2009.csv\n",
      "Checked accounting identity for 91154-2010.csv\n",
      "Checked accounting identity for 91154-2011.csv\n",
      "Checked accounting identity for 91154-2012.csv\n",
      "Checked accounting identity for 91154-2013.csv\n",
      "Checked accounting identity for 91154-2014.csv\n",
      "Checked accounting identity for 91154-2016.csv\n",
      "Checked accounting identity for 91154-2017.csv\n",
      "Checked accounting identity for 91154-2018.csv\n",
      "Checked accounting identity for 91154-2019.csv\n",
      "Checked accounting identity for 91154-2020.csv\n",
      "\n",
      "Our check has been cleared and removed line items accordingly\n"
     ]
    }
   ],
   "source": [
    "accounting_fix('Output/X-17A-5-Split/Liability & Equity/', liable_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Database consturction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def companyName(cik) -> str:\n",
    "    \"\"\"\n",
    "    Returns the company name for a given CIK number from the SEC by parsing the Edgar site\n",
    "    e.g. https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1904&type=X-17A-5&dateb=20201231\n",
    "    \n",
    "    Input:\n",
    "        :param: cik (type str)\n",
    "            The CIK number for a broker dealer e.g. 887767\n",
    "    Return:\n",
    "        :param: (type str)\n",
    "            Returns the accompanying name with the CIK provided e.g. 1ST GLOBAL CAPITAL CORP. \n",
    "    \"\"\"\n",
    "    # establishing base-url for company name search\n",
    "    baseURL = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&'\n",
    "    url = baseURL+'CIK={}&type=X-17A-5&dateb=20201231'.format(cik)\n",
    "    \n",
    "    # response time for retrieving company names, returning beautifulsoup object\n",
    "    res = requests.get(url, allow_redirects=True)\n",
    "    s1 = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # select the company information from the SEC website for a particular CIK\n",
    "    for val in s1.find_all('span', attrs={\"class\":\"companyName\"}):\n",
    "        # retrieve the company name from info class\n",
    "        return val.text.split('CIK')[0].split('/BD')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assetLines.txt', 'r') as f: assetSide = json.loads(f.read())\n",
    "with open('liabilityLines.txt', 'r') as f: liableSide = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "assetDict = dict([(i, np.nan) for i in assetSide])\n",
    "liableDict = dict([(i, np.nan) for i in liableSide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructured_data(filepaths, lineDictionary, lineItems) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Forms unstructured data frame from .csv file(s) located in s3 bucket\n",
    "    \n",
    "    :param: filepaths\n",
    "        filepaths from s3 that store .csv file(s) (/Output/BalanceSheet/)\n",
    "    :paran: lineDictionary\n",
    "        dictionary of total unstructured line items and corresponding values\n",
    "    :paran: lineItems\n",
    "        list of line items (asset/liabilites) that will be searched for\n",
    "        \n",
    "    :return: tuple\n",
    "        Returns a tuple, itters is a list of dataframes representing line items, error is a dictionary\n",
    "        tracking all error terms that exist due to univariate dataframes \n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize list to store dataframes and errors\n",
    "    itters = [0] * filepaths.size\n",
    "    \n",
    "    # iterate through files from s3 \n",
    "    for i, file in enumerate(filepaths):\n",
    "        \n",
    "        # create temporary dictionary copy for storage of values\n",
    "        tempDict = lineDictionary.copy()\n",
    "        \n",
    "        # creating two rows to track the CIK and year information released\n",
    "        cik, year = file.split('/')[-1].split('-')\n",
    "        \n",
    "        tempDict['CIK'] = cik                 # CIK number for firm \n",
    "        tempDict['Year'] = year[:4]           # Year for firm filing  \n",
    "        tempDict['Name'] = companyName(cik)   # returns the name of associated with the CIK\n",
    "        \n",
    "        # retrieving downloaded files from s3 bucket\n",
    "        s3.download_file(bucket, file, 'temp.pdf')\n",
    "        df = pd.read_csv('temp.pdf')\n",
    "        \n",
    "        # clean dataframe should be of size greater than 1\n",
    "        if len(df.columns) > 1:\n",
    "            \n",
    "            # extract line items from each dataframe (balance sheet)\n",
    "            lines = df[df.columns[0]]\n",
    "            \n",
    "            # filter dataframes according line items, and extract numerical values from dataframe \n",
    "            filterDF = df[np.isin(lines, lineItems)]\n",
    "            filterDF = filterDF.set_index(filterDF.columns[0])             # set line items as index\n",
    "\n",
    "            # iterate through items from (asset or liability items)\n",
    "            for item in filterDF.index:\n",
    "                lineVal = filterDF.loc[item]                 # line item e.g. Cash $72,343 $71,231\n",
    "                \n",
    "                # check to see scope of line value, checking if multi-rows present (Type DataFrame)\n",
    "                # in the event we have repeating 'item' lines (e.g. 2 Prepaid expense categories) we sum columns  \n",
    "                if type(lineVal) is not pd.Series:\n",
    "                    lineVal = lineVal.sum()\n",
    "                \n",
    "                value = lineVal.iloc[0]     # first column value e.g. 72343, either singular or sum \n",
    "                \n",
    "                # store value with appropriate item name\n",
    "                if ~np.isnan(value):\n",
    "                    tempDict[item] = value\n",
    "                \n",
    "            # convert the dictionary values to dataframe for database construction \n",
    "            row = pd.DataFrame.from_dict(tempDict, orient='index')\n",
    "            \n",
    "            # append dataframe set to array transposing \n",
    "            itters[i] = row.T\n",
    "            \n",
    "        else:\n",
    "            print('{} - encountered issue reading PDF'.format(file))\n",
    "        \n",
    "        # remove local file after it has been created\n",
    "        os.remove('temp.pdf')\n",
    "    \n",
    "    return pd.concat(itters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfBuild(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    # re-order the CIK and Year columns to appear as the first two columns\n",
    "    remap = df.columns[~np.isin(df.columns, ['CIK', 'Name', 'Year'])]\n",
    "    df = df[np.insert(remap, [0, 0, 0], ['CIK', 'Name', 'Year'])]\n",
    "\n",
    "    # filter out columns with NaN values, return only values\n",
    "    filterNaN = df.isnull().all()\n",
    "    cleanCols = filterNaN[filterNaN == False].index\n",
    "\n",
    "    # clean dataframe for unstructured asset terms\n",
    "    return df[cleanCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstructured data table from all .csv files \n",
    "assetpaths = np.array(session.list_s3_files(bucket, asset_folder))[1:]\n",
    "assetItters = unstructured_data(assetpaths, assetDict, assetSide)\n",
    "assetDF = dfBuild(assetItters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "assetDF.to_csv('unstructAsset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstructured data table from all .csv files \n",
    "liablepaths = np.array(session.list_s3_files(bucket, liable_folder))[1:]\n",
    "liableItters = unstructured_data(liablepaths, liableDict, liableSide)\n",
    "liableDF = dfBuild(liableItters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "liableDF.to_csv('unstructLiable.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
