{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import boto3\n",
    "import itertools\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, classification_report\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small class for holding \"special\" print flags\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with System Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Output/BalanceSheet/\"\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()\n",
    "\n",
    "# discover all of the pdfs that you want to parse\n",
    "paths = np.array(session.list_s3_files(bucket, data_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Training \n",
    "**We implement a classification model, with a hashing vectorizer to convert our string data into numerical measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_sec(X, y, vectorizer, classification, parameters, cvSplits:int=5, rng:int=1):\n",
    "    \"\"\"\n",
    "    Performs classification on text-based data, with corresponding CV scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # cross-validating engine used for CV score\n",
    "    cv = KFold(n_splits=cvSplits, shuffle=True, random_state=rng)\n",
    "    \n",
    "    # vectorizes the text via HashingVector to be used in classification implementation \n",
    "    xVector = vectorizer.fit_transform(X.values.flatten())\n",
    "    yVector = y.values.ravel()\n",
    "    \n",
    "    # compute the cross-validation for sample using base classification model\n",
    "    cv_score = cross_val_score(classification, xVector, yVector, cv=cv, scoring='f1_macro', verbose=0)\n",
    "    \n",
    "    print('Cross-validation measures using {} splits for default {} classifier'.format(cvSplits, \n",
    "                                                                                       type(classification).__name__))\n",
    "    print(cv_score)\n",
    "    print('\\nDefault estimator\\n', classification)\n",
    "    print('Average cross-validation score using base classifier: {s}%\\n'.format(s=round(np.average(cv_score) * 100, 4)))\n",
    "    \n",
    "    # split dependent and independent variables into training and testing windows (random split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xVector, yVector, test_size=0.20, shuffle=True, \n",
    "                                                        random_state=rng)\n",
    "    \n",
    "    # retreive test names for X (dependent) values, retain random state to match values for forecasts\n",
    "    _, test_names, _, _ = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=rng)\n",
    "    \n",
    "    # exhaustive search over specified parameter values for an estimator optimized by cross-validation\n",
    "    clf = GridSearchCV(estimator=classification, param_grid=parameters, n_jobs=1, cv=cv, verbose=1)\n",
    "    \n",
    "    # fit the classification model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # predict target classes based on classification model\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # compute the corresponding probabilites for each prediction\n",
    "    proba = clf.predict_proba(xVector)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Report back important measures of model accuracy\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    print('Best GridSearchCV estimator\\n', clf.best_estimator_)\n",
    "    print('Best GridSearchCV score: {}%'.format(round(clf.best_score_ * 100, 4)))\n",
    "    \n",
    "    score = round(precision_score(y_test, y_pred, average='micro') * 100, 4)\n",
    "    print(\"\\nPercision score from prediction using a micro average: {s}%\".format(s=score))\n",
    "    \n",
    "    print('1-Recall measures type-2 error (e.g. Expectation = Accounts payable, Prediction = Income tax payable)')\n",
    "    print('1-Percision measures type-1 error (e.g. Expectation = Income tax payable, Prediction = Accounts payable)')\n",
    "    print('\\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # construct a dataframe to keep track of all predictions for the dataset \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    c1 = pd.DataFrame(X[['LineItems']].values, \n",
    "                      columns=['Raw Lineitems'])\n",
    "    c2 = pd.DataFrame(y[['classification']].values, \n",
    "                      columns=['Manual Classification'])\n",
    "    c3 = pd.DataFrame(clf.predict(xVector), \n",
    "                      columns=['Model Predictions'])\n",
    "    c4 = pd.DataFrame(proba, columns=clf.classes_)           # each column stores the prediction probability\n",
    "\n",
    "    predDF = c1.join(c2).join(c3).join(c4)\n",
    "    \n",
    "    # reports back the maximum probability per row (use for each flag)\n",
    "    predDF['Max Probability'] = predDF[predDF.columns[3:]].max(axis=1)\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return clf, predDF, y_test, y_pred, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cMatrix(testcase:np.ndarray, prediction:np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes a confusion matrix from classification predictions, outputing heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    # each row shows the count of each corresponding label column wise, should have the columns the most populated\n",
    "    labelNames = np.unique(np.concatenate((np.unique(testcase), np.unique(prediction))))\n",
    "    cmat = pd.DataFrame(confusion_matrix(testcase, prediction, normalize=None), \n",
    "                        columns=labelNames, index=labelNames)\n",
    "    scaled_df = (cmat - cmat.min(axis=0)) / (cmat.max(axis=0) - cmat.min(axis=0))\n",
    "    \n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.title('Confusion Matrix (test-sample)', fontsize=14)\n",
    "    sns.heatmap(scaled_df, annot=cmat, linewidths=.5, cmap=\"RdYlGn\")\n",
    "    plt.ylabel('Expected LineItems', fontsize=14)\n",
    "    plt.xlabel('Predicted LineItems', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in asset and liability line items to be parsed\n",
    "assetML = pd.read_csv('assetML.csv')\n",
    "liabilityML = pd.read_csv('liabilityML.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break subset of line items and classification, for classification algorithm \n",
    "X1 = assetML[['LineItems']]\n",
    "y1 = assetML[['classification']]\n",
    "\n",
    "X2 = liabilityML[['LineItems']]\n",
    "y2 = liabilityML[['classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asset-line items classification model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return: Classification Model, Prediciton Mapping, Y-Perdictions, cross-validated score\n",
    "asset_mdl, asset_pred_df, asset_tests, asset_preds, asset_cv = cls_sec(X=X1, y=y1, \n",
    "                                    vectorizer=HashingVectorizer(strip_accents='unicode', lowercase=True, \n",
    "                                                                 analyzer='word', n_features=1000,  \n",
    "                                                                 norm='l2'), \n",
    "                                    classification=LogisticRegression(), \n",
    "                                    parameters={'penalty':['l2'], 'dual': [False], 'C': [1, 5, 10],\n",
    "                                                'fit_intercept': [True], 'intercept_scaling': [1, 5, 10],\n",
    "                                                'class_weight': ['balanced', None], 'random_state': [1], \n",
    "                                                'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                                                'max_iter': [1000], 'multi_class': ['multinomial']}, \n",
    "                                    cvSplits=5, rng=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_pred_df.to_csv('asset_ml_proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMatrix(asset_tests, asset_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liability & Equity line items classification model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return: Classification Model, Prediciton Mapping, Y-Perdictions, cross-validated score\n",
    "liablity_mdl, liablity_pred_df, liablity_tests, liablity_preds, liablity_cv = cls_sec(X=X2, y=y2, \n",
    "                                    vectorizer=HashingVectorizer(strip_accents='unicode', lowercase=True, \n",
    "                                                                 analyzer='word', n_features=1000,  \n",
    "                                                                 norm='l2'), \n",
    "                                    classification=LogisticRegression(), \n",
    "                                    parameters={'penalty':['l2'], 'dual': [False], 'C': [1, 5, 10],\n",
    "                                                'fit_intercept': [True], 'intercept_scaling': [1, 5, 10],\n",
    "                                                'class_weight': ['balanced', None], 'random_state': [1], \n",
    "                                                'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                                                'max_iter': [1000], 'multi_class': ['multinomial']}, \n",
    "                                    cvSplits=5, rng=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liablity_pred_df.to_csv('liable_ml_proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMatrix(liablity_tests, liablity_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving trained model to joblib for export, binary for model strictly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing model persistence https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "# dump asset & liability model classifiers () for future use\n",
    "dump(asset_mdl, 'asset_log_reg_mdl_v1.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(liablity_mdl, 'liability_log_reg_mdl_v1.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
