{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_data(unstructured_df:pd.DataFrame, cluster_df:pd.DataFrame, col_preserve:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a structured dataset from an unstructured column set\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: unstructured_df (type pandas.DataFrame)\n",
    "            unstuructured pandas dataframe with loose column construction \n",
    "        :param: cluster_df (type pandas.DataFrame)\n",
    "            a pandas dataframe of clustered labels and corresponding line items\n",
    "    Output:\n",
    "        :return: (type pandas DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_df = pd.DataFrame()\n",
    "    label_names = np.unique(cluster_df.Labels.values)\n",
    "    remap = {}\n",
    "    \n",
    "    # assume that the there exists columns 'CIK' and 'Year' for unstructured data\n",
    "    structured_df = unstructured_df[col_preserve]\n",
    "    \n",
    "    for label in label_names:\n",
    "        data = cluster_df[cluster_df['Labels'] == label]['LineItems']     # filter by corresponding cluster\n",
    "        \n",
    "        # we first select all predicted columns, then sum across rows for only numeric figures\n",
    "        selection = unstructured_df[data.values]\n",
    "        \n",
    "        sumV = selection.sum(axis=1, numeric_only=True)\n",
    "        \n",
    "        # we then select rows from the original unstructured dataframe with \n",
    "        # only np.nan and convert sumV index to np.nan\n",
    "        sumV[selection.isnull().all(axis=1)] = np.nan\n",
    "        \n",
    "        # assign dictionary to have labels and matching vector\n",
    "        remap[label] = sumV\n",
    "\n",
    "    structured_df = structured_df.assign(**remap)   \n",
    "    return structured_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probabilites(line_items:np.array, clf_mdl, vec_mdl) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a mapping convention for the machine learning predictions \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: line_items (type numpy.array)\n",
    "            list of all unstructured line item names\n",
    "        :param: clf_mdl (type joblib.obj)\n",
    "            a classification model to convert a line item \n",
    "        :param: vec_mdl (type joblib.obj)\n",
    "            a feature extraction model for string/text data \n",
    "    Output:\n",
    "        :return: (type pandas DataFrame)\n",
    "    \"\"\"\n",
    "    # predict the corresponding class for each line item\n",
    "    prediction = pd.DataFrame(data=clf_mdl.predict(vec_mdl.fit_transform(line_items)), columns=['Predicted Class'])\n",
    "    \n",
    "    # the actual line items that are used as predictors\n",
    "    lines = pd.DataFrame(line_items, columns=['Line Items'])\n",
    "    \n",
    "    # compute the probability for each prediction to the accompanying classes\n",
    "    prediction_probability = pd.DataFrame(data=clf_mdl.predict_proba(vec_mdl.fit_transform(line_items)),\n",
    "                                          columns=clf_mdl.classes_)\n",
    "    \n",
    "    # sum across row, determines total class probability measure \n",
    "    # NOTE: each class is bounded by 0.0-1.0, so total column wise sums can exceed 1.0\n",
    "    prediction_probability['Total Prediction score'] = prediction_probability.sum(axis=1) \n",
    "    \n",
    "    # join the line items to the prediction probabilities\n",
    "    return lines.join(prediction).join(prediction_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_pdf(df:pd.DataFrame, mdl):\n",
    "    \"\"\"\n",
    "    Return a dataframe for a company showcasing its column names, the predicted class and the original values.\n",
    "    This function is used for error handling and de-bugging as it returns (Lineitems, Predictions, Linevalues) \n",
    "    \"\"\"\n",
    "    # split values for company dataframe according to columns and values\n",
    "    colNames = df.index\n",
    "    colValues = df.values\n",
    "    \n",
    "    # predicting the column groups with accompanying sklearn model\n",
    "    # NOTE: We pre-process with a HashingVectorizer with 1000 features, this action is very model specific\n",
    "    predNames = mdl.predict(HashingVectorizer(n_features=1000).fit_transform(colNames))\n",
    "\n",
    "    retDF = pd.DataFrame({'Original Lineitems': colNames,                       # the original line items\n",
    "                          'Predicted Lineitems': predNames,                     # the predicted line items\n",
    "                          'Line values': colValues.flatten().tolist()})         # the corresponding line values\n",
    "    \n",
    "    return retDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final structured dataframe has been created.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    output_folder = 'Output/'\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # check available pdfs stored within desired output-folder\n",
    "    s3_path = session.list_s3_files(bucket, output_folder)\n",
    "    \n",
    "    # retrieving the unstructured asset values file from s3 bucket\n",
    "    s3.download_file(bucket, 'Output/unstructured_assets.csv', 'unstructAsset.csv')\n",
    "    s3.download_file(bucket, 'Output/unstructured_liable.csv', 'unstructLiable.csv')\n",
    "\n",
    "    # load in asset and liability dataframes\n",
    "    assetDF = pd.read_csv('unstructAsset.csv')\n",
    "    liableDF = pd.read_csv('unstructLiable.csv')\n",
    "\n",
    "    # remove local file after it has been created (variable is stored in memory)\n",
    "    os.remove('unstructAsset.csv')\n",
    "    os.remove('unstructLiable.csv')\n",
    "    # ==============================================================================\n",
    "    \n",
    "    # load in sklearn classification models\n",
    "    assetMDL = load('asset_log_reg_mdl_v1.joblib')\n",
    "    liableMDL = load('liability_log_reg_mdl_v1.joblib')\n",
    "    \n",
    "    # Use classification model to predict label names for each line item\n",
    "    # NOTE: we select the post-first 5 columns avoiding the CIK, Name, Filing Date, Fiscal Year, Totals Check\n",
    "    asset_label_predictions = assetMDL.predict(HashingVectorizer(n_features=1000).fit_transform(assetDF.columns[5:]))\n",
    "    liable_label_predictions = liableMDL.predict(HashingVectorizer(n_features=1000).fit_transform(liableDF.columns[5:]))\n",
    "    \n",
    "    # structured database for asset and liability terms \n",
    "    struct_asset_map = pd.DataFrame([assetDF.columns[5:], asset_label_predictions], \n",
    "                                   index=['LineItems', 'Labels']).T\n",
    "\n",
    "    struct_liable_map = pd.DataFrame([liableDF.columns[5:], liable_label_predictions], \n",
    "                                    index=['LineItems', 'Labels']).T\n",
    "    \n",
    "    # construct the line-item prediction classes with corresponding probabilites \n",
    "    a_proba_df = prediction_probabilites(assetDF.columns[5:], \n",
    "                                         assetMDL, \n",
    "                                         HashingVectorizer(n_features=1000))\n",
    "    l_proba_df = prediction_probabilites(liableDF.columns[5:], \n",
    "                                         liableMDL, \n",
    "                                         HashingVectorizer(n_features=1000))\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Database construction \n",
    "    # ==============================================================================\n",
    "    \n",
    "    filename = 'asset_prediction_proba.csv'\n",
    "    a_proba_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename, Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    filename = 'liable_prediction_proba.csv'\n",
    "    l_proba_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename, Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    \n",
    "    filename = 'asset_name_map.csv'\n",
    "    struct_asset_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename, Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    filename = 'liability_name_map.csv'\n",
    "    struct_liable_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename, Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    \n",
    "    # structured database for asset and liability terms \n",
    "    struct_asset_df = structured_data(assetDF, struct_asset_map, \n",
    "                             col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year', \n",
    "                                           'Total asset check'])\n",
    "    filename1 = 'structured_asset.csv'\n",
    "    struct_asset_df.to_csv(filename1, index=False)\n",
    "    with open(filename1, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename1, Body=data)\n",
    "        \n",
    "        \n",
    "    # structured database for asset and liability terms \n",
    "    struct_liable_df = structured_data(liableDF, struct_liable_map, \n",
    "                             col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year', \n",
    "                                           \"Total liabilities & shareholder's equity check\"])\n",
    "    filename2 = 'structured_liability.csv'\n",
    "    struct_liable_df.to_csv(filename2, index=False)\n",
    "    with open(filename2, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + filename2, Body=data)\n",
    "    \n",
    "    # remove local file after it has been created\n",
    "    os.remove(filename1)\n",
    "    os.remove(filename2)\n",
    "    # ==============================================================================\n",
    "    \n",
    "    print('The final structured dataframe has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_columns = struct_asset_df.columns[~np.isin(struct_asset_df.columns, \n",
    "#                                                  ['CIK', 'Name', 'Filing Date', 'Filing Year', 'Total asset check', 'Total assets'])]\n",
    "# (struct_asset_df['Total assets'] - struct_asset_df[other_columns].sum(axis=1)) / struct_asset_df['Total assets'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Amazon Textract client and Sagemaker session\n",
    "# s3 = boto3.client('s3')\n",
    "# session = Session()\n",
    "\n",
    "# bucket = 'ran-s3-systemic-risk'\n",
    "# output_folder = 'Output/'\n",
    "\n",
    "# # ==============================================================================\n",
    "# # check available pdfs stored within desired output-folder\n",
    "# s3_path = session.list_s3_files(bucket, output_folder)\n",
    "\n",
    "# # retrieving CIK-Dealers JSON file from s3 bucket\n",
    "# s3.download_file(bucket, 'Output/unstructured_assets.csv', 'unstructAsset.csv')\n",
    "# s3.download_file(bucket, 'Output/unstructured_liable.csv', 'unstructLiable.csv')\n",
    "\n",
    "# # load in asset and liability dataframes\n",
    "# assetDF = pd.read_csv('unstructAsset.csv')\n",
    "# liableDF = pd.read_csv('unstructLiable.csv')\n",
    "\n",
    "# # remove local file after it has been created (variable is stored in memory)\n",
    "# os.remove('unstructAsset.csv')\n",
    "# os.remove('unstructLiable.csv')\n",
    "# # ==============================================================================\n",
    "\n",
    "# # load in sklearn classification models\n",
    "# assetMDL = load('asset_log_reg_mdl_v1.joblib')\n",
    "# liableMDL = load('liability_log_reg_mdl_v1.joblib')\n",
    "\n",
    "# # Use classification model to predict label names for each line item\n",
    "# # (select the post-first 4 columns avoid the CIK, Name, Filing Date, Fiscal Year)\n",
    "# asset_label_predictions = assetMDL.predict(HashingVectorizer(n_features=1000).fit_transform(assetDF.columns[4:]))\n",
    "# liable_label_predictions = liableMDL.predict(HashingVectorizer(n_features=1000).fit_transform(liableDF.columns[4:]))\n",
    "\n",
    "# # structured database for asset and liability terms \n",
    "# struct_asset_map = pd.DataFrame([assetDF.columns[4:], asset_label_predictions], \n",
    "#                                index=['LineItems', 'Labels']).T\n",
    "\n",
    "# struct_liable_map = pd.DataFrame([liableDF.columns[4:], liable_label_predictions], \n",
    "#                                 index=['LineItems', 'Labels']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = liableDF[(liableDF.CIK == 1224385) & (liableDF['Filing Date'] == '2004-03-01')].T.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_pdf(temp.iloc[5:], liableMDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
