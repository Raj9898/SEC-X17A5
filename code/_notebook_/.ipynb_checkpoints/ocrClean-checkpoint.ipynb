{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.0.1)\n",
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.2.0)\n",
      "Requirement already satisfied: minecart in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: pdfminer3k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.3.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Requirement already satisfied: textract-trp in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install smart_open minecart\n",
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate s3 bucket and corresponding data folder\n",
    "bucket = \"ran-s3-systemic-risk\"\n",
    "data_folder =\"Input/X-17A-5-Subsets/\"\n",
    "\n",
    "# script to perform OCR (using Textract) for X-17A-5 subsets\n",
    "out_folder = 'Output/X-17A-5-BS/'\n",
    "\n",
    "# Amazon Textract client and Sagemaker session\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.array(session.list_s3_files(bucket, out_folder))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We begin by first stripping away NaN terms in the first column and then mapping all the NaN terms to an empty string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .csv files are cleaned of NaN terms\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "\n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "\n",
    "    # first begin by filtering out the NaN rows present in the first column\n",
    "    filterDF = df[np.isin(df[df.columns[0]], df[df.columns[0]].dropna())]\n",
    "    filterDF = filterDF.fillna('')\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    filterDF.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('All .csv files are cleaned of NaN terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly\n",
    "    - Example releases include but are note limited to 1224385-2016, 72267-2003\n",
    "    ----\n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                                          | NaN            | NaN  \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    # work on itterative merging for rows, check left/right and top/bottom\n",
    "    n = df.shape[0]\n",
    "    trans = []\n",
    "\n",
    "    for i in range(n):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        name = row.iloc[0]\n",
    "        col1 = row.iloc[1]\n",
    "        col2 = row.iloc[2]\n",
    "\n",
    "        # check the position of values for columns 1 vs 2\n",
    "        if col1 is not np.nan:\n",
    "            trans.append([name, col1]) \n",
    "        elif col2 is not np.nan:\n",
    "            trans.append([name, col2])\n",
    "\n",
    "        # we want to check if there exists two NaNs - is it real or false flag\n",
    "        # we assume the column-check will filter out year-matching in ocrTextract\n",
    "        if (col1 is np.nan) and (col2 is np.nan): \n",
    "            # look up one row (if possible to see if col1 and col2 are populated)\n",
    "            try:\n",
    "                # check the information for the above row\n",
    "                prior_row = df.iloc[i-1]\n",
    "                prior_col1 = prior_row.iloc[1]\n",
    "                prior_col2 = prior_row.iloc[2]\n",
    "                \n",
    "                # if both values present then we simply use the right hand side value above  \n",
    "                if (prior_col1 is not np.nan) and (prior_col2 is not np.nan):\n",
    "                    trans.append([name, prior_col2])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(trans, columns=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # if columns greater than 2, we have a weird data table\n",
    "    if df.columns.size > 2:\n",
    "        \n",
    "        # two events could occur at this point (either total splits, or year splits)\n",
    "        arr = df[df.columns[2]].values\n",
    "        \n",
    "        # check the scope of the second column \n",
    "        n = arr.size\n",
    "        k = arr.tolist().count(np.nan)\n",
    "        \n",
    "        # K-check: if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "        # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "        if k/n >= 0.50:\n",
    "            tempDF = singular_merge(df)\n",
    "        else:\n",
    "            tempDF = df[df.columns[:2]]\n",
    "\n",
    "        # writing data frame to .csv file\n",
    "        tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "        \n",
    "        print('We merged {}'.format(fileName))\n",
    "        # remove local file after it has been created\n",
    "        os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame, text_file:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes\n",
    "    - Example releases include but are note limited to 42352-2015, 58056-2009, 58056-2013, 58056-2019\n",
    "    \n",
    "    Input:\n",
    "        :param df: (type pandas dataframe)\n",
    "            References the balance sheet dataframe read in from AWS Textract\n",
    "        :param text_file: (type dictionary)\n",
    "            Stores text values with corresponding confidence level for balance sheet pages\n",
    "    \n",
    "    Output:\n",
    "        :param return: (type pandas dataframe) \n",
    "            A processed dataframe of size greater than or equal to the inputed dataframe\n",
    "    \n",
    "    NOTE: Our objective isn't to achieve a perfect split, but rather create labels easy enough for our predictive \n",
    "    model to identify and accurately predict. This is not a perfect method and we make the assumption that a merged \n",
    "    row exists when a space exists in the value column (e.g. [19,345 2,213])\n",
    "    \"\"\"\n",
    "    \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    def find_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            arr = val.split(' ')\n",
    "            \n",
    "            # remove the $ sign if present in the list (this helps avoid false pasitives) \n",
    "            arr = list(filter(lambda x: x != '$', arr))\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(arr) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        \n",
    "        # handle exception for NaN (no attribute to split) \n",
    "        except AttributeError: return False\n",
    "    \n",
    "    def extract_lineitems(val, dictionary:dict) -> list:\n",
    "        \"\"\"\n",
    "        Extract the appropriate line items from each line value \n",
    "        \"\"\"\n",
    "        splits = []\n",
    "        \n",
    "        # iterate through each line item\n",
    "        for i in dictionary.keys():\n",
    "    \n",
    "            # we check for real key-value names avoiding single character keys\n",
    "            if len(i) > 1: idx = val.find(i)\n",
    "\n",
    "                # if we find such a value we append the series\n",
    "                if idx >= 0: splits.append(i)\n",
    "        \n",
    "        # check whether we have a one-to-one mapping between line\n",
    "        # items and line values, if not we adjust\n",
    "        n = len(splits) - len(values)\n",
    "        if n > 0: return splits[n:]\n",
    "        else: return splits\n",
    "    \n",
    "    def recursive_splits(splits:list, lineName:list, sub=[]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Recursively breaks up merged rows for each split until no merged row is left\n",
    "        \"\"\"\n",
    "        # if our list exceeds 1 in length, we continue to split\n",
    "        if len(splits) > 1:\n",
    "            # construct a dataframe row of the first split term to append to sub list\n",
    "            row = pd.DataFrame([lineName[0], splits[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we pass the +1 index splits and line name, appending the first-most layer \n",
    "            return recursive_splits(splits[1:], lineName[1:], sub=sub)\n",
    "        else:\n",
    "            row = pd.DataFrame([lineName[0], splits[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we concatenate all DataFrames vertically to form a large DataFrame \n",
    "            return pd.concat(sub)\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################    \n",
    "    \n",
    "    # select all the rows that match our description, where a space exists \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_splits(x))]\n",
    "    idxs = selections.index\n",
    "    \n",
    "    # iterate through each row that is determined to be conjoined\n",
    "    for i in idxs:\n",
    "        \n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = df.loc[:i-1]\n",
    "        bottom = df.loc[i+1:]\n",
    "        \n",
    "        # divide the identified term from the selection e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        # and filter out the $ sign in the list e.g. [\"$\", \"9,112,943\", \"13,151,663\"] -> [9,112,943\", \"13,151,663\"]\n",
    "        values = df[df.columns[1]].loc[i].split(' ')\n",
    "        values = list(filter(lambda x: x != '$', values))\n",
    "        \n",
    "        # extract line names according to Text parsed list (requires parsed Text string)\n",
    "        lineName = df[df.columns[0]].loc[i]\n",
    "        lineName = extract_lineitems(lineName, text_file)\n",
    "        \n",
    "        # determine the splits for the corresponding row\n",
    "        mid = recursive_splits(values, lineName, sub=[])\n",
    "        mid.columns = ['0', '1']\n",
    "\n",
    "        # reassign the value of df2 to update across each iteration\n",
    "        df = pd.concat([top, mid, bottom])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We fixed all conjoined tables in sample\n"
     ]
    }
   ],
   "source": [
    "# track the presence of json file storing information on forms\n",
    "if os.path.exists('X17A5-text.json'):\n",
    "    with open('X17A5-text.json', 'r') as f: text = json.loads(f.read())\n",
    "\n",
    "    for csv in paths:\n",
    "        fileName = csv.split('/')[-1]\n",
    "\n",
    "        # work on combining columns that are issued seperately\n",
    "        s3.download_file(bucket, csv, 'temp.pdf')\n",
    "        df = pd.read_csv('temp.pdf')\n",
    "        \n",
    "        # compute the row merged figures \n",
    "        filter_json = fileName.split('-')[:2]\n",
    "        tempDF = row_split(df, filter_json)\n",
    "\n",
    "        # if difference is found then \n",
    "        if tempDF.shape != df.shape:\n",
    "            print(\"Fixed the rows for {}\".format(fileName))\n",
    "\n",
    "            # writing data frame to .csv file\n",
    "            tempDF.to_csv(fileName, index=False)\n",
    "\n",
    "            # save contents to AWS S3 bucket\n",
    "            with open(fileName, 'rb') as data:\n",
    "                s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "            # remove local file after it has been created\n",
    "            os.remove(fileName)\n",
    "\n",
    "        # remove local file after it has been created\n",
    "        os.remove('temp.pdf')\n",
    "\n",
    "    print('We fixed all conjoined tables in sample')\n",
    "    \n",
    "else:\n",
    "    print('We do not have a text file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format\n",
    "    :param: value, string value with hidden numeric quanity  \n",
    "    :return: floating point values\n",
    "    \n",
    "    Complexity -> O(n)\n",
    "    \n",
    "    e.g.\n",
    "        In[0]: $ 19,225     ->   Out[0]: 19255\n",
    "        In[0]: $ 19,225.76  ->   Out[0]: 19255.76\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    def num_strip(number):\n",
    "        numType = type(number)\n",
    "        \n",
    "        # if provided a non-empty string, perform regex operation \n",
    "        if (numType is str) and (len(number) > 0):\n",
    "            \n",
    "            # check for accounting formats that use parenthesis to signal losses \n",
    "            if number[0] == '(': number = '-' + number\n",
    "\n",
    "            # case replacing to handle poor textract reading of numbers\n",
    "            number = number.replace('I', '1').replace('l', '1')\n",
    "            \n",
    "            # --------------------------------------------------------------\n",
    "            # Explanation of the Regex Expression:\n",
    "            #      [^0-9|.|-]     = match all elements that are not numeric 0-9, periods \".\" or hyphens \"-\"\n",
    "            #      (?<!^)-        = match all elements that are hyphens \"-\" not in the first index position\n",
    "            #      \\.(?=[^.]*\\.)  = match all elements that are periods \".\" except the last instance\n",
    "            # --------------------------------------------------------------\n",
    "            \n",
    "            check1 = re.sub(\"[^0-9|.|-]\", \"\", number)         # remove all the non-numeric, periods \".\" or hyphens \"-\"\n",
    "            check2 = re.sub(\"(?<!^)-\", \"\", check1)            # removes all \"-\" that aren't in the first index \n",
    "            check3 = re.sub(\"\\.(?=[^.]*\\.)\", \"\", check2)      # removes all periods except the last instance of \".\" \n",
    "            \n",
    "            # --------------------------------------------------------------\n",
    "            \n",
    "            # we consider weird decimal values that exceed 2 spaces to the right (e.g. 432.2884)\n",
    "            period_check = check3.find('.')                         # returns the location of the period \n",
    "            right_tail_length = len(check3) - period_check - 1      # right-tail length should not exceed 2\n",
    "            \n",
    "            # if more than 2 trailing digits to decimal point we assume incorrect placement\n",
    "            if right_tail_length > 2:\n",
    "                check3 = check3.replace('.', '')\n",
    "            \n",
    "            # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "            if (check3 == '-') or (check3 == '.'):\n",
    "                return 0.0\n",
    "            else:\n",
    "                # try to cast to floating point value, else flat NaN\n",
    "                try: \n",
    "                    return float(check3)\n",
    "                except ValueError: \n",
    "                    return np.nan\n",
    "        \n",
    "        # if operator is an integer or float then simply return the value\n",
    "        elif (numType is int) or (numType is float):\n",
    "            return number\n",
    "    \n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value\n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We converted all tables in the sample to numeric figures\n"
     ]
    }
   ],
   "source": [
    "for csv in paths:\n",
    "    fileName = csv.split('/')[-1]\n",
    "    \n",
    "    # work on combining columns that are issued seperately\n",
    "    s3.download_file(bucket, csv, 'temp.pdf')\n",
    "    df = pd.read_csv('temp.pdf')\n",
    "    \n",
    "    # pass numeric converter to the column \n",
    "    df[df.columns[1]] = df[df.columns[1]].apply(cleanNumeric)\n",
    "    \n",
    "    # remove NaNs values and reset index to return values\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()[['0', '1']]\n",
    "\n",
    "    # writing data frame to .csv file\n",
    "    df.to_csv(fileName, index=False)\n",
    "\n",
    "    # save contents to AWS S3 bucket\n",
    "    with open(fileName, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + fileName, Body=data)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove(fileName)\n",
    "\n",
    "    # remove local file after it has been created\n",
    "    os.remove('temp.pdf')\n",
    "    \n",
    "print('We converted all tables in the sample to numeric figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
