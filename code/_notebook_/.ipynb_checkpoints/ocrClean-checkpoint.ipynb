{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: minecart in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: pdfminer3k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.3.4)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textract-trp in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Run on first instance to install required libraries\n",
    "%pip install smart_open\n",
    "%pip install minecart\n",
    "%pip install textract-trp\n",
    "%pip install python-Levenshtein\n",
    "%pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_strip(number):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format. \n",
    "    We handle input arguments of a string, integer or numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    numType = type(number)\n",
    "\n",
    "    # if provided a non-empty string, perform regex operation \n",
    "    if (numType is str) and (len(number) > 0):\n",
    "\n",
    "        # check for accounting formats that use parenthesis to signal losses \n",
    "        if number[0] == '(': number = '-' + number\n",
    "\n",
    "        # case replacing to handle poor textract reading of numbers\n",
    "        number = number.replace('I', '1').replace('l', '1')\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Explanation of the Regex Expression:\n",
    "        #      [^0-9|.|-]     = match all elements that are not numeric 0-9, periods \".\" or hyphens \"-\"\n",
    "        #      (?<!^)-        = match all elements that are hyphens \"-\" not in the first index position\n",
    "        #      \\.(?=[^.]*\\.)  = match all elements that are periods \".\" except the last instance\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        check1 = re.sub(\"[^0-9|.|-]\", \"\", number)         # remove all the non-numeric, periods \".\" or hyphens \"-\"\n",
    "        check2 = re.sub(\"(?<!^)-\", \"\", check1)            # removes all \"-\" that aren't in the first index \n",
    "        check3 = re.sub(\"\\.(?=[^.]*\\.)\", \"\", check2)      # removes all periods except the last instance of \".\" \n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        # we consider weird decimal values that exceed 2 spaces to the right (e.g. 432.2884)\n",
    "        period_check = check3.find('.')                         # returns the location of the period \n",
    "        right_tail_length = len(check3) - period_check - 1      # right-tail length should not exceed 2\n",
    "\n",
    "        # if more than 2 trailing digits to decimal point we assume incorrect placement\n",
    "        if right_tail_length > 2:\n",
    "            check3 = check3.replace('.', '')\n",
    "\n",
    "        # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "        if (check3 == '-') or (check3 == '.'):\n",
    "            return 0.0\n",
    "        else:\n",
    "            # try to cast to floating point value, else flat NaN\n",
    "            try: \n",
    "                return float(check3)\n",
    "            except ValueError: \n",
    "                return np.nan\n",
    "\n",
    "    # if operator is an integer or float then simply return the value\n",
    "    elif (numType is int) or (numType is float):\n",
    "        return number\n",
    "\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Shaving\n",
    "**Removing blank/empty rows that are reported in the line items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_purge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Column designed to filter out rows that are NaN (empty) and reduce dataframe size\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of size less than or equal to the original input \n",
    "    \"\"\"\n",
    "    # begin by filtering out the NaN rows present in the first column\n",
    "    first_col = df.columns[0]\n",
    "    new_df = df[np.isin(df[first_col], df[first_col].dropna())]    # select subset of rows \n",
    "    \n",
    "    # we reset the index of our new_df to recoup a consecutive index count\n",
    "    new_df = new_df.reset_index()\n",
    "    new_df = new_df[new_df.columns[1:]]    # skip the first column since we reset the index\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly. \n",
    "    Example releases include, but are note limited to, 1224385-2016 and 72267-2003\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of of size (Nx3) -> (Nx2)\n",
    "    \n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                                          | NaN            | NaN  \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    # work on itterative merging for rows, check left/right and top/bottom\n",
    "    n = df.shape[0]\n",
    "    trans = []\n",
    "\n",
    "    for i in range(n):\n",
    "        row = df.iloc[i]         # index into the row\n",
    "\n",
    "        name = row.iloc[0]       # the line item name (e.g. Total Assets)\n",
    "        col1 = row.iloc[1]       # the first value(s) column\n",
    "        col2 = row.iloc[2]       # the second value(s) column \n",
    "        \n",
    "        # ----------------------------------------------\n",
    "        # NOTE: We say nothing if both col 1 and 2 are \n",
    "        #     both populated with a numeric value\n",
    "        # ----------------------------------------------\n",
    "        \n",
    "        if num_strip(col1) is not np.nan:\n",
    "            trans.append([name, col1])     # if column 1 has a numeric value we take it by default\n",
    "        elif num_strip(col2) is not np.nan:\n",
    "            trans.append([name, col2])     # if column 1 has no numeric value, but column 2 does, we take it\n",
    "            \n",
    "        # ----------------------------------------------\n",
    "        \n",
    "        # we want to check if there exists two NaNs - is it real or false flag\n",
    "        if (col1 is np.nan) and (col2 is np.nan): \n",
    "            \n",
    "            # look up one row (if possible to see if col1 and col2 are populated)\n",
    "            try:\n",
    "                # check the information for the above row\n",
    "                prior_row = df.iloc[i-1]                     # previous dataframe row \n",
    "                prior_col1 = prior_row.iloc[1]               # first column from previous row\n",
    "                prior_col2 = prior_row.iloc[2]               # second column from previous row\n",
    "                \n",
    "                # if both values present then we simply use the right hand side value above  \n",
    "                if (prior_col1 is not np.nan) and (prior_col2 is not np.nan):\n",
    "                    trans.append([name, prior_col2])\n",
    "            \n",
    "            # IndexError if not possible to look up one row       \n",
    "            except IndexError: pass\n",
    "    \n",
    "    return pd.DataFrame(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function determines whether a Balance Sheet should be merged or simply filtered. \n",
    "    Our two cases are determined as follows:\n",
    "        * If the second column present in the balance sheet is mostly empty we assume that \n",
    "          the second column is an aggregated column, and we can merge it\n",
    "        * However, if the second column is mostly filled with values, we assume that this\n",
    "          represents accounting figures from the previous year \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of of size (Nx3) -> (Nx2)\n",
    "    \"\"\"\n",
    "    # two events could occur at this point (either the column represents totals, or values from a prior-year)\n",
    "    arr = df[df.columns[2]].values\n",
    "\n",
    "    # check the scope of the second column \n",
    "    n = arr.size\n",
    "    k = arr.tolist().count(np.nan)\n",
    "\n",
    "    # k-check: if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "    # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "    if k/n >= 0.50:\n",
    "        new_df = merge(df)            # merge rows by merge function\n",
    "    else:\n",
    "        new_df = df[df.columns[:2]]   # return the most recent year \n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dollar_check(num):\n",
    "    \"\"\"\n",
    "    A function to check the presence of a '$' or 'S'. This function is used to \n",
    "    complement our row splits function to determine \"True splits\"\n",
    "    \"\"\"\n",
    "    if num not in ['$', 'S']:\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame, text_file:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes into individual rows.\n",
    "    Example releases include, but are note limited to, 42352-2015, 58056-2009, 58056-2013, 58056-2019\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param df: (type pandas.DataFrame)\n",
    "            References the balance sheet dataframe read in from AWS Textract\n",
    "        :param text_file: (type dictionary)\n",
    "            Stores text values with corresponding confidence level for balance sheet pages\n",
    "    \n",
    "    Output:\n",
    "        :param return: (type pandas.DataFrame) \n",
    "            A processed dataframe of size greater than or equal to the inputed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # ##############################################################\n",
    "    # NESTED HELPER FUNCTIONS\n",
    "    # ##############################################################\n",
    "    \n",
    "    def find_row_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not. We make\n",
    "        the assumption that a row is conjoined or merged if there exists a space in the \n",
    "        first value column (omiting the dollar sign $ and S which may be read in)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            arr = val.split(' ')\n",
    "            \n",
    "            # remove the '$' sign or 'S' if present in the list (this helps avoid false pasitives) \n",
    "            arr = list(filter(dollar_check, arr))\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(arr) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        \n",
    "        # handle exception for NaN (no attribute to split) \n",
    "        except AttributeError: return False\n",
    "    \n",
    "    def extract_lineitems(line:list, value:list, dictionary:dict) -> list:\n",
    "        \"\"\"\n",
    "        Extract the appropriate line items from each line value.\n",
    "        \"\"\"\n",
    "        splits = []\n",
    "        \n",
    "        # iterate through each line item\n",
    "        for i in dictionary.keys():\n",
    "    \n",
    "            # we check for real key-value names avoiding single character keys\n",
    "            if len(i) > 1: \n",
    "                idx = line.find(i)    # find the index of key-value (if possible) in line item array\n",
    "\n",
    "                # if we find such a value we append the series (failure to find results idx = -1)\n",
    "                if idx >= 0: splits.append(i)\n",
    "        \n",
    "        # check whether we have a one-to-one mapping between line items and line values, \n",
    "        # e.g. ['Assets', 'Cash', 'Recievables'] -> ['1,233', '4,819'] (3x2 mapping)\n",
    "        n = len(splits) - len(value)\n",
    "        \n",
    "        # if n is equal to zero we have a \"perfect\" match\n",
    "        if n == 0:\n",
    "            return (splits, value)\n",
    "        elif n > 0:\n",
    "            return (splits[n:], value)       # more line items terms, assume values is right\n",
    "        elif n == -1:                        \n",
    "            return (splits, values[1:])      # more value terms, assume value is wrong only if difference is 1 in size\n",
    "        else: \n",
    "            return None                      # no specific rule paradigm (more values than items)\n",
    "    \n",
    "    def recursive_splits(values:list, lineName:list, sub=[]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Recursively breaks up merged rows for each split until no merged row is left\n",
    "        \"\"\"\n",
    "        # if our list exceeds 1 in length, we continue to split\n",
    "        if len(values) > 1:\n",
    "            # construct a dataframe row of the first split term to append to sub list\n",
    "            row = pd.DataFrame([lineName[0], values[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we pass the +1 index splits and line name, appending the first-most layer \n",
    "            return recursive_splits(values[1:], lineName[1:], sub=sub)\n",
    "        else:\n",
    "            row = pd.DataFrame([lineName[0], values[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we concatenate all DataFrames vertically to form a large DataFrame \n",
    "            return pd.concat(sub)\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################    \n",
    "    \n",
    "    # select all the rows that match our description, where a space exists = row merge \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_row_splits(x))]\n",
    "    idxs = selections.index\n",
    "    \n",
    "    # iterate through each row that is determined to be conjoined\n",
    "    for i in idxs:\n",
    "        \n",
    "        # find the index location od merged row\n",
    "        row_idx = np.argmax(df.index == i)\n",
    "        \n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = df.iloc[:row_idx]\n",
    "        bottom = df.iloc[row_idx+1:]\n",
    "\n",
    "        # divide the identified term from the selection e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        # and filter out the $ sign in the list e.g. [\"$\", \"9,112,943\", \"13,151,663\"] -> [9,112,943\", \"13,151,663\"]\n",
    "        values = df[df.columns[1]].loc[i].split(' ')\n",
    "        values = list(filter(dollar_check, values))\n",
    "        \n",
    "        # extract line names and corresponding values according to Text parsed list (requires parsed TEXT JSON)\n",
    "        # e.g. ['Securities Held Total Assets'] -> ['Securities Held', 'Total Assets']\n",
    "        lineName = df[df.columns[0]].loc[i]\n",
    "        \n",
    "        # return line items and values that should match in size\n",
    "        response_extraction = extract_lineitems(lineName, values, text_file)\n",
    "        \n",
    "        # if we retun a lineitem then we can perform recursive splits (otherwise avoid)\n",
    "        if type(response_extraction) is not type(None):\n",
    "            \n",
    "            clean_lineitems, clean_values = response_extraction\n",
    "            \n",
    "            # determine the splits for the corresponding row\n",
    "            mid = recursive_splits(clean_values, clean_lineitems, sub=[])\n",
    "            mid.columns = df.columns\n",
    "\n",
    "            # re-assign the value of df2 to update across each iteration\n",
    "            df = pd.concat([top, mid, bottom])\n",
    "            \n",
    "        else:\n",
    "            # no need for mid, since we have removed it from existence\n",
    "            df = pd.concat([top, bottom])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_scale(text_dict:dict) -> float:\n",
    "    \"\"\"\n",
    "    Function used for scaling accounting figures by reported unites\n",
    "    \"\"\"\n",
    "    scalar = {'thousands': 1e3, 'hundreds':1e2, 'millions':1e6, 'billions': 1e9}\n",
    "    \n",
    "    # iterate through each of the text values from dictionary map\n",
    "    for text_value in text_dict.keys():\n",
    "        \n",
    "        # we check to see whether the text is found in our scalar dictionary\n",
    "        for scale_type in scalar.keys():\n",
    "            \n",
    "            # search for the presence of the scale identifier (e.g. millions) \n",
    "            # we use a \"fuzzy-partial\" match to include partial fits (e.g. Dollar in Millions)\n",
    "            scale_search = fuzz.partial_ratio(scale_type.lower(), text_value.lower())\n",
    "            \n",
    "            # we make the assumption that a score of 90 or greater, signals a match\n",
    "            if scale_search >= 90:\n",
    "                return scalar[scale_type]              # if found we simply return the multiplier\n",
    "    \n",
    "    # default to no multiplier (1)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function is a wrapper for calling the numerical extraction function \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param value:\n",
    "            String with hidden numeric quanity (e.g. $ 19,225 = 19255)  \n",
    "        :param text_file: (type dictionary)\n",
    "            Stores text values with corresponding confidence level for balance sheet pages\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A processed numeric quantiity or numpy.nan value depending on string issues  \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value \n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wrapper(df: pd.DataFrame, textract_text: dict, key: str, file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A wrapper function that sequentially calls each cleaning function to fix issues that may arise\n",
    "    post Textract reading (i.e. Column Merging, Row Splitting, Numeric Conversion)\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param df:\n",
    "            Original unfiltered pandas.DataFrame object representing balance sheet figures\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A strucutred DataFrame that has been uniformly created from the raw Textract table\n",
    "    \"\"\"\n",
    "    \n",
    "    # re-assign dataframe of balance sheet after cleanse\n",
    "    df = column_purge(df)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # COLUMN MERGING (IF NECESSARY)\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # if columns greater than 2, we have a weird data table that needs to be \"merged\"\n",
    "    # NOTE: By construction we never have more than 3 columns present, thanks to our Textract check \n",
    "    if df.columns.size > 2:\n",
    "        df = column_merge(df)\n",
    "        print('\\tWe merged the columns of {}'.format(file))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # ROW SPLIT FOR MERGED ROWS (IF NECESSARY)\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # check for presence of row splits and correct any if found \n",
    "    tempDF = row_split(df, textract_text[key])\n",
    "\n",
    "    # if difference is found in shape, then a transformation was done \n",
    "    if tempDF.shape != df.shape:\n",
    "        print(\"\\tFixed the merged rows for {}\".format(file))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # NUMERIC CONVERSION\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # pass numeric converter to the column to convert string to numerics\n",
    "    tempDF[tempDF.columns[1]] = tempDF[tempDF.columns[1]].apply(cleanNumeric)\n",
    "\n",
    "    # remove any NaN rows post numeric-conversion\n",
    "    postDF = tempDF.dropna().copy()\n",
    "\n",
    "    # check for potential scaler multipler on cash flows (adjust multiplier if possible)\n",
    "    scale = num_scale(textract_text[key])\n",
    "    postDF[postDF.columns[1]] = postDF[postDF.columns[1]].apply(lambda x: x * scale)\n",
    "\n",
    "    print('\\tWe converted to numeric figures for {}'.format(file))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # BALANCE SHEET EXPORTATION\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    print(postDF)\n",
    "    return postDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initiate s3 bucket and corresponding data folder\n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    \n",
    "    pdf_data_folder = 'Output/X-17A-5-PDF-RAW/'\n",
    "    png_data_folder = 'Output/X-17A-5-PNG-RAW/'\n",
    "    \n",
    "    pdf_output_folder = 'Output/X-17A-5-CLEAN-PDFS/'\n",
    "    png_output_folder = 'Output/X-17A-5-CLEAN-PNGS/'\n",
    "\n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    textract = boto3.client('textract')\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # retrieving text JSON file from s3 bucket and store to temp \n",
    "    s3.download_file(bucket, 'Temp/X17A5-TEXT.json', 'temp2.json')\n",
    "\n",
    "    # read data on TEXT-Confidence dictionary\n",
    "    with open('temp2.json', 'r') as f: text_dictionary = json.loads(f.read())  \n",
    "\n",
    "    # remove local files for JSON\n",
    "    os.remove('temp2.json')\n",
    "    \n",
    "    # csv directory with all X-17A-5 balance sheet information \n",
    "    pdf_paths = np.array(session.list_s3_files(bucket, pdf_data_folder))[1:]\n",
    "    png_paths = np.array(session.list_s3_files(bucket, png_data_folder))[1:]\n",
    "    \n",
    "    # iterate through each csv path e.g. ['Output/X-17A-5-PDF-RAW/58056-2014-03-04.csv']\n",
    "    for pdf_csv in pdf_paths:\n",
    "        \n",
    "        fileName = pdf_csv.split('/')[-1]                  # strip filename from each csv\n",
    "        base_file = fileName.split('.')[0]                 # CIK-YYYY-MM-DD base name\n",
    "        print('\\nCleaning the {} file'.format(fileName))\n",
    "        \n",
    "        # download X-17A-5 csv file as a temporary csv file  \n",
    "        s3.download_file(bucket, pdf_csv, 'temp.csv')\n",
    "        df = pd.read_csv('temp.csv')\n",
    "        \n",
    "        # perform cleaning operations on read dataframe\n",
    "        out_df = clean_wrapper(df, text_dictionary, base_file, fileName)\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        out_df.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=pdf_output_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been used\n",
    "        os.remove(fileName)\n",
    "        os.remove('temp.csv')\n",
    "\n",
    "    # iterate through each csv path e.g. ['Output/X-17A-5-PNG-RAW/58056-2014-03-04.csv']\n",
    "    for png_csv in png_paths:\n",
    "        \n",
    "        fileName = png_csv.split('/')[-1]                  # strip filename from each csv\n",
    "        base_file = fileName.split('.')[0]                 # CIK-YYYY-MM-DD base name\n",
    "        print('\\nCleaning the {} file'.format(fileName))\n",
    "        \n",
    "        # download X-17A-5 csv file as a temporary csv file  \n",
    "        s3.download_file(bucket, png_csv, 'temp.csv')\n",
    "        df = pd.read_csv('temp.csv')\n",
    "        \n",
    "        # perform cleaning operations on read dataframe\n",
    "        out_df = clean_wrapper(df, text_dictionary, base_file, fileName)\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        out_df.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=png_output_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been used\n",
    "        os.remove(fileName)\n",
    "        os.remove('temp.csv')\n",
    "        \n",
    "    print('\\nAll .csv files are cleaned and primed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.download_file('ran-s3-systemic-risk', 'Output/X-17A-5-PDF-RAW/91154-2011-03-01.csv', 'temp.csv')\n",
    "# df = pd.read_csv('temp.csv')\n",
    "# os.remove('temp.csv')\n",
    "\n",
    "# # perform cleaning operations on read dataframe\n",
    "# merge(df)\n",
    "\n",
    "# # out_df = clean_wrapper(df, text_dictionary, '91154-2011-03-01', '91154-2011-03-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
