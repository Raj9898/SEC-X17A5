{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.26.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=420b63076ac1fcecb75e0df77512ce9aa2863c384d8885e3c97fa292f05ed793\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/f5/6d/a97dd4f22376d4472d5f4c76c7646876052ff3166b3cf71050\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run on first instance to install required libraries\n",
    "%pip install PyPDF2\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# console and directory access\n",
    "import os\n",
    "import re\n",
    "import time \n",
    "import json\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# interacting with Amazon AWS\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# data reading and exporting  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# parsing SEC website for data  \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# pdf manipulation\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF File Extraction\n",
    "Extract URL links per company filing to download accompaning X-17A-5 files from SEC EDGAR site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseURL(cik:str, file_type:str='X-17A-5') -> str:\n",
    "    \"\"\"\n",
    "    Constructs a base URL for searching for a paritcular SEC filing  \n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: cik (type str)\n",
    "            The CIK number for a registreed broker-dealer (e.g. 1904)\n",
    "        :param: file_type (type str)\n",
    "            The file type looking to parse for a given broker-dealer (e.g. default X-17A-5)\n",
    "            \n",
    "    Return:\n",
    "        :param: url (type str)\n",
    "            A URL string that points to the EDGAR webpage of a registred broker dealer\n",
    "            (e.g. https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1904&type=X-17A-5&dateb=20201231)\n",
    "    \"\"\"\n",
    "    \n",
    "    # forming the SEC search URLs from the select CIK, file type and date range\n",
    "    secFormat = 'https://www.sec.gov/cgi-bin/browse-edgar?'     # SEC base url\n",
    "    dataSelect = 'action=getcompany&CIK={}&type={}&dateb={}1231'    # select params.\n",
    "\n",
    "    # build lookup URLs for the SEC level data (base url)\n",
    "    url = secFormat + dataSelect.format(cik, file_type, datetime.datetime.today().year)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgarParse(url:str):\n",
    "    \"\"\"\n",
    "    Parses the EDGAR webpage of a provided URL and returns a tuple of arrays/lists\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: url (type str) \n",
    "            URL is a string representing a SEC website URL pointing to a CIK for X-17A-5 filings\n",
    "            e.g. https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1904&type=X-17A-5&dateb=20201231\n",
    "    \n",
    "    Return:\n",
    "        :param: filing_dates (type numpy array)\n",
    "            A vector of date strings for all X-17A-5 filings in chronological order, from newest\n",
    "            to oldest in filing date (e.g. ['2020-02-26', '2019-02-28', '2018-03-02'])\n",
    "        :param: archives (type list)\n",
    "            A vector of strings for all sec.gov URL links for each filings in chronological order\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.Response()\n",
    "    \n",
    "    # we try requesting the URL and break only if response object returns status of 200\n",
    "    for _ in range(100):\n",
    "        # requesting HTML data link from the EDGAR search results \n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    \n",
    "    # last check to see if response object is \"problamatic\" e.g. 403\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    # parse the HTML doc string from the response object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') \n",
    "    \n",
    "    # read in HTML tables from the url link provided \n",
    "    try:\n",
    "        # due to web-scrapping non-constant behavior (check against 100 tries)\n",
    "        for _ in range(100):\n",
    "            try: \n",
    "                filings = pd.read_html(url) \n",
    "                break\n",
    "            except urllib.error.HTTPError: pass\n",
    "        \n",
    "        filing_table = filings[2]                           # select the filings (IndexError Flag)\n",
    "        filing_dates = filing_table['Filing Date'].values   # select the filing dates columns\n",
    "\n",
    "        # parse the html-doc string for all instance of < a href= > from the URL \n",
    "        href = [link.get('href') for link in soup.find_all('a')]\n",
    "\n",
    "        # search for all links with Archive in handle, these are the search links for the X-17A-5 filings\n",
    "        archives = ['https://www.sec.gov' + link for link in href if str.find(link, 'Archives') > 0]\n",
    "        \n",
    "        # return a tuple of vectors, the filings dates and the corresponding urls\n",
    "        return filing_dates, archives\n",
    "        \n",
    "    # if there exists no active reports for a given CIK, we flag the error\n",
    "    except IndexError:\n",
    "        print('Currently no filings are present for the firm\\n')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileExtract(archive:str) -> list:\n",
    "    \"\"\"\n",
    "    Parses through the pdf links X-17A-5 pdf files to be saved in an s3 bucket\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: archive (type str)\n",
    "            A vector of strings for all sec.gov URL links for each filings in chronological order\n",
    "\n",
    "    Return:\n",
    "        This function returns a list of pdf url links that point to the EDGAR filing for a specific\n",
    "        broker-dealer at a particular year\n",
    "    \n",
    "    NOTE:   This script makes no effort to weed out amended releases, rather it will default to retaining \n",
    "            information on first published releases via iterative selection from the most recent filing \n",
    "    \"\"\"\n",
    "    \n",
    "    pdf_storage = requests.Response()\n",
    "    \n",
    "    # we try requesting the URL and break only if response object returns status of 200\n",
    "    for _ in range(100):\n",
    "        # data is organized linearly by most recent issue first, we request data from document links\n",
    "        pdf_storage = requests.get(archive, allow_redirects=True)\n",
    "        if pdf_storage.status_code == 200:\n",
    "            break\n",
    "    \n",
    "    # last check to see if response object is \"problamatic\" e.g. 403\n",
    "    if pdf_storage.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    # table from filing detail Edgar table \n",
    "    soup = BeautifulSoup(pdf_storage.text, 'html.parser') \n",
    "\n",
    "    # extracts all link within the filing table, filtering for pdfs\n",
    "    extract_link = [file.get('href') for file in soup.find_all('a')]\n",
    "\n",
    "    # filter for all pdf links from the extracted file links  \n",
    "    pdf_files = [string for string in extract_link if str.find(string, 'pdf') > 0]\n",
    "\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePdfs(files:list) -> PdfFileWriter:\n",
    "    \"\"\"\n",
    "    Combines pdfs files iteratively by page for each of the accompanying SEC filings \n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: files (type List)\n",
    "            A list of pdfs retrieved from filing details for each broker-detal in Edgar's website\n",
    "            e.g. https://www.sec.gov/Archives/edgar/data/1904/000000190420000002/0000001904-20-000002-index.htm\n",
    "\n",
    "    Return:\n",
    "        :param: pdfWriter (type PdfFileWriter)\n",
    "            A PdfFileWriter object that serves as a container to store each of the select pdf files from our\n",
    "            list into a larger merged pdf \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize a pdf object to be store pdf pages\n",
    "    pdfWriter = PdfFileWriter()\n",
    "    \n",
    "    for pdf in files:\n",
    "        pdf_file = 'https://www.sec.gov' + pdf \n",
    "        \n",
    "        # we try requesting the URL and break only if response object returns status of 200\n",
    "        for _ in range(100):\n",
    "            # request the specific pdf file from the the SEC\n",
    "            pdf_storage = requests.get(pdf_file, allow_redirects=True)\n",
    "            if pdf_storage.status_code == 200:\n",
    "                break\n",
    "\n",
    "        # last check to see if response object is \"problamatic\" e.g. 403\n",
    "        if pdf_storage.status_code != 200:\n",
    "            continue\n",
    "        \n",
    "        # save PDF contents to local file location \n",
    "        open('temp.pdf', 'wb').write(pdf_storage.content)\n",
    "        \n",
    "        # read pdf file as PyPDF2 object\n",
    "        pdf = PdfFileReader('temp.pdf', strict=False) \n",
    "        nPages = pdf.getNumPages()          # detemine the number of pages in pdf\n",
    "        \n",
    "        # add the pages from the document as specified \n",
    "        for page_num in np.arange(nPages):\n",
    "            pdfWriter.addPage(pdf.getPage(page_num))\n",
    "    \n",
    "    # remove temporary file on local directory\n",
    "    if os.path.isfile('./temp.pdf'):\n",
    "        os.remove('temp.pdf')\n",
    "    \n",
    "    return pdfWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIK naming conventions for broker-dealers in order of total asset, we run these in batches\n",
    "# (according to https://www.auditanalytics.com/products/sec/broker-dealers)\n",
    "top_9_big_banks = ['782124', '42352', '68136', '91154', '72267', '1224385', '851376', '853784', '58056']\n",
    "top_18_big_banks = ['318336', '356628', '895502', '877559', '922792', '230611', '890203', '920417', '87634']\n",
    "top_27_big_banks = ['26617', '1616344', '803012', '1591458', '1215680', '1146184', '867626', '1261467', '29648']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(top_9_big_banks + top_18_big_banks + top_27_big_banks)\n",
    "b = np.array(['87634', '853784', '806135','851376','782124','769993','318336','68136','1070766','895502','48966', '860220',\n",
    "     '808379','58056','754542','200565','753835','1346817','874362','91154','703004','1101180','1230485','276523',\n",
    "     '1675365'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_big_banks = b[~np.isin(b, a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['806135', '769993', '1070766', '48966', '860220', '808379',\n",
       "       '754542', '200565', '753835', '1346817', '874362', '703004',\n",
       "       '1101180', '1230485', '276523', '1675365'], dtype='<U7')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_big_banks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main File Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading X-17A-5 files for LEHMAN BROTHERS INC.  - CIK (89562)\n",
      "\thttps://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=89562&type=X-17A-5&dateb=2021\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2008-01-29\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2007-01-29\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2006-01-30\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2005-01-31\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2004-01-29\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2003-01-29\n",
      "\tSaved X-17A-5 files for LEHMAN BROTHERS INC.  on 2002-01-29\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    output_folder = 'Input/X-17A-5/'\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # check available pdfs stored within desired output-folder\n",
    "    s3_path = session.list_s3_files(bucket, output_folder)\n",
    "    \n",
    "    # retrieving CIK-Dealers JSON file from s3 bucket\n",
    "    s3.download_file(bucket, 'Temp/CIKandDealers.json', 'temp.json')\n",
    "\n",
    "    # read all CIK and Dealer name information from storage\n",
    "    with open('temp.json', 'r') as f: cik2brokers = json.loads(f.read())\n",
    "\n",
    "    # remove local file after it has been created (variable is stored in memory)\n",
    "    os.remove('temp.json')\n",
    "    # ==============================================================================\n",
    "    \n",
    "    # iterate through a list of CIKs, for full list we have cik2brokers['broker-dealers'].keys()\n",
    "    for cik_id in ['89562']:\n",
    "        try:\n",
    "            companyName = cik2brokers['broker-dealers'][cik_id]\n",
    "\n",
    "            # build lookup URLs for the SEC level data (base url)\n",
    "            url = baseURL(cik_id)\n",
    "\n",
    "            # return the filing dates and archived url's for each SEC company (TypeError if return None)\n",
    "            response = edgarParse(url)\n",
    "\n",
    "            if type(response) is not None:\n",
    "                filing_dates, archives = response\n",
    "\n",
    "                # logging info for when files are being downloaded\n",
    "                print('Downloading X-17A-5 files for {} - CIK ({})'.format(companyName, cik_id))\n",
    "                print('\\t{}'.format(url))\n",
    "\n",
    "                 # itterate through each of the pdf URLs corresponding to archived contents\n",
    "                for i, pdf_url in enumerate(archives):\n",
    "\n",
    "                    # filing date in full yyyy-MM-dd format\n",
    "                    date = filing_dates[i]\n",
    "\n",
    "                    # Construct filename & pdf file naming convention (e.g. filename = 1904-2020-02-26.pdf) \n",
    "                    file_name = str(cik_id) + '-' + date + '.pdf'\n",
    "                    pdf_name = output_folder + file_name\n",
    "\n",
    "                    if pdf_name in s3_path: \n",
    "                        print('\\tAll files for {} are downloaded'.format(companyName))\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        # extract all acompanying pdf files, merging all to one large pdf\n",
    "                        pdf_files = fileExtract(pdf_url)\n",
    "                        \n",
    "                        if len(pdf_files) > 0:\n",
    "                            concatPdf = mergePdfs(pdf_files)\n",
    "\n",
    "                            # open file and save to local instance\n",
    "                            with open(file_name, 'wb') as f:\n",
    "                                concatPdf.write(f)\n",
    "                                f.close()\n",
    "\n",
    "                            # save contents to AWS S3 bucket\n",
    "                            with open(file_name, 'rb') as data:\n",
    "                                s3.upload_fileobj(data, bucket, pdf_name)\n",
    "\n",
    "                            # remove local file after it has been created\n",
    "                            os.remove(file_name)\n",
    "                            print('\\tSaved X-17A-5 files for {} on {}'.format(companyName, date))\n",
    "\n",
    "                        else:\n",
    "                            print('\\tNo files found for {} on {}'.format(companyName, date))\n",
    "\n",
    "            # identify error in the event edgar parse (web-scrapping returns None)\n",
    "            else: print('ERROR: Unable to download X-17A-5 files for {} - CIK ({})'.format(companyName, cik_id))\n",
    "            \n",
    "        except KeyError:\n",
    "            print('{} is not a valid broker-dealer\\n'.format(cik_id))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# # Amazon Textract client and Sagemaker session\n",
    "# s3 = boto3.client('s3')\n",
    "# session = Session()\n",
    "\n",
    "# bucket = 'ran-s3-systemic-risk'\n",
    "# output_folder = 'Input/X-17A-5/'\n",
    "\n",
    "# # ==============================================================================\n",
    "# # check available pdfs stored within desired output-folder\n",
    "# s3_path = session.list_s3_files(bucket, output_folder)\n",
    "\n",
    "# # retrieving CIK-Dealers JSON file from s3 bucket\n",
    "# s3.download_file(bucket, 'Temp/CIKandDealers.json', 'temp.json')\n",
    "\n",
    "# # read all CIK and Dealer name information from storage\n",
    "# with open('temp.json', 'r') as f: cik2brokers = json.loads(f.read())\n",
    "\n",
    "# # remove local file after it has been created (variable is stored in memory)\n",
    "# os.remove('temp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = list(map(int, cik2brokers['broker-dealers'].keys()))\n",
    "# a.sort()\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cik2brokers['years-covered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
