{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smart_open\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting minecart\n",
      "  Downloading minecart-0.3.0-py3-none-any.whl (23 kB)\n",
      "Collecting textract-trp\n",
      "  Downloading textract_trp-0.1.3-py3-none-any.whl (5.8 kB)\n",
      "Collecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "Installing collected packages: pdfminer3k, textract-trp, smart-open, minecart\n",
      "Successfully installed minecart-0.3.0 pdfminer3k-1.3.4 smart-open-5.1.0 textract-trp-0.1.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run on first instance to install required libraries\n",
    "%pip install smart_open minecart textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Asynchronous Textract Script (requesting Job)\n",
    "**Content modified from Amazon AWS Textract repository (refer to [URL](https://github.com/aws-samples/amazon-textract-code-samples/blob/master/python/12-pdf-text.py) below)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startJob(s3BucketName:str, objectName:str) -> str:\n",
    "    \"\"\"\n",
    "    Starts a Textract job on AWS server \n",
    "    \"\"\"\n",
    "    # initialize return and client object\n",
    "    response = None                         \n",
    "    client = boto3.client('textract')\n",
    "    \n",
    "    # issue response to AWS to start Textract job for table analysis \n",
    "    response = client.start_document_analysis(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3BucketName,     # location of data to be read from s3 bucket \n",
    "                'Name': objectName}},       # file name to be read from Textract  \n",
    "        FeatureTypes=['FORMS', 'TABLES']    # selecting FORMS (key-values) and TABLES from the OCR\n",
    "    )\n",
    "    \n",
    "    # return response job ID for service\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isJobComplete(jobId:str) -> str:\n",
    "    \"\"\"\n",
    "    Tracks the completion status of the Textract job when queued\n",
    "    \"\"\"\n",
    "    # allow for interal sleep timer (efficiency)\n",
    "    time.sleep(1)                               \n",
    "    \n",
    "    client = boto3.client('textract')\n",
    "    response = client.get_document_analysis(JobId=jobId)\n",
    "    \n",
    "    # job-status of the response object \n",
    "    status = response[\"JobStatus\"]                        \n",
    "    print(\"Job status: {}\".format(status))\n",
    "    \n",
    "    # if job still running check current status every 5 seconds\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        \n",
    "        # time lag before reporting status\n",
    "        time.sleep(5)                                         \n",
    "        response = client.get_document_analysis(JobId=jobId)\n",
    "        \n",
    "        # job-status of the response object\n",
    "        status = response[\"JobStatus\"]                        \n",
    "        print(\"Job status: {}\".format(status))\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobResults(jobId:str) -> list:\n",
    "    \"\"\"\n",
    "    Returns the contents of the Textract job, after job status is completed\n",
    "    \"\"\"\n",
    "    # initialize list object to track pages read\n",
    "    pages = []                    \n",
    "\n",
    "    client = boto3.client('textract')\n",
    "    response = client.get_document_analysis(JobId=jobId)\n",
    "    \n",
    "    # add first page response to list (length of pages will be arbitrary) \n",
    "    pages.append(response)      \n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    \n",
    "    # if NextToken present we have a pointer to page (e.g. Response -> Page) \n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "    \n",
    "    # iterate through the pages and append to response figure (assuming nextToken not None)\n",
    "    while(nextToken):\n",
    "        response = client.get_document_analysis(JobId=jobId, NextToken=nextToken)\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        \n",
    "        # move along linked-list for presence of NextToken response\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "    \n",
    "    # return amalgamation of all page responses \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runJob(bucket:str, key:str):\n",
    "    \"\"\"\n",
    "    Function designed to call an AWS Textract job (implements helper function above)\n",
    "    \"\"\"\n",
    "    jobId = startJob(bucket, key)   \n",
    "    print(\"Started job with id: {}\".format(jobId))\n",
    "\n",
    "    # if job is complete on AWS return page responses \n",
    "    if(isJobComplete(jobId)):\n",
    "        response = getJobResults(jobId)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Extraction Scripts (Key-Value Pairs)\n",
    "**The content was modified from AWS to extract key-value pairs in form documents from Block objects that are stored in a map. (refer to [URL](https://docs.aws.amazon.com/textract/latest/dg/examples-extract-kvp.html))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_block(key_block, value_map):\n",
    "    \"\"\"\n",
    "    Retrieving value block from AWS textract job, this contains the value text \n",
    "    \"\"\"\n",
    "    # iterate through the key blocks in the FORM relationships (should have a VALUE and CHILD type, n=2)\n",
    "    for relationship in key_block['Relationships']:\n",
    "        \n",
    "        # if our key block object type is a VALUE we examine the relationship ID\n",
    "        # NOTE WE SHOULD HAVE ONLY ONE ID FOR THE VALUE RELATIONSHIP TYPE\n",
    "        if relationship['Type'] == 'VALUE':\n",
    "            \n",
    "            # singular ID item stored in list object (return value block object)\n",
    "            for value_id in relationship['Ids']:\n",
    "                value_block = value_map[value_id]\n",
    "            \n",
    "    # return all corresponding value series\n",
    "    return value_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kv_relationship(key_map, value_map, block_map):\n",
    "    \"\"\"\n",
    "    Retrieving the Key-Value relationship from FORM OCR Textract \n",
    "    \"\"\"\n",
    "    # initialize key-map dictionary for lineitems and corresponding accounting values\n",
    "    key_value_map = {}\n",
    "    \n",
    "    # unpack the key_map to retrieve the block id and key names\n",
    "    for block_id, key_block in key_map.items():\n",
    "\n",
    "        # retrieve value block provided the key_block from each block id\n",
    "        value_block = find_value_block(key_block, value_map)\n",
    "\n",
    "        # get text value from key and value blocks\n",
    "        key = get_text(key_block, block_map)\n",
    "        val = get_text(value_block, block_map)\n",
    "        \n",
    "        # map the key and value pairs (e.g. {'Total Assets':'$ 189,232'})\n",
    "        key_value_map[key] = val\n",
    "        \n",
    "    return key_value_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(result, blocks_map):\n",
    "    \"\"\"\n",
    "    Retrieving text values from given block object\n",
    "    \"\"\"\n",
    "    # initialize container for text\n",
    "    text = ''\n",
    "    \n",
    "    # if relationships header exists we can extract CHILD header\n",
    "    if 'Relationships' in result:\n",
    "        \n",
    "        # relationship maps to a list (iterate through to reveal a dictionary)\n",
    "        # e.g. 'Relationships' : [{'Type' : 'CHILD', 'Ids': ['e2b3b12f-ebb7-4f6e-914f-97b315672530']}]\n",
    "        for relationship in result['Relationships']:\n",
    "            \n",
    "            # if relationship type is CHILD we explore job-id (indicates good fit)\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                \n",
    "                # iterate through Ids list\n",
    "                for child_id in relationship['Ids']:\n",
    "                    \n",
    "                    # select corresponding CHILD_ID from block map, this is sub-dictionary\n",
    "                    word = blocks_map[child_id]\n",
    "                    \n",
    "                    # if block type is a word then we append with a space\n",
    "                    if word['BlockType'] == 'WORD':\n",
    "                        text += word['Text'] + ' '\n",
    "                        \n",
    "                    # if block type is a selection element (e.g. an option button/mark)\n",
    "                    # note we treat these cases with an X to denote an optional field \n",
    "                    if word['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                        if word['SelectionStatus'] == 'SELECTED':\n",
    "                            text += 'X '    \n",
    "    \n",
    "    # return string corresponding with word \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Wrapper Functions\n",
    "**The scripts perform an OCR job from AWS Textract, and returning well formated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trp2df(table:trp.Table) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to convert a trp table into a dataframe Complexity -> O(n^2) approx.\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param table: (type trp.Table)\n",
    "            A trp table object parsed from a pdf using AWS Textract  \n",
    "    \n",
    "    Output\n",
    "        :return: type pandas.DataFrame\n",
    "            A DataFrame object that is constructed by deconstructed a Textract trp table\n",
    "    \"\"\"\n",
    "    N = len(table.rows)               # number of rows in table\n",
    "    M = len(table.rows[0].cells)      # number of columns in table\n",
    "    arr = [0]*N                       # initialize matrix container\n",
    "    \n",
    "    # iterate through each row within the provided table\n",
    "    for row in np.arange(N):\n",
    "        \n",
    "        # strip the text from the cell references to construct (N X M) matrix\n",
    "        arr[row] = [table.rows[row].cells[col].text.strip() for col in np.arange(M)]    # move column-wise to get text\n",
    "        \n",
    "    return pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dollar_sign(row):\n",
    "    \"\"\"\n",
    "    Determines if there exists a dollar sign present within a given row\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: row (type nd.array)\n",
    "            A given row from a dataframe\n",
    "    Return:\n",
    "        This function returns a bollean sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def re_dollar_check(x):\n",
    "        # we search for the presence of a dollar sign ($) in a string followed by character\n",
    "        dollar_search = re.search('\\$[^\\]]+', x, flags=re.IGNORECASE)\n",
    "        \n",
    "        if dollar_search is not None: return True\n",
    "        return False\n",
    "\n",
    "    vFunc = np.vectorize(re_dollar_check)      # vectorize function to apply to numpy array\n",
    "    cleanValue = vFunc(row)                    # apply vector function\n",
    "    \n",
    "    # search each vector return for presence of True\n",
    "    # if True we have found a dollar '$' character\n",
    "    series = np.argwhere(cleanValue == True)\n",
    "    if len(series) > 0: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTable(response:list) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dataframe, by searching for tables\n",
    "     ------------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param response: (type list)\n",
    "            An AWS Textract response object corresponding to pages of a given document page \n",
    "    \n",
    "    Output\n",
    "        :return: type tuple\n",
    "            A (3x1) tuple is returned, storing the concated dataframe at the first index, and the accompanying \n",
    "            trp page objects for where the balance sheet was determined to reside at the second index\n",
    "    \"\"\"\n",
    "    \n",
    "    catDF = []          # in the event multiple tables detected on one page (concat them)\n",
    "    page_series = []    # keep track of page objects where balance sheet was flagged\n",
    "    page_nums = []\n",
    "    page_count = 0\n",
    "    \n",
    "    # format the Textract response type \n",
    "    doc = trp.Document(response)\n",
    "    \n",
    "    # iterate through document pages\n",
    "    for page in doc.pages:\n",
    "        \n",
    "        # itterate through page tables\n",
    "        for table in page.tables: \n",
    "            \n",
    "            # convert trp-table into dataframe object\n",
    "            df = trp2df(table)\n",
    "            \n",
    "            # remove columns that are completely empty\n",
    "            empty_cols = [col for col in df.columns if (df[col] == '').all()]\n",
    "            df = df.drop(empty_cols, axis=1)\n",
    "  \n",
    "            # number of columns in dataframe\n",
    "            n = df.columns.size\n",
    "            \n",
    "            # reset the column names (avoid the column names)\n",
    "            df.columns = np.arange(n)\n",
    "            \n",
    "            ##############################################################\n",
    "            #                           NOTES\n",
    "            #          a good dataframe should have 2-3 columns\n",
    "            #      anything more or less is a reading error we ignore\n",
    "            ##############################################################\n",
    "            \n",
    "            # if the dataframe has more than 3 columns then we most likley have an issue in parsing\n",
    "            if n > 3:\n",
    "                pass\n",
    "            \n",
    "            elif n > 1:\n",
    "                \n",
    "                ##############################\n",
    "                # Balance Sheet Assummptions\n",
    "                ##############################\n",
    "                \n",
    "                # this is the column with all line items (e.g. Cash, Total Assets, Total Liabilites)\n",
    "                lineIndex = df.columns[0]\n",
    "\n",
    "                # check for the word \"cash\" in a string at the begining, ignoring case sensitivity (asset check)\n",
    "                assetCheck = df[lineIndex].str.contains('^Cash', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "                # check for the word \"Liabilities\" in a string at the end, ignoring case sensitivity (liability check)\n",
    "                debtCheck1 = df[lineIndex].str.contains('Liabilities$|^Liabilities', regex=True, flags=re.IGNORECASE)\n",
    "                debtCheck2 = df[lineIndex].str.contains('Liability$|^Liability', regex=True, flags=re.IGNORECASE)\n",
    "                \n",
    "                # check for the presence of $ sign, we assume the balance sheet items should have at least one $ sign\n",
    "                # this check is used to avoid reading the table of contents, which was flagged in prior reads\n",
    "                dollarCheck = df[df.columns[1:]].apply(check_dollar_sign, axis=1)\n",
    "                \n",
    "                ##############################\n",
    "                # Balance Sheet Determination\n",
    "                ##############################\n",
    "\n",
    "                # check if the key words have been found \n",
    "                check1 = df[assetCheck | debtCheck1 | debtCheck2].empty      # check for line item terms\n",
    "                check2 = df[dollarCheck == True].empty                       # check for presence of '$' sign  \n",
    "                check3 = df[debtCheck1 == True].empty                        # debt check for Liabilities\n",
    "                check4 = df[debtCheck2 == True].empty                        # debt check for Liability \n",
    "                \n",
    "                # make sure the cash term appears toward the top of the balance sheet\n",
    "                if np.argmax(assetCheck==True) < assetCheck.shape[0]/2:\n",
    "                \n",
    "                    # if either asset term or liability term is found, with a $ sign we append the dataframe\n",
    "                    if not check1 and not check2:\n",
    "\n",
    "                        # we append pages since asset and liablility tables are often seperate\n",
    "                        # there is no loss of generality if asset and liability terms are in one table\n",
    "                        catDF.append(df)                \n",
    "\n",
    "                        # we want to keep track of pages that have been deemed as balance sheet\n",
    "                        if page not in page_series:\n",
    "                            page_series.append(page)   # only append if page isn't already recorded\n",
    "                            page_nums.append(page_count)\n",
    "\n",
    "                        if not check3 or not check4:\n",
    "                            # if liability table was found on the first iteration we simply concat data frames \n",
    "                            return (pd.concat(catDF), page_series, page_nums)\n",
    "                    \n",
    "        page_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPNG(pages:list, png_path:str, bucket='ran-s3-systemic-risk') -> tuple:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dataframe, by searching for tables\n",
    "     ------------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param response: (type list)\n",
    "            \n",
    "    \n",
    "    Output\n",
    "        :return: type tuple\n",
    "          \n",
    "    \"\"\"\n",
    "    subfolder = png_path.split('/')[-2]      # subfolder where PNG files are stored\n",
    "    \n",
    "    # construct PNG directories with relevant pages\n",
    "    textract_paths = [png_path + subfolder + '-p{}.png'.format(idx) for idx in pages]\n",
    "    \n",
    "    catDF = []          # in the event multiple pages we concat them\n",
    "    \n",
    "    # path iterates through each png image matching the page numbers found in PDFs\n",
    "    for path in textract_paths:\n",
    "        \n",
    "        try:\n",
    "            # temporary data frame object for balance sheet information\n",
    "            res = runJob(bucket, path)\n",
    "            \n",
    "            # if Textract job did not fail we continue extraction\n",
    "            if res[0]['JobStatus'] != 'FAILED':\n",
    "\n",
    "                # format the Textract response type \n",
    "                doc = trp.Document(res)\n",
    "\n",
    "                # iterate through document pages\n",
    "                for page in doc.pages:\n",
    "                    \n",
    "                    # itterate through page tables\n",
    "                    for table in page.tables: \n",
    "\n",
    "                        # convert trp-table into dataframe object\n",
    "                        df = trp2df(table)\n",
    "                        \n",
    "                        # remove columns that are completely empty\n",
    "                        empty_cols = [col for col in df.columns if (df[col] == '').all()]\n",
    "                        df = df.drop(empty_cols, axis=1)\n",
    "\n",
    "                        # number of columns in dataframe\n",
    "                        n = df.columns.size\n",
    "\n",
    "                        # reset the column names (avoid the column names)\n",
    "                        df.columns = np.arange(n)\n",
    "                        \n",
    "                        ##############################################################\n",
    "                        #                           NOTES\n",
    "                        #          a good dataframe should have 2-3 columns\n",
    "                        #      anything more or less is a reading error we ignore\n",
    "                        ##############################################################\n",
    "\n",
    "                        # if the dataframe has more than 3 columns then we most likley have an issue in parsing\n",
    "                        if n > 3:\n",
    "                            pass \n",
    "\n",
    "                        elif n > 1:\n",
    "\n",
    "                            ##############################\n",
    "                            # Balance Sheet Assummptions\n",
    "                            ##############################\n",
    "\n",
    "                            # this is the column with all line items (e.g. Cash, Total Assets, Total Liabilites)\n",
    "                            lineIndex = df.columns[0]\n",
    "\n",
    "                            # check for the word \"cash\" in a string at the begining, ignoring case sensitivity \n",
    "                            assetCheck = df[lineIndex].str.contains('^Cash', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "                            # check for the word \"Liabilities\" in a string at the end, ignoring case sensitivity \n",
    "                            debtCheck1 = df[lineIndex].str.contains('Liabilities$|^Liabilities', \n",
    "                                                                    regex=True, flags=re.IGNORECASE)\n",
    "                            debtCheck2 = df[lineIndex].str.contains('Liability$|^Liability', \n",
    "                                                                    regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "                            # check for the presence of $ sign, we assume the balance sheet items should have \n",
    "                            # this check is used to avoid reading the table of contents, which was flagged in prior reads\n",
    "                            dollarCheck = df[df.columns[1:]].apply(check_dollar_sign, axis=1)\n",
    "                        \n",
    "                            ##############################\n",
    "                            # Balance Sheet Determination\n",
    "                            ##############################\n",
    "                            \n",
    "                            # make sure the cash term appears toward the top of the balance sheet\n",
    "                            if np.argmax(assetCheck==True) < assetCheck.shape[0]/2:\n",
    "                    \n",
    "                                # check if the key words have been found \n",
    "                                check1 = df[assetCheck | debtCheck1 | debtCheck2].empty      # check for line item terms\n",
    "                                check2 = df[dollarCheck == True].empty                       # check for presence of '$' sign  \n",
    "                                check3 = df[debtCheck1 == True].empty                        # debt check for Liabilities\n",
    "                                check4 = df[debtCheck2 == True].empty                        # debt check for Liability \n",
    "\n",
    "                                # if either asset term or liability term is found, with a $ sign we append the dataframe\n",
    "                                if not check1 and not check2:\n",
    "\n",
    "                                    # we append pages since asset and liablility tables are often seperate\n",
    "                                    # there is no loss of generality if asset and liability terms are in one table\n",
    "                                    catDF.append(df)                \n",
    "\n",
    "                                    if not check3 or not check4:\n",
    "                                        # if liability table was found on the first iteration we simply concat data frames \n",
    "                                        return pd.concat(catDF)\n",
    "        \n",
    "        # broad exeption to catch Textract parsing errors\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # default return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readForm(doc_pages:list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dictionary, by searching for key value pairs\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param doc_pages: (type list)\n",
    "            TRP page(s) for a AWS Textract response object corresponding to pages of a given document page \n",
    "    \n",
    "    Output\n",
    "        :return: type dict\n",
    "            A python dictionary that maps KEYS (line items) with VALUES (corresponding records) for broker\n",
    "            dealers balance sheet (e.g. {'Cash and cash equivalents : $ 12,513})\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing dictionary maps for KEY and VALUE pairs\n",
    "    key_map = {}\n",
    "    value_map = {}\n",
    "    block_map = {}\n",
    "\n",
    "    # iterate through document pages\n",
    "    for page in doc_pages:\n",
    "\n",
    "        # itterate through page tables\n",
    "        for block in page.blocks: \n",
    "\n",
    "            # store the block id in map to retrive information later\n",
    "            block_id = block['Id']\n",
    "            block_map[block_id] = block\n",
    "\n",
    "            # if Key-value set has been seen we deconstruct each KEY and VALUE map\n",
    "            if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "\n",
    "                # if KEY is labeled as entity type then we found Key, else we found VALUE\n",
    "                if 'KEY' in block['EntityTypes']:\n",
    "                    key_map[block_id] = block\n",
    "                else:\n",
    "                    value_map[block_id] = block\n",
    "    \n",
    "    # convert block objects to text dictionary map\n",
    "    return get_kv_relationship(key_map, value_map, block_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readText(doc_pages:list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to transform AWS Textract object to a dictionary of text values and confidence \n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param doc_pages: (type list)\n",
    "            TRP page(s) for a AWS Textract response object corresponding to pages of a given document page\n",
    "    \n",
    "    Output\n",
    "        :return: type dict\n",
    "            A python dictionary that maps TEXT (line items) with corresponding confidence figures as reported\n",
    "            by AWS Textract object (e.g. {'Cash and cash equivalents : 99.97891})\n",
    "    \"\"\"\n",
    "    # initializing dictionary maps for text\n",
    "    text_map = {}\n",
    "    \n",
    "    # iterate through document pages\n",
    "    for page in doc_pages:\n",
    "        \n",
    "        # itterate through page tables\n",
    "        for block in page.blocks: \n",
    "            \n",
    "            # if our block type is a line, we map the line text and confidence\n",
    "            if block['BlockType'] == \"LINE\":\n",
    "                text_map[block['Text']] = block['Confidence']\n",
    "    \n",
    "    # return completed text to confidence map\n",
    "    return text_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Balance Sheet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textractParse(pdf_path:str, png_path:str, bucket:str) -> dict:\n",
    "    \"\"\"\n",
    "    Function runs a Textract job and saves Balance Sheet information to .csv file in s3 folder \n",
    "    \"\"\"\n",
    "    errors = ''\n",
    "    \n",
    "    # temporary data frame object for balance sheet information\n",
    "    res = runJob(bucket, pdf_path)\n",
    "    \n",
    "    # if Textract job did not fail we continue extraction\n",
    "    if res[0]['JobStatus'] != 'FAILED':\n",
    "\n",
    "        # perform OCR and return balance sheet with corresponding page object(s)\n",
    "        tb_response = readTable(res)           \n",
    "        \n",
    "        # checks for type of return, if none then we log an error\n",
    "        if type(tb_response) == tuple:\n",
    "            \n",
    "            # deconstruct the table response tuple into dataframe and page object parts\n",
    "            df1, page_obj, page_num = tb_response\n",
    "            print('\\nPage number(s) for extraction in PNG are {}\\n'.format(page_num))\n",
    "            \n",
    "            # try to extract from a PNG (we can still return a None here)\n",
    "            df2 = readPNG(page_num, png_path)\n",
    "            \n",
    "            # provided balance sheet page number we select FORM and TEXT data\n",
    "            forms_data = readForm(page_obj)      \n",
    "            text_data = readText(page_obj)        \n",
    "            \n",
    "            print('\\nTextract-PDF dataframe')\n",
    "            print(df1)\n",
    "            \n",
    "            print('\\nTextract-PNG dataframe')\n",
    "            print(df2)\n",
    "            \n",
    "            return (df1, df2, forms_data, text_data, None)\n",
    "        else:\n",
    "            error = 'No Balance Sheet found, or parsing error'\n",
    "            return (None, None, None, None, error)\n",
    "    else:\n",
    "        error = 'Could not parse, JOB FAILED'\n",
    "        return (None, None, None, None, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main File Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing OCR for 276523-2006-08-08.csv\n",
      "Started job with id: f0f5f2b3fc707d8f65242f43f059bec5760f774b79c55cc92093dbcf2afbe5e5\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "Resultset page recieved: 2\n",
      "Resultset page recieved: 3\n",
      "Resultset page recieved: 4\n",
      "Resultset page recieved: 5\n",
      "False False False False\n",
      "                                                    0            1         2\n",
      "0                           Cash and cash equivalents               $ 63,380\n",
      "1             Short term investments. at market value                 19,711\n",
      "2   Receivable from brokers. dealers, and clearing...                  6,442\n",
      "3          Securities owned, at market value (note 4)                     39\n",
      "4   Furniture, equipment, and leasehold improvemen...  accumulated       311\n",
      "5                  Receivable from affiliate (note 6)                  2,074\n",
      "6                                        Other assets                    411\n",
      "7                                        Total assets               $ 92,368\n",
      "8                            Liabilities and Member's       Equity          \n",
      "9                                        Liabilities:                       \n",
      "10          Accrued compensation and related benefits                $ 6,424\n",
      "11  Liability associated with exit activity (note 11)                  1,565\n",
      "12                    Payables to affiliates (note 6)                    202\n",
      "13             Other liabilities and accrued expenses                    892\n",
      "14                                  Total liabilities                  9,083\n",
      "15                                    Member's equity                 83,285\n",
      "16              Total liabilities and member's equity               $ 92,368\n",
      "\n",
      "Page number(s) for extraction in PNG are [4]\n",
      "\n",
      "Started job with id: 8c083c4acb3f2b35e6457ef7f955eede63049517a43c7beb9985474d0a6c02af\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "\n",
      "Textract-PDF dataframe\n",
      "                                                    0            1         2\n",
      "0                           Cash and cash equivalents               $ 63,380\n",
      "1             Short term investments. at market value                 19,711\n",
      "2   Receivable from brokers. dealers, and clearing...                  6,442\n",
      "3          Securities owned, at market value (note 4)                     39\n",
      "4   Furniture, equipment, and leasehold improvemen...  accumulated       311\n",
      "5                  Receivable from affiliate (note 6)                  2,074\n",
      "6                                        Other assets                    411\n",
      "7                                        Total assets               $ 92,368\n",
      "8                            Liabilities and Member's       Equity          \n",
      "9                                        Liabilities:                       \n",
      "10          Accrued compensation and related benefits                $ 6,424\n",
      "11  Liability associated with exit activity (note 11)                  1,565\n",
      "12                    Payables to affiliates (note 6)                    202\n",
      "13             Other liabilities and accrued expenses                    892\n",
      "14                                  Total liabilities                  9,083\n",
      "15                                    Member's equity                 83,285\n",
      "16              Total liabilities and member's equity               $ 92,368\n",
      "\n",
      "Textract-PNG dataframe\n",
      "                                                    0            1         2\n",
      "0                           Cash and cash equivalents               $ 63,380\n",
      "1             Short term investments, at market value                 19,711\n",
      "2   Receivable from brokers, dealers, and clearing...                  6,442\n",
      "3          Securities owned, at market value (note 4)                     39\n",
      "4   Furniture, equipment, and leasehold improvemen...  accumulated       311\n",
      "5                  Receivable from affiliate (note 6)                  2,074\n",
      "6                                        Other assets                    411\n",
      "7                                        Total assets               $ 92,368\n",
      "8                            Liabilities and Member's       Equity          \n",
      "9                                        Liabilities:                       \n",
      "10          Accrued compensation and related benefits                $ 6,424\n",
      "11  Liability associated with exit activity (note 11)                  1,565\n",
      "12                    Payables to affiliates (note 6)                    202\n",
      "13             Other liabilities and accrued expenses                    892\n",
      "14                                  Total liabilities                  9,083\n",
      "15                                    Member's equity                 83,285\n",
      "16              Total liabilities and member's equity               $ 92,368\n",
      "{'WELLS FARGO SECURITIES, LLC': 99.79641723632812, 'Statement of Financial Condition': 99.90912628173828, 'December 31, 2005': 99.60372161865234, '(In thousands)': 99.73423767089844, 'Assets': 99.98014831542969, 'Cash and cash equivalents': 91.20899200439453, '$': 95.36679077148438, '63,380': 98.90155029296875, 'Short term investments. at market value': 96.14716339111328, '19,711': 99.64421844482422, 'Receivable from brokers. dealers, and clearing organizations (note 3)': 94.574951171875, '6,442': 99.80035400390625, 'Securities owned, at market value (note 4)': 96.17324829101562, '39': 99.961181640625, 'Furniture, equipment, and leasehold improvements, at cost, less accumulated': 95.11461639404297, 'depreciation and amortization of $1,662 (note 5)': 99.15733337402344, '311': 99.87916564941406, 'Receivable from affiliate (note 6)': 99.71543884277344, '2,074': 99.82704162597656, 'Other assets': 99.90467071533203, '411': 99.64934539794922, 'Total assets': 99.94158172607422, '92,368': 99.75897979736328, \"Liabilities and Member's Equity\": 99.94054412841797, 'Liabilities:': 97.90596008300781, 'Accrued compensation and related benefits': 99.9814453125, '6,424': 99.86283111572266, 'Liability associated with exit activity (note 11)': 99.86707305908203, '1,565': 99.9119644165039, 'Payables to affiliates (note 6)': 99.2813949584961, '202': 99.95805358886719, 'Other liabilities and accrued expenses': 99.97012329101562, '892': 99.89617919921875, 'Total liabilities': 99.94499969482422, '9,083': 99.62391662597656, \"Member's equity\": 99.9350814819336, '83,285': 99.44788360595703, \"Total liabilities and member's equity\": 99.80240631103516, 'See accompanying notes to financial statements.': 99.65464782714844, '2': 99.47178649902344}\n",
      "-----------------------------------------------------\n",
      "Saved 276523-2006-08-08.csv file to s3 bucket\n",
      "\n",
      "\n",
      "Finished performing OCR on parsed FOCUS reports.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    textract = boto3.client('textract')\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # initiate s3 bucket and corresponding data/output folder\n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    \n",
    "    data_png_folder = 'Input/X-17A-5-PNG-SUBSETS/'\n",
    "    data_pdf_folder = 'Input/X-17A-5-PDF-SUBSETS/'\n",
    "    \n",
    "    output_png_folder = 'Output/X-17A-5-PNG-RAW/'\n",
    "    output_pdf_folder = 'Output/X-17A-5-PDF-RAW/'\n",
    "    \n",
    "    temp_folder = 'Temp/'\n",
    "    \n",
    "    # csv directory where we store balance sheet information \n",
    "    output_png_csvs = np.array(session.list_s3_files(bucket, output_png_folder))\n",
    "    output_pdf_csvs = np.array(session.list_s3_files(bucket, output_pdf_folder))\n",
    "    \n",
    "    # temp directory where JSON files is stored\n",
    "    temp = np.array(session.list_s3_files(bucket, temp_folder))\n",
    "    \n",
    "    # pdf directory where we store the broker-dealer information \n",
    "    pdf_files = np.array(session.list_s3_files(bucket, data_pdf_folder))[1:]\n",
    "    png_files = np.array(session.list_s3_files(bucket, data_png_folder))[1:]\n",
    "    png_file_directory = list(set((map(lambda x: '/'.join(x.split('/')[:-1]), png_files))))\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Load in Temp JSON files if present (FORM, TEXT, ERROR)\n",
    "    # ===========================================================================\n",
    "    \n",
    "    if 'Temp/X17A5-FORMS.json' in temp:\n",
    "        # retrieving downloaded files from s3 bucket\n",
    "        s3.download_file(bucket, 'Temp/X17A5-FORMS.json', 'temp1.json')\n",
    "        \n",
    "        # read data on KEY-VALUE dictionary (i.e Textract FORMS) \n",
    "        with open('temp1.json', 'r') as f: forms_dictionary = json.loads(f.read())\n",
    "        \n",
    "        # remove local files for JSON\n",
    "        os.remove('temp1.json')\n",
    "    else:\n",
    "        forms_dictionary = {}\n",
    "    \n",
    "    if 'Temp/X17A5-TEXT.json' in temp:\n",
    "        # retrieving downloaded files from s3 bucket\n",
    "        s3.download_file(bucket, 'Temp/X17A5-TEXT.json', 'temp2.json')\n",
    "        \n",
    "        # read data on TEXT-Confidence dictionary\n",
    "        with open('temp2.json', 'r') as f: text_dictionary = json.loads(f.read())  \n",
    "            \n",
    "        # remove local files for JSON\n",
    "        os.remove('temp2.json')\n",
    "    else:\n",
    "        text_dictionary = {}\n",
    "    \n",
    "    if 'Temp/ERROR-TEXTRACT.json' in temp:\n",
    "        # retrieving downloaded files from s3 bucket\n",
    "        s3.download_file(bucket, 'Temp/ERROR-TEXTRACT.json', 'temp3.json')\n",
    "        \n",
    "        # read data on errors derived from Textract\n",
    "        with open('temp3.json', 'r') as f: error_dictionary = json.loads(f.read()) \n",
    "            \n",
    "        # remove local files for JSON\n",
    "        os.remove('temp3.json')\n",
    "    else:\n",
    "        error_dictionary = {}\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Perform Textract analysis on PDFs and PNGs\n",
    "    # ===========================================================================\n",
    "    \n",
    "    # e.g. ['Input/X-17A-5-PDF-SUBSETS/42352-2012-02-29-subset.pdf'] otherwise pdf_files (full sample)\n",
    "    select_sample = ['Input/X-17A-5-PDF-SUBSETS/276523-2006-08-08-subset.pdf']\n",
    "\n",
    "    for pdf_paths in select_sample:\n",
    "        \n",
    "        # baseFile name to name export .csv file e.g. 1224385-2004-03-01.csv\n",
    "        basefile = pdf_paths.split('/')[-1].split('-subset')[0]\n",
    "        fileName = basefile + '.csv'\n",
    "        print('\\nPerforming OCR for {}'.format(fileName))\n",
    "        \n",
    "        # if file is not found in directory we extract the balance sheet\n",
    "        # WE LOOK TO AVOID RE-RUNNING OLD TEXTRACT PARSES TO SAVE TIME\n",
    "        if (output_pdf_folder + fileName not in output_pdf_csvs):\n",
    "            \n",
    "            # run Textract OCR job and extract the parsed data \n",
    "            png_paths = data_png_folder + basefile + '/'\n",
    "            df1, df2, forms_data, text_data, error = textractParse(pdf_paths, png_paths, bucket)\n",
    "\n",
    "            # if no error is reported we save FORMS, TEXT, DataFrame\n",
    "            if error is None:\n",
    "\n",
    "                # store accompanying information for JSONs\n",
    "                forms_dictionary[basefile] = forms_data\n",
    "                text_dictionary[basefile]  = text_data\n",
    "                print(text_data)\n",
    "                \n",
    "                # writing data frame to .csv file\n",
    "                df1.to_csv(fileName, index=False)\n",
    "\n",
    "                # save contents to AWS S3 bucket\n",
    "                with open(fileName, 'rb') as data:\n",
    "                    s3.put_object(Bucket=bucket, Key=output_pdf_folder + fileName, Body=data)\n",
    "                \n",
    "                # writing data frame to .csv file extracted from PNG\n",
    "                if df2 is not None:\n",
    "                    df2.to_csv(fileName, index=False)\n",
    "                    \n",
    "                    with open(fileName, 'rb') as data:\n",
    "                        s3.put_object(Bucket=bucket, Key=output_png_folder + fileName, Body=data)\n",
    "    \n",
    "                # remove local file after it has been created\n",
    "                os.remove(fileName)\n",
    "\n",
    "                print('-----------------------------------------------------')\n",
    "                print('Saved {} file to s3 bucket'.format(fileName))\n",
    "            \n",
    "            else:\n",
    "                error_dictionary[basefile] = error\n",
    "                \n",
    "        else:\n",
    "            print('{} has been downloaded'.format(fileName))\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Save JSON files for updated figures (FORM, TEXT, ERROR)\n",
    "    # ===========================================================================\n",
    "    \n",
    "    # write to a JSON file for FORMS \n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/X17A5-FORMS.json', 'w') as file: \n",
    "        json.dump(forms_dictionary, file)\n",
    "        file.close()\n",
    "    \n",
    "    # save contents to AWS S3 bucket\n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/X17A5-FORMS.json', 'rb') as data: \n",
    "        s3.upload_fileobj(data, bucket, 'Temp/X17A5-FORMS.json')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    \n",
    "    # write to a JSON file for TEXT \n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/X17A5-TEXT.json', 'w') as file: \n",
    "        json.dump(text_dictionary, file)\n",
    "        file.close()\n",
    "    \n",
    "    # save contents to AWS S3 bucket\n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/X17A5-TEXT.json', 'rb') as data: \n",
    "        s3.upload_fileobj(data, bucket, 'Temp/X17A5-TEXT.json')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    \n",
    "    # write to a JSON file for FORMS \n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/ERROR-TEXTRACT.json', 'w') as file: \n",
    "        json.dump(error_dictionary, file)\n",
    "        file.close()\n",
    "    \n",
    "    # save contents to AWS S3 bucket\n",
    "    with open('/home/ec2-user/SageMaker/SEC_X17A5/temp/ERROR-TEXTRACT.json', 'rb') as data: \n",
    "        s3.upload_fileobj(data, bucket, 'Temp/ERROR-TEXTRACT.json')\n",
    "\n",
    "    print('\\n\\nFinished performing OCR on parsed FOCUS reports.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started job with id: 563593dd6cd208ba0575b077888947c7f46d4887ae761e6820667d2f92203eac\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "{'*******': 95.68363189697266, 'd': 86.50421905517578}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, 'No Balance Sheet found, or parsing error')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single reading for testing purposes and debugging Textract results e.g. 853784-2002-03-01\n",
    "textractParse('Input/X-17A-5-PDF-SUBSETS/753835-2021-03-15-subset.pdf', \n",
    "              'Input/X-17A-5-PNG-SUBSETS/753835-2021-03-15/', 'ran-s3-systemic-risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # temporary data frame object for balance sheet information\n",
    "# res = runJob('ran-s3-systemic-risk', 'Input/X-17A-5-PDF-SUBSETS/753835-2021-03-15-subset.pdf')\n",
    "\n",
    "# # if Textract job did not fail we continue extraction\n",
    "# if res[0]['JobStatus'] != 'FAILED':\n",
    "\n",
    "#     # perform OCR and return balance sheet with corresponding page object(s)\n",
    "#     tb_response = readTable(res)         \n",
    "    \n",
    "#     # format the Textract response type \n",
    "#     doc = trp.Document(res)\n",
    "    \n",
    "#     # iterate through document pages\n",
    "#     for page in doc.pages:\n",
    "\n",
    "#         # itterate through page tables\n",
    "#         for table in page.tables: \n",
    "#             print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
