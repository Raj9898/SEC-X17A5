{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_data(unstructured_df:pd.DataFrame, cluster_df:pd.DataFrame, col_preserve:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a structured dataset from an unstructured column set\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: unstructured_df (type pandas.DataFrame)\n",
    "            unstuructured pandas dataframe with loose column construction \n",
    "        :param: cluster_df (type pandas.DataFrame)\n",
    "            a pandas dataframe of clustered labels and corresponding line items\n",
    "    Output:\n",
    "        :return: (type pandas DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_df = pd.DataFrame()\n",
    "    label_names = np.unique(cluster_df.Labels.values)\n",
    "    remap = {}\n",
    "    \n",
    "    # assume that the there exists columns 'CIK' and 'Year' for unstructured data\n",
    "    structured_df = unstructured_df[col_preserve]\n",
    "    \n",
    "    for label in label_names:\n",
    "        data = cluster_df[cluster_df['Labels'] == label]['LineItems']     # filter by corresponding cluster\n",
    "        \n",
    "        # we first select all predicted columns, then sum across rows for only numeric figures\n",
    "        selection = unstructured_df[data.values]\n",
    "        \n",
    "        sumV = selection.sum(axis=1, numeric_only=True)\n",
    "        \n",
    "        # we then select rows from the original unstructured dataframe with \n",
    "        # only np.nan and convert sumV index to np.nan\n",
    "        sumV[selection.isnull().all(axis=1)] = np.nan\n",
    "        \n",
    "        # assign dictionary to have labels and matching vector\n",
    "        remap[label] = sumV\n",
    "\n",
    "    structured_df = structured_df.assign(**remap)   \n",
    "    return structured_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probabilites(line_items:np.array, clf_mdl, vec_mdl) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a mapping convention for the machine learning predictions \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: line_items (type numpy.array)\n",
    "            list of all unstructured line item names\n",
    "        :param: clf_mdl (type joblib.obj)\n",
    "            a classification model to convert a line item \n",
    "        :param: vec_mdl (type joblib.obj)\n",
    "            a feature extraction model for string/text data \n",
    "    Output:\n",
    "        :return: (type pandas DataFrame)\n",
    "    \"\"\"\n",
    "    # predict the corresponding class for each line item\n",
    "    prediction = pd.DataFrame(data=clf_mdl.predict(vec_mdl.fit_transform(line_items)), columns=['Predicted Class'])\n",
    "    \n",
    "    # the actual line items that are used as predictors\n",
    "    lines = pd.DataFrame(line_items, columns=['Line Items'])\n",
    "    \n",
    "    # compute the probability for each prediction to the accompanying classes\n",
    "    prediction_probability = pd.DataFrame(data=clf_mdl.predict_proba(vec_mdl.fit_transform(line_items)),\n",
    "                                          columns=clf_mdl.classes_)\n",
    "    \n",
    "    # sum across row, determines total class probability measure \n",
    "    # NOTE: each class is bounded by 0.0-1.0, so total column wise sums can exceed 1.0\n",
    "    prediction_probability['Total Prediction score'] = prediction_probability.sum(axis=1) \n",
    "    \n",
    "    # join the line items to the prediction probabilities\n",
    "    return lines.join(prediction).join(prediction_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_pdf(df:pd.DataFrame, mdl):\n",
    "    \"\"\"\n",
    "    Return a dataframe for a company showcasing its column names, the predicted class and the original values.\n",
    "    This function is used for error handling and de-bugging as it returns (Lineitems, Predictions, Linevalues) \n",
    "    \"\"\"\n",
    "    # split values for company dataframe according to columns and values\n",
    "    colNames = df.index\n",
    "    colValues = df.values\n",
    "    \n",
    "    # predicting the column groups with accompanying sklearn model\n",
    "    # NOTE: We pre-process with a HashingVectorizer with 1000 features, this action is very model specific\n",
    "    predNames = mdl.predict(HashingVectorizer(strip_accents='unicode', \n",
    "                                              lowercase=True, analyzer='word',\n",
    "                                              n_features=1000, norm='l2').fit_transform(colNames))\n",
    "    print(predNames.size)\n",
    "    print(colNames.size)\n",
    "    print(colValues.size)\n",
    "    retDF = pd.DataFrame({'Original Lineitems': colNames,                       # the original line items\n",
    "                          'Predicted Lineitems': predNames,                     # the predicted line items\n",
    "                          'Line values': colValues.flatten().tolist()})         # the corresponding line values\n",
    "    \n",
    "    return retDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_indicator(pct):\n",
    "    \"\"\"\n",
    "    Determines the level of matching accuracy for a particular firm/year\n",
    "    \"\"\"\n",
    "    def indicator(x):\n",
    "        \n",
    "        if type(x) is float:\n",
    "            y = x\n",
    "        else:\n",
    "            y = min(x)     # from an array determine the minimum relative error\n",
    "        \n",
    "        if y == 0: return 'PERFECT MATCH'\n",
    "        if 0 < y < 0.01: return 'BOUNDED MATCH'\n",
    "        if y >= 0.01: return 'GROSS MISMATCH'\n",
    "        if np.isnan(y): return 'NOT FOUND'\n",
    "    \n",
    "    vFunc = np.vectorize(indicator)      # vectorize function to apply to numpy array\n",
    "    cleanValue = indicator(pct)            # apply vector function\n",
    "    \n",
    "    return cleanValue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_finder(pct):\n",
    "    \"\"\"\n",
    "    Determines the level of matching accuracy for a particular firm/year\n",
    "    \"\"\"\n",
    "    def min_find(x):\n",
    "        return min(x)\n",
    "    \n",
    "    vFunc = np.vectorize(min_find)      # vectorize function to apply to numpy array\n",
    "    cleanValue = min_find(pct)            # apply vector function\n",
    "    \n",
    "    return cleanValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final structured dataframe has been created.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    output_folder = 'Output/'\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # check available pdfs stored within desired output-folder\n",
    "    s3_path = session.list_s3_files(bucket, output_folder)\n",
    "    \n",
    "    # retrieving the unstructured asset values file from s3 bucket\n",
    "    s3.download_file(bucket, 'Output/unstructured_assets.csv', 'unstructAsset.csv')\n",
    "    s3.download_file(bucket, 'Output/unstructured_liable.csv', 'unstructLiable.csv')\n",
    "\n",
    "    # load in asset and liability dataframes\n",
    "    assetDF = pd.read_csv('unstructAsset.csv')\n",
    "    liableDF = pd.read_csv('unstructLiable.csv')\n",
    "\n",
    "    # remove local file after it has been created (variable is stored in memory)\n",
    "    os.remove('unstructAsset.csv')\n",
    "    os.remove('unstructLiable.csv')\n",
    "    # ==============================================================================\n",
    "    \n",
    "    # load in sklearn classification models\n",
    "    assetMDL = load('/home/ec2-user/SageMaker/SEC_X17A5/code/notebook/ml-model/trained_models/asset_log_reg_mdl_v2.joblib')\n",
    "    liableMDL = load('/home/ec2-user/SageMaker/SEC_X17A5/code/notebook/ml-model/trained_models/liability_log_reg_mdl_v2.joblib')\n",
    "    \n",
    "    str_mdl = HashingVectorizer(strip_accents='unicode', lowercase=True, analyzer='word', n_features=1000, norm='l2')\n",
    "    \n",
    "    # NOTE: we select the post-first 4 columns avoiding the CIK, Name, Filing Date, Fiscal Year, # Totals Check\n",
    "    a_columns = assetDF.columns[4:]\n",
    "    l_columns = liableDF.columns[4:]\n",
    "    \n",
    "    # Use classification model to predict label names for each line item\n",
    "    asset_label_predictions = assetMDL.predict(str_mdl.fit_transform(a_columns))\n",
    "    liable_label_predictions = liableMDL.predict(str_mdl.fit_transform(l_columns))\n",
    "    \n",
    "    # structured database for asset and liability terms \n",
    "    struct_asset_map = pd.DataFrame([a_columns, asset_label_predictions], \n",
    "                                    index=['LineItems', 'Labels']).T\n",
    "\n",
    "    struct_liable_map = pd.DataFrame([l_columns, liable_label_predictions], \n",
    "                                     index=['LineItems', 'Labels']).T\n",
    "    \n",
    "    # construct the line-item prediction classes with corresponding probabilites \n",
    "    a_proba_df = prediction_probabilites(a_columns, assetMDL, str_mdl)\n",
    "    l_proba_df = prediction_probabilites(l_columns, liableMDL, str_mdl)\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Auxillary Database Files \n",
    "    # ==============================================================================\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/asset_prediction_proba.csv'\n",
    "    a_proba_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'asset_prediction_proba.csv', Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/liable_prediction_proba.csv'\n",
    "    l_proba_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'liable_prediction_proba.csv', Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/asset_name_map.csv'\n",
    "    struct_asset_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'asset_name_map.csv', Body=data)\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/liability_name_map.csv'\n",
    "    struct_liable_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'liability_name_map.csv', Body=data)\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Database construction \n",
    "    # ==============================================================================\n",
    "    \n",
    "    # structured database for asset and liability terms \n",
    "    struct_asset_df = structured_data(assetDF, struct_asset_map, \n",
    "                             col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year'])\n",
    "    \n",
    "    # we drop ammended releases, preserving unique CIKs with Filing Year\n",
    "    struct_asset_df = struct_asset_df.drop_duplicates(subset=['CIK', 'Filing Year'], keep='first')\n",
    "    \n",
    "    # extract all line items to reconstruct the appropriate total categories and compute relative differences\n",
    "    asset_lines = struct_asset_df.columns[~np.isin(struct_asset_df.columns, \n",
    "                                            ['CIK', 'Name', 'Filing Date', \n",
    "                                             'Filing Year',  'Total assets'])]\n",
    "    struct_asset_df['Reconstructed Total assets'] = struct_asset_df[asset_lines].sum(axis=1)\n",
    "    struct_asset_df['Relative Error'] = abs(struct_asset_df['Reconstructed Total assets'] - struct_asset_df['Total assets']) / struct_asset_df['Total assets']\n",
    "\n",
    "    struct_asset_df['Total asset check'] = struct_asset_df['Relative Error'].apply(relative_indicator)\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/structured_asset.csv'\n",
    "    struct_asset_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'structured_asset.csv', Body=data)\n",
    "        \n",
    "        \n",
    "    # structured database for asset and liability terms \n",
    "    struct_liable_df = structured_data(liableDF, struct_liable_map, \n",
    "                             col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year'])\n",
    "    \n",
    "    struct_liable_df = struct_liable_df.drop_duplicates(subset=['CIK', 'Filing Year'], keep='first')\n",
    "    \n",
    "     # extract all line items to reconstruct the appropriate total categories and compute relative differences\n",
    "    liable_lines = struct_liable_df.columns[~np.isin(struct_liable_df.columns, \n",
    "                                            ['CIK', 'Name', 'Filing Date', \n",
    "                                             'Filing Year',  \"Total liabilities and shareholder's equity\"])]\n",
    "    \n",
    "    # we remove all other premature totals from the reconsturctured\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity\"] = struct_liable_df[liable_lines].sum(axis=1) \n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total liabilites)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df['Total liabilities'].fillna(0)\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total equity)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df[\"Total shareholder's equity\"].fillna(0)\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total L+E)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df['Total liabilities'].fillna(0) - struct_liable_df[\"Total shareholder's equity\"].fillna(0)\n",
    "    \n",
    "    # constructing measures of relative erorrs against each different reconstruction frameworks\n",
    "    struct_liable_df['Relative Error1'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "    struct_liable_df['Relative Error2'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total liabilites)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "    struct_liable_df['Relative Error3'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total equity)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "    struct_liable_df['Relative Error4'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total L+E)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "\n",
    "    struct_liable_df[\"Total liabilities & shareholder's equity check\"] = struct_liable_df[['Relative Error1', 'Relative Error2', 'Relative Error3', 'Relative Error4']].apply(relative_indicator, axis=1)\n",
    "    struct_liable_df[\"Relative Error\"] = struct_liable_df[['Relative Error1', 'Relative Error2', 'Relative Error3', 'Relative Error4']].apply(relative_finder, axis=1)\n",
    "    \n",
    "    filename = '/home/ec2-user/SageMaker/SEC_X17A5/output/structured_liability.csv'\n",
    "    struct_liable_df[struct_liable_df.columns[~np.isin(struct_liable_df.columns, ['Relative Error1', 'Relative Error2', 'Relative Error3', 'Relative Error4'])]].to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=output_folder + 'structured_liability.csv', Body=data)\n",
    "    # ==============================================================================\n",
    "    \n",
    "    print('The final structured dataframe has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Amazon Textract client and Sagemaker session\n",
    "# s3 = boto3.client('s3')\n",
    "# session = Session()\n",
    "\n",
    "# bucket = 'ran-s3-systemic-risk'\n",
    "# output_folder = 'Output/'\n",
    "\n",
    "# # ==============================================================================\n",
    "# # check available pdfs stored within desired output-folder\n",
    "# s3_path = session.list_s3_files(bucket, output_folder)\n",
    "\n",
    "# # retrieving the unstructured asset values file from s3 bucket\n",
    "# s3.download_file(bucket, 'Output/unstructured_assets.csv', 'unstructAsset.csv')\n",
    "# s3.download_file(bucket, 'Output/unstructured_liable.csv', 'unstructLiable.csv')\n",
    "\n",
    "# # load in asset and liability dataframes\n",
    "# assetDF = pd.read_csv('unstructAsset.csv')\n",
    "# liableDF = pd.read_csv('unstructLiable.csv')\n",
    "\n",
    "# # remove local file after it has been created (variable is stored in memory)\n",
    "# os.remove('unstructAsset.csv')\n",
    "# os.remove('unstructLiable.csv')\n",
    "# # =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liableDF[(liableDF.CIK == 895502) & (liableDF['Filing Date'] == '2002-02-28')].T.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
