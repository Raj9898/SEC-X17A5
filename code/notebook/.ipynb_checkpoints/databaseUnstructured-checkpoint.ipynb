{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-Levenshtein in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fuzzywuzzy\n",
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import botocore\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from difflib import SequenceMatcher\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting term matching\n",
    "**Check to see if we report totals (e.g. Total Liability & Shareholder Equity) or sub-totals (e.g. Total Financial Instruments), these figure are not needed for construction of the unstructured database (avoid classification issue)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy exception for handling invalid log10 RunTime error (we opt to not show)\n",
    "# switch 'ignore' to 'warn', if you would like to flag the RunTime error \n",
    "np.seterr(invalid = 'ignore') \n",
    "\n",
    "def multiple_check(x1:float, x2:float):\n",
    "    \"\"\"\n",
    "    Determine whether the two values are the same number scaled by 10\n",
    "    \"\"\"\n",
    "    # prevent zero division error since x1 is the denominator and log10 zero division error\n",
    "    if (x1 == 0) or (x2 == 0): return (x1, False)\n",
    "    else:\n",
    "        # if our backward sum is a multiple of 10, we return True \n",
    "        # (e.g. Total Assets (x1) 745.2322 vs Backward Sum (x2) 7452322)\n",
    "        check1 = np.log10(x2 / x1).is_integer()\n",
    "\n",
    "        # if our backward sum is a substring of a line item, with a difference of one in length, we return True \n",
    "        # (e.g. Total Assets (x1) 174182935 vs Backward Sum (x2) 74182935)\n",
    "        check2 = (str(x2) in str(x1) ) & (len(str(x2)) == len(str(x1)) - 1)\n",
    "\n",
    "        if check1 or check2:\n",
    "            return (x2, True)\n",
    "        else: \n",
    "            return (x1, False)\n",
    "\n",
    "def epsilon_error(x1:float, x2:float, tol:float=0.01):\n",
    "    \"\"\"\n",
    "    Determine whether the two values are within a similar epsilon bound. We default our error tolerance\n",
    "    to 0.01, implying that if two numbers are within a specified toleracnce (default 1% ) of one another \n",
    "    \"\"\"\n",
    "    if (x1 == 0) or (x2 == 0): return False\n",
    "    else:\n",
    "        # first we convert the numeric quantities into strings\n",
    "        current = str(x1)\n",
    "        lookback = str(x2)\n",
    "\n",
    "        # we only want to check against the relative difference if one element in the number is read wrong\n",
    "        if len(current) == len(lookback):\n",
    "            \n",
    "            # we iterate linearly through each string and check to see the positional match \n",
    "            # if we catch a mismatch we flag it with a 1, othewise skip with 0\n",
    "            changes = [0 if (current[i] == lookback[i]) else 1 for i in range(len(current))]\n",
    "\n",
    "            # check set differences produce a set with exactly 1 in length\n",
    "            if sum(changes) == 1:\n",
    "\n",
    "                diff = abs(x1 - x2)      # compute numeric differences\n",
    "\n",
    "                # check to see whether an accounting condition was met wihtin a boundary condition\n",
    "                if abs(diff / x1) <= tol:\n",
    "                    return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totals_check(df:pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Checks to see if a line row meets the conditon of a total, if true we remove these rows as we make \n",
    "    have checked the terms before have meet our conditions (these include major and minor totals)\n",
    "    ------------------------------------------------------------------------------------\n",
    "    :param: (type pandas.DataFrame)\n",
    "        A DataFrame that represents the Asset or Liability & Equity portion of the balance sheet\n",
    "        \n",
    "    :return: (type tuple)\n",
    "        Return a cleaned DataFrame that strips the rows that represent totals\n",
    "    \"\"\"\n",
    "    m, n = df.shape                  # unpack the shape of dataframe\n",
    "    data_col = df.columns[1]         # the values column for balance sheet\n",
    "    \n",
    "    total_flag = 2       # default 2 (no measure found), 1 (sum is correct), 0 (sum is not correct)\n",
    "    total_amt = np.nan\n",
    "    \n",
    "    # iterate through each of the line items\n",
    "    for i in range(m):\n",
    "        \n",
    "        # check the value of line items at a given index (forward index)\n",
    "        item1 = df.loc[i].values[1]\n",
    "        name = df.loc[i].values[0]\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # Perform regex search to determine \"special\" total rows\n",
    "        # ------------------------------------------------------------------\n",
    "        a_check = re.search('total assets', name, flags=re.I)\n",
    "        le_check = re.search('(?=.*(liability|liabilities))(?=.*(equity|deficit|capital))', \n",
    "                             name, flags=re.I)\n",
    "        # ------------------------------------------------------------------\n",
    "        \n",
    "        # if we find either total measure we re-write indicators\n",
    "        if a_check is not None or le_check is not None:\n",
    "            total_flag = 0; total_amt = item1;\n",
    "        \n",
    "        # compute backward sum (lookback index) \n",
    "        for j in range(i):\n",
    "            \n",
    "            # check whether dataframe empty (if so we skip to avoid fitting errors)\n",
    "            # NOTE: Index position (i-1)   = line above current line\n",
    "            #                      (i-j-1) = trailing look up line 'j' lines above the line above current line\n",
    "            lookback = df.loc[i-j-1:i-1][data_col]\n",
    "            \n",
    "            # we check whether the lookback period is empty (if so we most likely deleted the row)\n",
    "            if not lookback.empty:\n",
    "                # backward sum for line items (index minus j-periods before)\n",
    "                item2 = lookback.sum()\n",
    "\n",
    "                # if we achieve this then we strip totals and break, no need to continue backward sum\n",
    "                check1 = item1 == item2\n",
    "                val, check2 = multiple_check(item1, item2)\n",
    "                check3 = epsilon_error(item1, item2, tol=0.01)\n",
    "                \n",
    "                if check1 or check2 or check3:\n",
    "                    df = df.drop(index=i)\n",
    "                    \n",
    "                    # if we drop the \"Total\" line-item then we re-assign flag to 1\n",
    "                    if a_check is not None or le_check is not None:\n",
    "                        total_flag = 1\n",
    "                        total_amt = val\n",
    "                    \n",
    "                    # Error Handling for row deletions (uncomment for when not in use)\n",
    "                    print('\\tWe dropped row {}, {}, with lookback window of {}.'.format(i, name, j+1))\n",
    "                    print('\\t\\tOur row is valued at {}, our lookback sum is {}'.format(item1, item2))\n",
    "                    \n",
    "                    # we break from inner loop to avoid key error flag \n",
    "                    break     \n",
    "                \n",
    "    return (df, total_flag, total_amt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging PDFs and PNGs \n",
    "**Functions to combine PDFs and PNGs where rows may be omitted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_merge(df1:pd.DataFrame, df2:pd.DataFrame, col:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Special type of merge for dataframes, combining all unique row items for a specified column. \n",
    "    This is designed to combine PDF and PNG balance sheets that differ in one or more rows.\n",
    "    ------------------------------------------------------------------------------------\n",
    "    :param: (type pandas.DataFrame)\n",
    "        A DataFrame that represents either the PDF or PNG retreived from Balance Sheet\n",
    "    :param: (type pandas.DataFrame)\n",
    "        A DataFrame that represents either the PDF or PNG retreived from Balance Sheet\n",
    "    :param: (type str)\n",
    "        A shared column name that exists in both pandas.DataFrames (i.e. df1, df2)\n",
    "        \n",
    "    :return: (type pandas.DataFrame)\n",
    "        Return a cleaned DataFrame that merges any row that was omitted, changed or missing\n",
    "    \"\"\"\n",
    "    arr1 = df1[col].values\n",
    "    arr2 = df2[col].values\n",
    "    concat_pdf = []\n",
    "    \n",
    "    # find the sequences that match between either lineitems\n",
    "    sm = SequenceMatcher(a=arr1, b=arr2)\n",
    "    \n",
    "    # The SequenceMathcer returns a 5-tupled for each correspond \"obj\"\n",
    "    # 'replace'     a[i1:i2] should be replaced by b[j1:j2].\n",
    "    # 'delete'      a[i1:i2] should be deleted. Note that j1 == j2 in this case.\n",
    "    # 'insert'      b[j1:j2] should be inserted at a[i1:i1]. Note that i1 == i2 in this case.\n",
    "    # 'equal'       a[i1:i2] == b[j1:j2] (the sub-sequences are equal)\n",
    "    for (obj, i1, i2, j1, j2) in sm.get_opcodes():\n",
    "        \n",
    "        # implies that we want to \"replace\" the left side elment with the corresponding\n",
    "        # right side element at the same index position (we perseve both)\n",
    "        if obj == 'replace':\n",
    "            \n",
    "            # check the value of a fuzzy match, only insert both rows if they vastly different\n",
    "            left_names = arr1[i1:i2]\n",
    "            right_names = arr2[j1:j2]\n",
    "            \n",
    "            # iterate through each of the checks\n",
    "            for it, (left, right) in enumerate(zip(left_names, right_names)):\n",
    "                \n",
    "                # compute the fuzz match between string (how close are these values)\n",
    "                score = fuzz.partial_ratio(left.lower(), right.lower())\n",
    "\n",
    "                # if not close in match then we append both values\n",
    "                if score < 90:\n",
    "                    concat_pdf.append(df1.iloc[i1:i1+it+1])\n",
    "                    concat_pdf.append(df2.iloc[j1:j1+it+1])\n",
    "                else:\n",
    "                    concat_pdf.append(df1.iloc[i1:i1+it+1])\n",
    "        \n",
    "        # implies that we want to \"delete\" the left side element (we preserve this side)\n",
    "        elif obj == 'delete':\n",
    "            concat_pdf.append(df1.iloc[i1:i2])\n",
    "        \n",
    "        # implied that we want to \"insert\" the right side element (we preserve this side)\n",
    "        elif obj == 'insert':\n",
    "            concat_pdf.append(df2.iloc[j1:j2])\n",
    "            \n",
    "        elif obj == 'equal':\n",
    "            concat_pdf.append(df1.iloc[i1:i2])\n",
    "    \n",
    "    # return concated pandas.DataFrame and reset index, removing old index\n",
    "    return pd.concat(concat_pdf).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Database construction\n",
    "**We develop our unstructured database from each of the non-total rows (concating the line items)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructured_data(df, filing_d, fiscal_y, cik, cik2name:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Forms unstructured row for larger database to be stored in s3 bucket\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: df (type pandas.DataFrame)\n",
    "            The balance sheet for a particular  \n",
    "        :paran: filing_d (type str)\n",
    "            The filing date for release of X-17A-5 filings for a broker dealer e.g. 2013-03-21\n",
    "        :paran: fiscal_y (type str)\n",
    "            The fiscal year for the balance sheet to cover e.g. 2012 (usually 1-year prior to filing date)\n",
    "        :paran: cik (type str)\n",
    "            The CIK number for a broker dealer e.g. 887767\n",
    "        :paran: cik2name (type dict)\n",
    "            A dictionary that maps CIK to Broker Deale names \n",
    "    Output:  \n",
    "        :return: (type pandas.DataFrame)\n",
    "             Return a transposed dataframe with additional columns corresponding to filing data\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize the first column (line items)\n",
    "    first_column = df.columns[0]\n",
    "    \n",
    "    # clean dataframe should be of size greater than 1\n",
    "    if len(df.columns) > 1:\n",
    "        \n",
    "        # transpose split balance sheet figure (our line items are now columns for DataFrame)\n",
    "        # we first groupby the first column (this become index) and sum to group congruent names\n",
    "        row = df.groupby(first_column).sum(min_count=1).T\n",
    "        \n",
    "        # creating additional columns in row\n",
    "        row['CIK'] = cik                                  # CIK number for firm \n",
    "        row['Filing Date'] = filing_d                     # Filing Date for firm filing\n",
    "        row['Filing Year'] = fiscal_y                     # Year for balance sheet filing\n",
    "        row['Name'] = cik2name['broker-dealers'][cik]     # returns the name of associated with the CIK\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    else:\n",
    "        print('{}-{}.csv - encountered issue reading PDF'.format(cik, filing_d))\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_cols(csv_name:str):\n",
    "    \"\"\"\n",
    "    Check to see whether their exists the presence of a total term from the line items\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: csv_name (type str)\n",
    "            The file directory on the s3 where data is stored (e.g. )\n",
    "    Output:  \n",
    "        :return: (type tuple)\n",
    "            Returns a tuple for corresponding (file_name, filing_date, fiscal_year, cik)\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = csv_name.split('/')[-1]        # e.g. '1224385-2005-03-01.csv'\n",
    "    csv_strip = file_name[:-4]                 # ignore last four elements from the back (i.e. .csv)\n",
    "\n",
    "    # construct a string measure of important data measures \n",
    "    data_split = csv_strip.split('-')              \n",
    "    filing_date = '-'.join(data_split[1:])         # join YYYY-mm-dd component for filing date\n",
    "    fiscal_year = int(data_split[1]) - 1           # fiscal year are generally the previous year of filing\n",
    "    cik = data_split[0]                            # extract the CIK number  \n",
    "    \n",
    "    return (file_name, filing_date, fiscal_year, cik)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(df:pd.DataFrame, col_preserve:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Re-order the completed DataFrame by ordering the CIK, Name, Filing Data and Filing Year \n",
    "    ------------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: df (type pandas.DataFrame)\n",
    "            The unstructured database for balance sheet figures\n",
    "    Output:  \n",
    "        :return: (type pandas.DataFrame)\n",
    "            Return a dataframe with dimensions less than or equal to input dataframe (MxN) -> (MxK), \n",
    "            where K <= N\n",
    "    \"\"\"\n",
    "    # re-order the CIK and Year columns to appear as the first two columns\n",
    "    remap = df.columns[~np.isin(df.columns, col_preserve)]                             \n",
    "    df = df[np.insert(remap,                                       # pass all other columns, not in preserve list\n",
    "                      np.zeros(len(col_preserve), dtype=int),      # map the location to the first index (i.e. 0)\n",
    "                      col_preserve)]                               # insert columns we wished to preserve \n",
    "\n",
    "    filterNaN = df.isnull().all()                      # find if any column is all NaN \n",
    "    cleanCols = filterNaN[filterNaN == False].index    # select columns with at least one value\n",
    "\n",
    "    # clean dataframe for unstructured asset terms\n",
    "    return df[cleanCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We created an unstructured asset and liability & equity\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initiate s3 bucket and corresponding data folder\n",
    "    bucket = \"ran-s3-systemic-risk\"\n",
    "    \n",
    "    pdf_asset_folder = \"Output/X-17A-5-SPLIT-PDFS/Assets/\"\n",
    "    pdf_liable_folder = \"Output/X-17A-5-SPLIT-PDFS/Liability & Equity/\"\n",
    "    \n",
    "    png_asset_folder = \"Output/X-17A-5-SPLIT-PNGS/Assets/\"\n",
    "    png_liable_folder = \"Output/X-17A-5-SPLIT-PNGS/Liability & Equity/\"\n",
    "    \n",
    "    out_folder = \"Output/\"\n",
    "\n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # ALL TEMPORARY FILE INFORMATION \n",
    "    # ==============================================================================\n",
    "    # retrieving CIK-Dealers JSON file from s3 bucket\n",
    "    s3.download_file(bucket, 'Temp/CIKandDealers.json', 'temp.json')\n",
    "    with open('temp.json', 'r') as f: cik2brokers = json.loads(f.read())\n",
    "\n",
    "    # remove local file after it has been created (variable is stored in memory)\n",
    "    os.remove('temp.json')\n",
    "    # ==============================================================================\n",
    "    \n",
    "    # s3 paths where asset and liability paths are stored\n",
    "    asset_paths = session.list_s3_files(bucket, pdf_asset_folder)\n",
    "    liable_paths = session.list_s3_files(bucket, pdf_liable_folder)\n",
    "    \n",
    "    # intialize list to store dataframes for asset and liability & equity\n",
    "    asset_concat = [0] * len(asset_paths)\n",
    "    liable_concat = [0] * len(liable_paths)\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # Asset Unstructured Database\n",
    "    # --------------------------------------------\n",
    "    print('Assets Unstructured Database')\n",
    "    for idx, csv in enumerate(asset_paths):\n",
    "        \n",
    "        # decompose csv name into corresponding terms\n",
    "        fileName, filing_date, fiscal_year, cik = extra_cols(csv)\n",
    "        \n",
    "        # first load in both the PNG and PDF split balance sheets\n",
    "        # NOTE: All these balance sheets are cleaned numerical values\n",
    "        try:\n",
    "            s3.download_file(bucket, csv, 'temp.csv')\n",
    "            pdf_df = pd.read_csv('temp.csv')\n",
    "            s3.download_file(bucket, png_asset_folder + fileName, 'temp.csv')\n",
    "            png_df = pd.read_csv('temp.csv')\n",
    "            os.remove('temp.csv')\n",
    "\n",
    "            print('Working on {}-{}'.format(cik, filing_date))\n",
    "\n",
    "            # do a special merge that combines unique line items names between PDF & PNG\n",
    "            temp_df1 = special_merge(pdf_df, png_df, '0')\n",
    "\n",
    "            # run accounting check to remove sub-totals for each respective line-item\n",
    "            df, total_flag, total_amt = totals_check(temp_df1)\n",
    "            \n",
    "            # construct row for the unstructured data frame \n",
    "            export_df = unstructured_data(df, filing_date, fiscal_year, cik, cik2brokers)\n",
    "            \n",
    "            # we have that no \"total asset\" figure was found\n",
    "            if total_flag == 2:\n",
    "                export_df[\"Total asset check\"] = \"Total asset not found\"\n",
    "                export_df[\"Total asset\"] = total_amt\n",
    "\n",
    "            # we have that \"total asset\" was found and matches\n",
    "            elif total_flag == 1:\n",
    "                export_df[\"Total asset check\"] = \"Total asset found & match\"\n",
    "                export_df[\"Total asset\"] = total_amt\n",
    "\n",
    "            # we have that \"total asset\" was found, but did not match correctly\n",
    "            elif total_flag == 0:\n",
    "                export_df[\"Total asset check\"] = \"Total asset found & no match\"\n",
    "\n",
    "            # stores the reported data frame \n",
    "            asset_concat[idx] = export_df\n",
    "        \n",
    "        # in the event we can't download file from s3 (i.e. does not exist, we ignore the )\n",
    "        except botocore.exceptions.ClientError:\n",
    "            \n",
    "            # assign an empty DataFrame and print out error\n",
    "            asset_concat[idx] = pd.DataFrame()\n",
    "            \n",
    "            print('\\nCLIENT-ERROR: WE COULD NOT DOWNLOAD SPLIT DATA FOR {}\\n'.format(fileName))\n",
    "     \n",
    "    print('\\n\\n\\n\\n')\n",
    "        \n",
    "    # --------------------------------------------\n",
    "    # Liability & Equity Unstructured Database\n",
    "    # --------------------------------------------\n",
    "    print('\\nLiability & Equity Unstructured Database')\n",
    "    for idx, csv in enumerate(liable_paths):\n",
    "        \n",
    "        # decompose csv name into corresponding terms\n",
    "        fileName, filing_date, fiscal_year, cik = extra_cols(csv)\n",
    "        \n",
    "        try:\n",
    "            # first load in both the PNG and PDF split balance sheets\n",
    "            # NOTE: All these balance sheets are cleaned numerical values\n",
    "            s3.download_file(bucket, csv, 'temp.csv')\n",
    "            pdf_df = pd.read_csv('temp.csv')\n",
    "            s3.download_file(bucket, png_liable_folder + fileName, 'temp.csv')\n",
    "            png_df = pd.read_csv('temp.csv')\n",
    "            os.remove('temp.csv')\n",
    "\n",
    "            print('Working on {}-{}'.format(cik, filing_date))\n",
    "\n",
    "            # do a special merge that combines unique line items names between PDF & PNG\n",
    "            temp_df1 = special_merge(pdf_df, png_df, '0')\n",
    "\n",
    "            # run accounting check to remove sub-totals for each respective line-item\n",
    "            df, total_flag, total_amt = totals_check(temp_df1)\n",
    "            \n",
    "            # construct row for the unstructured data frame \n",
    "            export_df = unstructured_data(df, filing_date, fiscal_year, cik, cik2brokers)\n",
    "            \n",
    "            # we have that no \"total asset\" figure was found\n",
    "            if total_flag == 2:\n",
    "                export_df[\"Total liabilities & shareholder's equity check\"] = \"Total liabilities & shareholder's equity not found\"\n",
    "                export_df[\"Total liabilities & shareholder's equity\"] = total_amt\n",
    "\n",
    "            # we have that \"total asset\" was found and matches\n",
    "            elif total_flag == 1:\n",
    "                export_df[\"Total liabilities & shareholder's equity check\"] = \"Total liabilities & shareholder's equity found & match\"\n",
    "                export_df[\"Total liabilities & shareholder's equity\"] = total_amt\n",
    "        \n",
    "            # we have that \"total asset\" was found, but did not match correctly\n",
    "            elif total_flag == 0:\n",
    "                export_df[\"Total liabilities & shareholder's equity check\"] = \"Total liabilities & shareholder's equity found & no match\"\n",
    "                \n",
    "            # stores the reported data frame \n",
    "            liable_concat[idx] = export_df\n",
    "        \n",
    "        # in the event we can't download file from s3 (i.e. does not exist, we ignore the )\n",
    "        except botocore.exceptions.ClientError:\n",
    "            \n",
    "            # assign an empty DataFrame and print out error\n",
    "            liable_concat[idx] = pd.DataFrame()\n",
    "            \n",
    "            print('\\nCLIENT-ERROR: WE COULD NOT DOWNLOAD SPLIT DATA FOR {}\\n'.format(fileName))\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # Database exportation\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # writing data frame to .csv file\n",
    "    asset_df = pd.concat(asset_concat)        # asset dataframe combining all rows from \n",
    "    asset_df = reorder_columns(asset_df,      # re-order columns for dataframe\n",
    "                               col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year', \n",
    "                                             'Total asset check'])      \n",
    "    \n",
    "    filename1 = '/home/ec2-user/SageMaker/SEC_X17A5/output/unstructured_assets.csv'\n",
    "    asset_df.to_csv(filename1, index=False)\n",
    "    with open(filename1, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + 'unstructured_assets.csv', Body=data)\n",
    "    \n",
    "    \n",
    "    # writing data frame to .csv file\n",
    "    liable_df = pd.concat(liable_concat)     \n",
    "    liable_df = reorder_columns(liable_df, \n",
    "                                col_preserve=['CIK', 'Name', 'Filing Date', 'Filing Year', \n",
    "                                              \"Total liabilities & shareholder's equity check\"])    \n",
    "    \n",
    "    filename2 = '/home/ec2-user/SageMaker/SEC_X17A5/output/unstructured_liable.csv'\n",
    "    liable_df.to_csv(filename2, index=False)\n",
    "    with open(filename2, 'rb') as data:\n",
    "        s3.put_object(Bucket=bucket, Key=out_folder + 'unstructured_liable.csv', Body=data)\n",
    "    \n",
    "    print('\\nWe created an unstructured asset and liability & equity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # work on combining columns that are issued seperately\n",
    "# s3 = boto3.client('s3')\n",
    "# session = Session()\n",
    "# bucket = \"ran-s3-systemic-risk\"\n",
    "\n",
    "# # e.g. file name = 1224385-2004-03-01, 42352-2003-01-28\n",
    "# s3.download_file(bucket, 'Output/X-17A-5-SPLIT-PDFS/Assets/42352-2011-03-01.csv', 'temp.csv')\n",
    "# pdf_df = pd.read_csv('temp.csv')\n",
    "# s3.download_file(bucket, 'Output/X-17A-5-SPLIT-PNGS/Assets/42352-2011-03-01.csv', 'temp.csv')\n",
    "# png_df = pd.read_csv('temp.csv')\n",
    "# os.remove('temp.csv')\n",
    "\n",
    "# # do a special merge that combines unique line items names between PDF & PNG\n",
    "# temp_df1 = special_merge(pdf_df, png_df, '0')\n",
    "\n",
    "# # run accounting check to remove sub-totals for each respective line-item\n",
    "# df, total_flag, total_amt = totals_check(temp_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totals_check(temp_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
