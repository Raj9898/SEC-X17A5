{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (5.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: minecart in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: pdfminer3k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.3.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from minecart) (1.15.0)\n",
      "Requirement already satisfied: ply in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pdfminer3k->minecart) (3.11)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textract-trp in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp36-cp36m-linux_x86_64.whl size=155938 sha256=c9fbe4b1961ebc8266c13a8f4cfd414f8a9d4cb3d25b3a6ee4b15dda22432a14\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4a/a4/bf/d761b0899395c75fa76d003d607b3869ee47f5035b8afc30a2\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run on first instance to install required libraries\n",
    "%pip install smart_open\n",
    "%pip install minecart\n",
    "%pip install textract-trp\n",
    "%pip install python-Levenshtein\n",
    "%pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re\n",
    "import os\n",
    "import trp\n",
    "import boto3\n",
    "import minecart\n",
    "import json\n",
    "import logging \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from smart_open import open\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_strip(number):\n",
    "    \"\"\"\n",
    "    This function converts a string to a numeric quantity, handles weird string format. \n",
    "    We handle input arguments of a string, integer or numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    numType = type(number)\n",
    "\n",
    "    # if provided a non-empty string, perform regex operation \n",
    "    if (numType is str) and (len(number) > 0):\n",
    "\n",
    "        # check for accounting formats that use parenthesis to signal losses \n",
    "        if number[0] == '(': number = '-' + number\n",
    "\n",
    "        # case replacing to handle poor textract reading of numbers\n",
    "        number = number.replace('I', '1').replace('l', '1')\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Explanation of the Regex Expression:\n",
    "        #      [^0-9|.|-]     = match all elements that are not numeric 0-9, periods \".\" or hyphens \"-\"\n",
    "        #      (?<!^)-        = match all elements that are hyphens \"-\" not in the first index position\n",
    "        #      \\.(?=[^.]*\\.)  = match all elements that are periods \".\" except the last instance\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        check1 = re.sub(\"[^0-9|.|-]\", \"\", number)         # remove all the non-numeric, periods \".\" or hyphens \"-\"\n",
    "        check2 = re.sub(\"(?<!^)-\", \"\", check1)            # removes all \"-\" that aren't in the first index \n",
    "        check3 = re.sub(\"\\.(?=[^.]*\\.)\", \"\", check2)      # removes all periods except the last instance of \".\" \n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        # we consider weird decimal values that exceed 2 spaces to the right (e.g. 432.2884)\n",
    "        period_check = check3.find('.')                         # returns the location of the period \n",
    "        right_tail_length = len(check3) - period_check - 1      # right-tail length should not exceed 2\n",
    "\n",
    "        # if more than 2 trailing digits to decimal point we assume incorrect placement\n",
    "        if right_tail_length > 2:\n",
    "            check3 = check3.replace('.', '')\n",
    "\n",
    "        # last check against poor lagging formats e.g. \".\" or \"-\" to return nan or floating-point number\n",
    "        if (check3 == '-') or (check3 == '.'):\n",
    "            return 0.0\n",
    "        else:\n",
    "            # try to cast to floating point value, else flat NaN\n",
    "            try: \n",
    "                return float(check3)\n",
    "            except ValueError: \n",
    "                return np.nan\n",
    "\n",
    "    # if operator is an integer or float then simply return the value\n",
    "    elif (numType is int) or (numType is float):\n",
    "        return number\n",
    "\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Shaving\n",
    "**Removing blank/empty rows that are reported in the line items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_purge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Column designed to filter out rows that are NaN (empty) and reduce dataframe size\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of size less than or equal to the original input \n",
    "    \"\"\"\n",
    "    # begin by filtering out the NaN rows present in the first column\n",
    "    first_col = df.columns[0]\n",
    "    new_df = df[np.isin(df[first_col], df[first_col].dropna())]    # select subset of rows \n",
    "    \n",
    "    # we reset the index of our new_df to recoup a consecutive index count\n",
    "    new_df = new_df.reset_index()\n",
    "    new_df = new_df[new_df.columns[1:]]    # skip the first column since we reset the index\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table column merging\n",
    "**For tables with three columns we merge the last two columns into a once unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function passes a special dataframe, and reduces its dimensions accordingly. \n",
    "    Example releases include, but are note limited to, 1224385-2016 and 72267-2003\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of of size (Nx3) -> (Nx2)\n",
    "    \n",
    "    e.g.\n",
    "    \n",
    "    Converts a wide dataframe, balance sheet into a smaller rectangular form\n",
    "                  0                                                 1                 2\n",
    "            ====================================================================================\n",
    "        0   Assets                                          | NaN            | NaN  \n",
    "        1   Cash and cash equivalents                       | $ 606,278      |     \n",
    "        2   Cash and securities segregated pursuant         | 273,083        | \n",
    "        3   Collateralized short-term financing agreements: | NaN            | $ 1,345\n",
    "    \n",
    "    \n",
    "    Rectangular form of the the dataframe ->\n",
    "                   0                                                 1          \n",
    "            =====================================================================\n",
    "        0   Assets                      \n",
    "        1   Cash and cash equivalents                       | $ 606,278        \n",
    "        2   Cash and securities segregated pursuant         | 273,083        \n",
    "        3   Collateralized short-term financing agreements: | $ 1,345            \n",
    "    \"\"\"\n",
    "    # work on itterative merging for rows, check left/right and top/bottom\n",
    "    n = df.shape[0]\n",
    "    trans = []\n",
    "\n",
    "    for i in range(n):\n",
    "        row = df.iloc[i]         # index into the row\n",
    "\n",
    "        name = row.iloc[0]       # the line item name (e.g. Total Assets)\n",
    "        col1 = row.iloc[1]       # the first value(s) column\n",
    "        col2 = row.iloc[2]       # the second value(s) column \n",
    "        \n",
    "        # ----------------------------------------------\n",
    "        # NOTE: We say nothing if both col 1 and 2 are \n",
    "        #     both populated with a numeric value\n",
    "        # ----------------------------------------------\n",
    "        \n",
    "        if num_strip(col1) is not np.nan:\n",
    "            trans.append([name, col1])     # if column 1 has a numeric value we take it by default\n",
    "        elif num_strip(col2) is not np.nan:\n",
    "            trans.append([name, col2])     # if column 1 has no numeric value, but column 2 does, we take it\n",
    "            \n",
    "        # ----------------------------------------------\n",
    "        \n",
    "        # we want to check if there exists two NaNs - is it real or false flag\n",
    "        if (col1 is np.nan) and (col2 is np.nan): \n",
    "            \n",
    "            # look up one row (if possible to see if col1 and col2 are populated)\n",
    "            try:\n",
    "                # check the information for the above row\n",
    "                indexer = i-1\n",
    "                \n",
    "                # we don't want to do reverse lookup with negatives\n",
    "                if indexer > 0:\n",
    "                    prior_row = df.iloc[indexer]                 # previous dataframe row \n",
    "                    prior_col1 = prior_row.iloc[1]               # first column from previous row\n",
    "                    prior_col2 = prior_row.iloc[2]               # second column from previous row\n",
    "\n",
    "                    # if both values present then we simply use the right hand side value above  \n",
    "                    if (prior_col1 is not np.nan) and (prior_col2 is not np.nan):\n",
    "                        trans.append([name, prior_col2])\n",
    "            \n",
    "            # IndexError if not possible to look up one row       \n",
    "            except IndexError: pass\n",
    "    \n",
    "    return pd.DataFrame(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_merge(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function determines whether a Balance Sheet should be merged or simply filtered. \n",
    "    Our two cases are determined as follows:\n",
    "        * If the second column present in the balance sheet is mostly empty we assume that \n",
    "          the second column is an aggregated column, and we can merge it\n",
    "        * However, if the second column is mostly filled with values, we assume that this\n",
    "          represents accounting figures from the previous year \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input\n",
    "        :param data: (type pandas.DataFrame)\n",
    "            A dataframe object that corresponds to the X-17A-5 filings\n",
    "    \n",
    "    Output\n",
    "        :return: (type pandas.DataFrame)\n",
    "            Returns a dataframe of of size (Nx3) -> (Nx2)\n",
    "    \"\"\"\n",
    "    # two events could occur at this point (either the column represents totals, or values from a prior-year)\n",
    "    arr = df[df.columns[2]].values\n",
    "\n",
    "    # check the scope of the second column \n",
    "    n = arr.size\n",
    "    k = arr.tolist().count(np.nan)\n",
    "\n",
    "    # k-check: if more than half the arr size is np.nan we assume this is a \"fake column\"\n",
    "    # we merge these columns since there are many blank rows, otherwise we assume year split \n",
    "    if k/n >= 0.50:\n",
    "        new_df = merge(df)            # merge rows by merge function\n",
    "    else:\n",
    "        new_df = df[df.columns[:2]]   # return the most recent year \n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Row Split\n",
    "**Since many of the existing tables run the risk of overlapping rows we work to split these rows to appropriate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dollar_check(num):\n",
    "    \"\"\"\n",
    "    A function to check the presence of a '$' or 'S'. This function is used to \n",
    "    complement our row splits function to determine \"True splits\"\n",
    "    \"\"\"\n",
    "    if num not in ['$', 'S']:\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(df:pd.DataFrame, text_file:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function designed to split conjoined rows from Balance sheet dataframes into individual rows.\n",
    "    Example releases include, but are note limited to, 42352-2015, 58056-2009, 58056-2013, 58056-2019\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param df: (type pandas.DataFrame)\n",
    "            References the balance sheet dataframe read in from AWS Textract\n",
    "        :param text_file: (type dictionary)\n",
    "            Stores text values with corresponding confidence level for balance sheet pages\n",
    "    \n",
    "    Output:\n",
    "        :param return: (type pandas.DataFrame) \n",
    "            A processed dataframe of size greater than or equal to the inputed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # ##############################################################\n",
    "    # NESTED HELPER FUNCTIONS\n",
    "    # ##############################################################\n",
    "    \n",
    "    def find_row_splits(val) -> bool:\n",
    "        \"\"\"\n",
    "        Compute a boolean measure to assess whether a row is conjoined or not. We make\n",
    "        the assumption that a row is conjoined or merged if there exists a space in the \n",
    "        first value column (omiting the dollar sign $ and S which may be read in)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # split the data figures for each balance sheet figure\n",
    "            arr = val.split(' ')\n",
    "            \n",
    "            # remove the '$' sign or 'S' if present in the list (this helps avoid false pasitives) \n",
    "            arr = list(filter(dollar_check, arr))\n",
    "            \n",
    "            # if length of read list exceeds 1 then we know there exists a multi-row bunch\n",
    "            if len(arr) > 1:\n",
    "                return True\n",
    "            else: return False\n",
    "        \n",
    "        # handle exception for NaN (no attribute to split) \n",
    "        except AttributeError: return False\n",
    "    \n",
    "    def extract_lineitems(line:list, value:list, dictionary:dict) -> list:\n",
    "        \"\"\"\n",
    "        Extract the appropriate line items from each line value.\n",
    "        \"\"\"\n",
    "        splits = []\n",
    "        \n",
    "        # iterate through each line item\n",
    "        for i in dictionary.keys():\n",
    "    \n",
    "            # we check for real key-value names avoiding single character keys\n",
    "            if len(i) > 1: \n",
    "                idx = line.find(i)    # find the index of key-value (if possible) in line item array\n",
    "\n",
    "                # if we find such a value we append the series (failure to find results idx = -1)\n",
    "                if idx >= 0: splits.append(i)\n",
    "        \n",
    "        # check whether we have a one-to-one mapping between line items and line values, \n",
    "        # e.g. ['Assets', 'Cash', 'Recievables'] -> ['1,233', '4,819'] (3x2 mapping)\n",
    "        n = len(splits) - len(value)\n",
    "        \n",
    "        # if n is equal to zero we have a \"perfect\" match\n",
    "        if n == 0:\n",
    "            return (splits, value)\n",
    "        elif n > 0:\n",
    "            return (splits[n:], value)       # more line items terms, assume values is right\n",
    "        elif n == -1:                        \n",
    "            return (splits, values[1:])      # more value terms, assume value is wrong only if difference is 1 in size\n",
    "        else: \n",
    "            return None                      # no specific rule paradigm (more values than items)\n",
    "    \n",
    "    def recursive_splits(values:list, lineName:list, sub=[]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Recursively breaks up merged rows for each split until no merged row is left\n",
    "        \"\"\"\n",
    "        # if our list exceeds 1 in length, we continue to split\n",
    "        if len(values) > 1:\n",
    "            # construct a dataframe row of the first split term to append to sub list\n",
    "            row = pd.DataFrame([lineName[0], values[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we pass the +1 index splits and line name, appending the first-most layer \n",
    "            return recursive_splits(values[1:], lineName[1:], sub=sub)\n",
    "        else:\n",
    "            row = pd.DataFrame([lineName[0], values[0]]).T\n",
    "            sub.append(row)\n",
    "            \n",
    "            # we concatenate all DataFrames vertically to form a large DataFrame \n",
    "            return pd.concat(sub)\n",
    "        \n",
    "    # ##############################################################\n",
    "    # ##############################################################    \n",
    "    \n",
    "    # select all the rows that match our description, where a space exists = row merge \n",
    "    selections = df[df[df.columns[1]].apply(lambda x: find_row_splits(x))]\n",
    "    idxs = selections.index\n",
    "    \n",
    "    # iterate through each row that is determined to be conjoined\n",
    "    for i in idxs:\n",
    "        \n",
    "        # find the index location od merged row\n",
    "        row_idx = np.argmax(df.index == i)\n",
    "        \n",
    "        # slice dataframe according to the idx selection (we search for all periods were a break occurs)\n",
    "        top = df.iloc[:row_idx]\n",
    "        bottom = df.iloc[row_idx+1:]\n",
    "\n",
    "        # divide the identified term from the selection e.g. \"$ 9,112,943 13,151,663\" -> [\"$\", \"9,112,943\", \"13,151,663\"] \n",
    "        # and filter out the $ sign in the list e.g. [\"$\", \"9,112,943\", \"13,151,663\"] -> [9,112,943\", \"13,151,663\"]\n",
    "        values = df[df.columns[1]].loc[i].split(' ')\n",
    "        values = list(filter(dollar_check, values))\n",
    "        \n",
    "        # extract line names and corresponding values according to Text parsed list (requires parsed TEXT JSON)\n",
    "        # e.g. ['Securities Held Total Assets'] -> ['Securities Held', 'Total Assets']\n",
    "        lineName = df[df.columns[0]].loc[i]\n",
    "        \n",
    "        # return line items and values that should match in size\n",
    "        response_extraction = extract_lineitems(lineName, values, text_file)\n",
    "        \n",
    "        # if we retun a lineitem then we can perform recursive splits (otherwise avoid)\n",
    "        if type(response_extraction) is not type(None):\n",
    "            \n",
    "            clean_lineitems, clean_values = response_extraction\n",
    "            \n",
    "            # determine the splits for the corresponding row\n",
    "            mid = recursive_splits(clean_values, clean_lineitems, sub=[])\n",
    "            mid.columns = df.columns\n",
    "\n",
    "            # re-assign the value of df2 to update across each iteration\n",
    "            df = pd.concat([top, mid, bottom])\n",
    "            \n",
    "        else:\n",
    "            # no need for mid, since we have removed it from existence\n",
    "            df = pd.concat([top, bottom])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Conversion\n",
    "**Work on converting all string and poor formating quantities to numerical type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_scale(text_dict:dict, key_value:str, old_cik:int, old_scale:float) -> float:\n",
    "    \"\"\"\n",
    "    Function used for scaling accounting figures by reported unites\n",
    "    \"\"\"\n",
    "    scalar = {'thousands': 1e3, 'hundreds':1e2, 'millions':1e6, 'billions': 1e9}\n",
    "    \n",
    "    text_data = text_dict[key_value]\n",
    "    \n",
    "    # iterate through each of the text values from dictionary map\n",
    "    for text_value in text_data.keys():\n",
    "        \n",
    "        # we check to see whether the text is found in our scalar dictionary\n",
    "        for scale_type in scalar.keys():\n",
    "            \n",
    "            # search for the presence of the scale identifier (e.g. millions) \n",
    "            # we use a \"fuzzy-ratio\" on string splits to handle embeeded keyes (e.g. Dollar in Millions)\n",
    "            scale_search = [fuzz.ratio(scale_type.lower(), elm.lower()) for elm in text_value.lower().split(' ')]\n",
    "            \n",
    "            # we make the assumption that a score of 90 or greater, signals a match\n",
    "            if max(scale_search) >= 90:\n",
    "                print('Numeric scaler {}, for {}'.format(scale_type, text_value.lower()))\n",
    "                return scalar[scale_type]              # if found we simply return the multiplier\n",
    "    \n",
    "    if old_cik == key_value.split('-')[0]:\n",
    "        return old_scale\n",
    "    \n",
    "    # default to no multiplier (1)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNumeric(value):\n",
    "    \"\"\"\n",
    "    This function is a wrapper for calling the numerical extraction function \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param value:\n",
    "            String with hidden numeric quanity (e.g. $ 19,225 = 19255)  \n",
    "        :param text_file: (type dictionary)\n",
    "            Stores text values with corresponding confidence level for balance sheet pages\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A processed numeric quantiity or numpy.nan value depending on string issues  \n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(value) is str or int or np.ndarray, 'Value must be of type string, integer, float or numpy array'\n",
    "    \n",
    "    # checks to see what type of value is being provided\n",
    "    operator = type(value)\n",
    "    \n",
    "    # if provided a string, perform regex operation \n",
    "    if (operator is str) and (len(value) > 0):\n",
    "        return num_strip(value)\n",
    "    \n",
    "    # if operator is integer then simply return the value, no need to modify \n",
    "    elif (operator is int) or (operator is float):\n",
    "        return value \n",
    "    \n",
    "    # if operator is numpy array then we perform a extraction per element in array\n",
    "    elif (operator is np.ndarray):\n",
    "        vFunc = np.vectorize(num_strip)      # vectorize function to apply to numpy array\n",
    "        cleanValue = vFunc(value)            # apply vector function\n",
    "        return cleanValue \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J.P Morgan Handle\n",
    "**Dealing with the weird formating issue present within the J.P. Morgan X-17A-5 series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpm_check(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A wrapper function that reduces the amount of rows present within special J.P. Morgan\n",
    "    releases that contain a special sub-balance sheet for VIE figures\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param df:\n",
    "            Original unfiltered pandas.DataFrame object representing balance sheet figures\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A strucutred DataFrame that has been uniformly created from the raw Textract table\n",
    "    \"\"\"\n",
    "    arr = df[df.columns[0]]\n",
    "    \n",
    "    # iterate through each line item, looking for cut off naming convention\n",
    "    for idx, line_item in enumerate(arr):\n",
    "        \n",
    "        try:\n",
    "            # our key phrase is \"(a) The following table...\" found in J.P. Morgan filings with VIE\n",
    "            check1 = re.search('\\(a\\) The following table', line_item, flags=re.I)\n",
    "            check2 = re.search('\\(a\\) The follow', line_item, flags=re.I)\n",
    "            \n",
    "            if check1 is not None or check2 is not None:\n",
    "                # remove all the line below the condition being met\n",
    "                return df.iloc[:idx] \n",
    "            \n",
    "        # trying to perform regex on a NaN object (not-compatible)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Idiosyncratic Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idio_chg(df:pd.DataFrame, base_file:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Idiosyncratic changes for each Textract version we encounter \n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param base_file:\n",
    "            Base file for a particular broker-dealer recorded as CIK-YYYY-MM-DD\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A strucutred DataFrame that has been modified to handle speicfic Textract errors\n",
    "    \"\"\"\n",
    "    \n",
    "    if base_file == '356628-2006-03-02':\n",
    "        # Textract fails to read top line items “Cash“ and “Cash and resale agreements segregated under federal regulation“, resulting in underestimation of total asset value\n",
    "        temp_df = pd.DataFrame({'0':['Cash', 'Cash and resale agreements segregated under federal regulation'], \n",
    "                                '1':[32494000.0, 6813110000.0]})\n",
    "        \n",
    "        return pd.concat([temp_df, df])\n",
    "        \n",
    "    elif base_file == '318336-2018-03-01':\n",
    "        # Our backward total checking algorithm removes “Customers“ item incorrectly since it very closely matches the lookback sum of 3 previous line items\n",
    "        df = df.replace({13482000000.0 : 13482000111.0, 1030000000.0: 1030000111.0, 12876000000.0: 12876000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '318336-2005-03-01':\n",
    "        # Our backward total checking algorithm removes “Commercial paper“ item incorrectly since it very closely matches to the above item, “Derivatives contracts“\n",
    "        df = df.replace({1171000000.0 : 1171000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '87634-2020-02-27':\n",
    "        # Our backward total checking algorithm removes “Goodwill“ item incorrectly since it very closely matches to the above item, “Equipment, office facilities, and property - net“\n",
    "        df = df.replace({935000000.0 : 935000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '91154-2015-03-02':\n",
    "        # Our backward total checking algorithm removes “Brokers, dealers and clearing organizations“ item incorrectly since it very closely matches 'Customers'\n",
    "        df = df.replace({7584000000.0 : 7584000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '91154-2019-03-05':\n",
    "        # Our backward total checking algorithm removes “Securities received as collateral, at fair value (all pledged to counterparties)“ item incorrectly since it very closely matches the lookback sum of 5 previous line items\n",
    "        df = df.replace({15877000000.0 : 15877000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '89562-2006-01-30':\n",
    "        # Our backward total checking algorithm removes “Property. equipment and leasehold improvements“ item incorrectly since it very closely matches 'Others'\n",
    "        df = df.replace({163000000.0 : 163000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '808379-2015-03-02':\n",
    "        # Our backward total checking algorithm removes “Financial instruments owned, at fair value“ item incorrectly since it very closely matches the lookback sum of 4 previous line items\n",
    "        df = df.replace({15263000000.0 : 15263000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '356628-2008-02-29':\n",
    "        # Textract error, where the top of the table (i.e. Cash line) is not read leading to an undercount of the “Total Asset” figure by that amount.\n",
    "        temp_df = pd.DataFrame({'0': ['Cash'], '1':[103017000]})\n",
    "        \n",
    "        return pd.concat([temp_df, df])\n",
    "    \n",
    "    elif base_file == '895502-2009-12-30':\n",
    "        # Textract fails to read top line items “Cash“ resulting in underestimation of total asset value\n",
    "        temp_df = pd.DataFrame({'0': ['Cash'], '1':[358998000]})\n",
    "        \n",
    "        return pd.concat([temp_df, df])\n",
    "    \n",
    "    elif base_file == '29648-2010-03-01':\n",
    "        # Our backward total checking algorithm removes “Accumulated earnings“ item incorrectly since it very closely matches the lookback sum of 4 previous line items\n",
    "        df = df.replace({1030000000.0 : 1030000111.0})\n",
    "        return df\n",
    "\n",
    "    elif base_file == '42352-2015-03-10':\n",
    "        # Textract error, which understates the value of the “Securities loaned” returning a value for 4.151000e+10 instead of 8.151000e+10\n",
    "        df = df.replace({4.151000e+10 : 8.151000e+10})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '42352-2017-03-01':\n",
    "        # Textract error, which understates the value of the “Securities loaned” returning a value for 4.151000e+10 instead of 8.151000e+10\n",
    "        df = df.replace({4.340500e+10 : 4.340600e+10})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '72267-2012-03-15':\n",
    "        # Due to poor Textract reading we overlap parts of the liabilities values with some of the asset rows, grossly overestimating the totals\n",
    "        df = df.drop([11])\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '87634-2010-03-01':\n",
    "        # Our backward total checking algorithm removes “Retained earnings“ item incorrectly since it very closely matches “Additional paid-in capital“\n",
    "        df = df.replace({1079000000.0 : 1079000111.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '72267-2014-05-30':\n",
    "        df = pd.concat([df.iloc[:12], df.iloc[14:]])  # remove a read-mistep with the \"other category\"\n",
    "        \n",
    "        # Due to poor Textract reading on the PNG file, we omit a singular row which complicates our balance sheet script\n",
    "        if df[df[1] == 8.105411e+10].empty:\n",
    "            temp_df1 = pd.DataFrame({0: ['Securities sold under agreements to repurchase'], \n",
    "                                     1:[8.105411e+10]})\n",
    "            return pd.concat([temp_df1, df])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    elif base_file == '1146184-2021-02-25':\n",
    "        # Issues with Textract grossly omitting many rows from the balance sheet table\n",
    "        temp_df1 = pd.DataFrame({'0':['Cash', 'Securities owned, at fair value', 'Securities borrowed', \n",
    "                                    'Receivable from brokers and dealers', 'Receivable from clearing organizations and custodian',\n",
    "                                    'Securities purchased under agreements to resell'], \n",
    "                                '1':[523000000, 66707000000, 1628000000, 841000000, 648000000, 492000000]})\n",
    "        temp_df2 = pd.DataFrame({'0': ['Total Assets'], '1':[71004000000]})\n",
    "        \n",
    "        return pd.concat([temp_df1, df.iloc[:1], temp_df2, df.iloc[1:]])\n",
    "    \n",
    "    elif base_file == '91154-2009-03-02': \n",
    "        # Our backward total checking algorithm removes “Other financial instruments“ item incorrectly since it very closely matches “Foreign government securities“ \n",
    "        df = df.replace({125000000.0 : 125000111.0, 2.058200e+10: np.nan})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '91154-2019-03-05':\n",
    "        # Due to poor Textract reading on the PNG file, we omit a singular row which complicates our balance sheet script\n",
    "        temp_df1 = pd.DataFrame({'0': ['Short-term borrowing'], \n",
    "                                     '1':[508000000]})\n",
    "        return pd.concat([temp_df1, df])\n",
    "    \n",
    "    elif base_file == '808379-2007-03-01':\n",
    "        # Due to Textract reading, we have double counted the total asset line item, we remove this to avoid complications in the liability and equity table\n",
    "        df = df.drop([8])\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '895502-2002-02-28':\n",
    "        # Due to Textract reading, we have double counted the total asset line item, we remove this to avoid complications in the liability and equity table\n",
    "        df = df.replace({2.357964e+09: np.nan})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '895502-2012-12-28' or base_file == '895502-2014-01-02':\n",
    "        # Our backward total checking algorithm removes “Liabilities subordinated“ item incorrectly since it very closely matches “Long-term borrowing“ \n",
    "        df = df.replace({1400000000.0 : 1400000111.0, 167769234000.0: 67769234000.0})\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '867626-2013-02-28':\n",
    "        # our numeric scaler scales by the wrong value using 1e6 as opposed to 1e3, we scale back everything\n",
    "        df[df.columns[1]] = df[df.columns[1]].apply(lambda x: x / 1e3)\n",
    "        return df\n",
    "    \n",
    "    elif base_file == '890203-2020-03-02':\n",
    "        # out numeric scaler can't find the correct value to scale the balance sheet by, we scale manually\n",
    "        df[df.columns[1]] = df[df.columns[1]].apply(lambda x: x * 1e3)\n",
    "        return df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wrapper(df: pd.DataFrame, textract_text: dict, key: str, file: str, \n",
    "                  old_scaler: str, old_cik: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A wrapper function that sequentially calls each cleaning function to fix issues that may arise\n",
    "    post Textract reading (i.e. Column Merging, Row Splitting, Numeric Conversion)\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param df:\n",
    "            Original unfiltered pandas.DataFrame object representing balance sheet figures\n",
    "            \n",
    "    Output:\n",
    "        :param return:\n",
    "            A strucutred DataFrame that has been uniformly created from the raw Textract table\n",
    "    \"\"\"\n",
    "    \n",
    "    # re-assign dataframe of balance sheet after cleanse\n",
    "    df = column_purge(df)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # J.P. Morgan Special Case Handle\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # performs a check to remove uncessary rows for specific J.P. Morgan releases\n",
    "    df = jpm_check(df)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # COLUMN MERGING (IF NECESSARY)\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # if columns greater than 2, we have a weird data table that needs to be \"merged\"\n",
    "    # NOTE: By construction we never have more than 3 columns present, thanks to our Textract check \n",
    "    if df.columns.size > 2:\n",
    "        df = merge(df)\n",
    "        print('\\tWe merged the columns of {}'.format(file))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # ROW SPLIT FOR MERGED ROWS (IF NECESSARY)\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # check for presence of row splits and correct any if found \n",
    "    tempDF = row_split(df, textract_text[key])\n",
    "\n",
    "    # if difference is found in shape, then a transformation was done \n",
    "    if tempDF.shape != df.shape:\n",
    "        print(\"\\tFixed the merged rows for {}\".format(file))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # NUMERIC CONVERSION\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # pass numeric converter to the column to convert string to numerics\n",
    "    tempDF[tempDF.columns[1]] = tempDF[tempDF.columns[1]].apply(cleanNumeric)\n",
    "\n",
    "    # remove any NaN rows post numeric-conversion\n",
    "    postDF = tempDF.dropna().copy()\n",
    "\n",
    "    # check for potential scaler multipler on cash flows (adjust multiplier if possible)\n",
    "    scale = num_scale(textract_text, key, old_cik, old_scaler)\n",
    "    postDF[postDF.columns[1]] = postDF[postDF.columns[1]].apply(lambda x: x * scale)\n",
    "\n",
    "    print('\\tWe converted to numeric figures for {}'.format(file))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # Idiosyncratic changes (specific balance sheet)\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # performs modification to handle Textract specific errors\n",
    "    out_df = idio_chg(postDF, key).dropna()\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # BALANCE SHEET EXPORTATION\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    print(out_df)\n",
    "    return out_df, scale, key.split('-')[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning the 890203-2020-03-02.csv file\n",
      "890203-2020-03-02 890203-2020-03-02.csv\n",
      "\tWe converted to numeric figures for 890203-2020-03-02.csv\n",
      "                                                    0             1\n",
      "0                                                Cash  2.203800e+07\n",
      "1   Cash segregated in compliance with federal and...  3.146900e+07\n",
      "2                           Receivable from affiliate  4.000000e+03\n",
      "3   Receivables from brokers or dealers and cleari...  1.671010e+08\n",
      "4                          Receivables from customers  1.782000e+06\n",
      "5                     Receivables from counterparties  5.135400e+07\n",
      "6                     Receivables from correspondents  2.071100e+07\n",
      "7    Securities borrowed (including accrued interest)  3.417971e+09\n",
      "8    Securities received as collateral, at fair value  9.924300e+08\n",
      "9   Securities purchased under agreement to resell...  4.696491e+10\n",
      "10  Securities owned, at fair value (including acc...  1.496500e+08\n",
      "11                                 Right of Use Asset  7.360000e+06\n",
      "12                                   Prepaid expenses  8.100000e+05\n",
      "13                                       Other assets  1.812000e+06\n",
      "14  Furniture, equipment and leasehold improvement...  8.650000e+05\n",
      "15                                       Total assets  5.183027e+10\n",
      "17                      Accrued and other liabilities  5.080000e+06\n",
      "18                                    Lease Liability  8.265000e+06\n",
      "19                                      Taxes Payable  5.000000e+04\n",
      "20                               Payable to customers  1.215760e+08\n",
      "21                          Payable to counterparties  1.660640e+08\n",
      "22                          Payable to correspondents  3.479000e+06\n",
      "23     Securities loaned (including accrued interest)  2.992976e+09\n",
      "24  Obligation to return securities received as co...  9.924300e+08\n",
      "25  Securities sold under agreement to repurchase,...  4.729738e+10\n",
      "26  Payables to brokers or dealers and clearing or...  1.092500e+07\n",
      "27                                  Total liabilities  5.159823e+10\n",
      "30                          outstanding 52,000 shares  1.000000e+03\n",
      "31                         Additional paid-in capital  2.619990e+08\n",
      "32                                Accumulated deficit -2.995800e+07\n",
      "33                         Total stockholder's equity  2.320420e+08\n",
      "34         Total liabilities and stockholder's equity  5.183027e+10\n",
      "\n",
      "Cleaning the 890203-2020-03-02.csv file\n",
      "\tWe converted to numeric figures for 890203-2020-03-02.csv\n",
      "                                                    0             1\n",
      "0                                                Cash  2.203800e+07\n",
      "1   Cash segregated in compliance with federal and...  3.146900e+07\n",
      "2                           Receivable from affiliate  4.000000e+03\n",
      "3   Receivables from brokers or dealers and cleari...  1.671010e+08\n",
      "4                          Receivables from customers  1.782000e+06\n",
      "5                     Receivables from counterparties  5.135400e+07\n",
      "6                     Receivables from correspondents  2.071100e+07\n",
      "7    Securities borrowed (including accrued interest)  3.417971e+09\n",
      "8    Securities received as collateral, at fair value  9.924300e+08\n",
      "9   Securities purchased under agreement to resell...  4.696491e+10\n",
      "10  Securities owned, at fair value (including acc...  1.496500e+08\n",
      "11                                 Right of Use Asset  7.360000e+06\n",
      "12                                   Prepaid expenses  8.100000e+05\n",
      "13                                       Other assets  1.812000e+06\n",
      "14  Furniture, equipment and leasehold improvement...  8.650000e+05\n",
      "15                                       Total assets  5.183027e+10\n",
      "17                      Accrued and other liabilities  5.080000e+06\n",
      "18                                    Lease Liability  8.265000e+06\n",
      "19                                      Taxes Payable  5.000000e+04\n",
      "20                               Payable to customers  1.215760e+08\n",
      "21                          Payable to counterparties  1.660640e+08\n",
      "22                          Payable to correspondents  3.479000e+06\n",
      "23     Securities loaned (including accrued interest)  2.992976e+09\n",
      "24  Obligation to return securities received as co...  9.924300e+08\n",
      "25  Securities sold under agreement to repurchase,...  4.729738e+10\n",
      "26  Payables to brokers or dealers and clearing or...  1.092500e+07\n",
      "27                                  Total liabilities  5.159823e+10\n",
      "30                          outstanding 52,000 shares  1.000000e+03\n",
      "31                         Additional paid-in capital  2.619990e+08\n",
      "32                                Accumulated deficit -2.995800e+07\n",
      "33                         Total stockholder's equity  2.320420e+08\n",
      "34         Total liabilities and stockholder's equity  5.183027e+10\n",
      "\n",
      "\n",
      "\n",
      "All .csv files are cleaned and primed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initiate s3 bucket and corresponding data folder\n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    \n",
    "    pdf_data_folder = 'Output/X-17A-5-PDF-RAW/'\n",
    "    png_data_folder = 'Output/X-17A-5-PNG-RAW/'\n",
    "    \n",
    "    pdf_output_folder = 'Output/X-17A-5-CLEAN-PDFS/'\n",
    "    png_output_folder = 'Output/X-17A-5-CLEAN-PNGS/'\n",
    "\n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # retrieving text JSON file from s3 bucket and store to temp \n",
    "    s3.download_file(bucket, 'Temp/X17A5-TEXT.json', 'temp2.json')\n",
    "\n",
    "    # read data on TEXT-Confidence dictionary\n",
    "    with open('temp2.json', 'r') as f: text_dictionary = json.loads(f.read())  \n",
    "\n",
    "    # remove local files for JSON\n",
    "    os.remove('temp2.json')\n",
    "    \n",
    "    # csv directory with all X-17A-5 balance sheet information \n",
    "    pdf_paths = np.array(session.list_s3_files(bucket, pdf_data_folder))[1:]\n",
    "    png_paths = np.array(session.list_s3_files(bucket, png_data_folder))[1:]\n",
    "    \n",
    "    # ==============================================================================================\n",
    "    \n",
    "    # trailing scaler for firms who forgot to add \"in amounts\"\n",
    "    prior_scaler = 1.0\n",
    "    prior_cik = np.nan\n",
    "    \n",
    "    # iterate through each csv path e.g. ['Output/X-17A-5-PDF-RAW/72267-2012-03-15.csv'] vs pdf_paths\n",
    "    for pdf_csv in ['Output/X-17A-5-PDF-RAW/890203-2020-03-02.csv']:\n",
    "        \n",
    "        fileName = pdf_csv.split('/')[-1]                  # strip filename from each csv\n",
    "        base_file = fileName.split('.')[0]                 # CIK-YYYY-MM-DD base name\n",
    "        print('\\nCleaning the {} file'.format(fileName))\n",
    "        \n",
    "        # download X-17A-5 csv file as a temporary csv file  \n",
    "        s3.download_file(bucket, pdf_csv, 'temp.csv')\n",
    "        df = pd.read_csv('temp.csv')\n",
    "        \n",
    "        # perform cleaning operations on read dataframe\n",
    "        out_df, prior_scaler, prior_cik = clean_wrapper(df, text_dictionary, base_file, fileName, \n",
    "                                                        prior_scaler, prior_cik)\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        out_df.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=pdf_output_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been used\n",
    "        os.remove(fileName)\n",
    "        os.remove('temp.csv')\n",
    "    \n",
    "    # ==============================================================================================\n",
    "    \n",
    "    # trailing scaler for firms who forgot to add \"in amounts\"\n",
    "    prior_scaler = 1.0\n",
    "    prior_cik = np.nan\n",
    "    \n",
    "    # iterate through each csv path e.g. ['Output/X-17A-5-PNG-RAW/58056-2014-03-04.csv'] vs png_paths\n",
    "    for png_csv in ['Output/X-17A-5-PNG-RAW/890203-2020-03-02.csv']:\n",
    "        \n",
    "        fileName = png_csv.split('/')[-1]                  # strip filename from each csv\n",
    "        base_file = fileName.split('.')[0]                 # CIK-YYYY-MM-DD base name\n",
    "        print('\\nCleaning the {} file'.format(fileName))\n",
    "        \n",
    "        # download X-17A-5 csv file as a temporary csv file  \n",
    "        s3.download_file(bucket, png_csv, 'temp.csv')\n",
    "        df = pd.read_csv('temp.csv')\n",
    "        \n",
    "        # perform cleaning operations on read dataframe\n",
    "        out_df, prior_scaler, prior_cik = clean_wrapper(df, text_dictionary, base_file, fileName, \n",
    "                                                        prior_scaler, prior_cik)\n",
    "        \n",
    "        # writing data frame to .csv file\n",
    "        out_df.to_csv(fileName, index=False)\n",
    "\n",
    "        # save contents to AWS S3 bucket\n",
    "        with open(fileName, 'rb') as data:\n",
    "            s3.put_object(Bucket=bucket, Key=png_output_folder + fileName, Body=data)\n",
    "\n",
    "        # remove local file after it has been used\n",
    "        os.remove(fileName)\n",
    "        os.remove('temp.csv')\n",
    "        \n",
    "    print('\\n\\n\\nAll .csv files are cleaned and primed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = set(map(lambda x: x.split('/')[-1].split('.')[0], pdf_paths))\n",
    "# b = set(text_dictionary.keys())\n",
    "# np.array(list(map(lambda x: 'Input/X-17A-5-PDF-SUBSETS/{}-subset.pdf'.format(x), a - b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open a single corresponding csv\n",
    "# s3 = boto3.client('s3')\n",
    "# session = Session()\n",
    "\n",
    "# s3.download_file('ran-s3-systemic-risk', 'Output/X-17A-5-PNG-RAW/754542-2003-03-03.csv', 'temp.csv')\n",
    "# df = pd.read_csv('temp.csv')\n",
    "# os.remove('temp.csv')\n",
    "\n",
    "# # retrieving text JSON file from s3 bucket and store to temp \n",
    "# s3.download_file('ran-s3-systemic-risk', 'Temp/X17A5-TEXT.json', 'temp2.json')\n",
    "# with open('temp2.json', 'r') as f: text_dictionary = json.loads(f.read())  \n",
    "# os.remove('temp2.json')\n",
    "\n",
    "# prior_scaler = 1.0\n",
    "# prior_cik = np.nan\n",
    "\n",
    "# a, b, c = clean_wrapper(df, text_dictionary, '754542-2003-03-03', '754542-2003-03-03.csv', prior_scaler, prior_cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
