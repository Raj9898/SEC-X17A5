{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_data(unstructured_df:pd.DataFrame, cluster_df:pd.DataFrame, col_preserve:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a structured dataset from an unstructured column set\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Input:\n",
    "        :param: unstructured_df (type pandas.DataFrame)\n",
    "            unstuructured pandas dataframe with loose column construction \n",
    "        :param: cluster_df (type pandas.DataFrame)\n",
    "            a pandas dataframe of clustered labels and corresponding line items\n",
    "    Output:\n",
    "        :return: (type pandas DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_df = pd.DataFrame()\n",
    "    label_names = np.unique(cluster_df.Labels.values)\n",
    "    remap = {}\n",
    "    \n",
    "    # assume that the there exists columns 'CIK' and 'Year' for unstructured data\n",
    "    structured_df = unstructured_df[col_preserve]\n",
    "    \n",
    "    for label in label_names:\n",
    "        data = cluster_df[cluster_df['Labels'] == label]['Lineitems']     # filter by corresponding cluster\n",
    "        \n",
    "        # we first select all predicted columns, then sum across rows for only numeric figures\n",
    "        selection = unstructured_df[data.values]\n",
    "        \n",
    "        sumV = selection.sum(axis=1, numeric_only=True)\n",
    "        \n",
    "        # we then select rows from the original unstructured dataframe with \n",
    "        # only np.nan and convert sumV index to np.nan\n",
    "        sumV[selection.isnull().all(axis=1)] = np.nan\n",
    "        \n",
    "        # assign dictionary to have labels and matching vector\n",
    "        remap[label] = sumV\n",
    "\n",
    "    structured_df = structured_df.assign(**remap)   \n",
    "    return structured_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probabilites(line_items:np.array, clf_mdl, vec_mdl) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a mapping convention for the machine learning predictions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    line_items : pandas.DataFrame\n",
    "        list of all unstructured line item names\n",
    "    \n",
    "    clf_mdl : pandas.DataFrame\n",
    "        a classification model to convert a line item \n",
    "        \n",
    "    vec_mdl : list\n",
    "        a feature extraction model for string/text data \n",
    "    \"\"\"\n",
    "    \n",
    "    # predict the corresponding class for each line item\n",
    "    prediction = pd.DataFrame(data=clf_mdl.predict(vec_mdl.fit_transform(line_items)), \n",
    "                              columns=['Manual Classification'])    # actually a predicted class, but this naming\n",
    "                                                                    # convention helps in concatination\n",
    "    \n",
    "    # the actual line items that are used as predictors\n",
    "    lines = pd.DataFrame(line_items, columns=['Lineitems'])\n",
    "    \n",
    "    # compute the probability for each prediction to the accompanying classes\n",
    "    prediction_probability = pd.DataFrame(data=clf_mdl.predict_proba(vec_mdl.fit_transform(line_items)),\n",
    "                                          columns=clf_mdl.classes_)\n",
    "    \n",
    "    # compute the maximum value across prediction probabilites\n",
    "    prediction_probability['Max Prediction score'] = prediction_probability.max(axis=1) \n",
    "    \n",
    "    # sum across row, determines total class probability measure \n",
    "    # NOTE: each class is distributed 0.0-1.0, so total row wise sums equal 1\n",
    "    prediction_probability['Total Prediction score'] = prediction_probability[prediction_probability.columns[:-1]].sum(axis=1)\n",
    "    \n",
    "    # join the line items to the prediction probabilities\n",
    "    return lines.join(prediction).join(prediction_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_pdf(df:pd.DataFrame, mdl):\n",
    "    \"\"\"\n",
    "    Return a dataframe for a company showcasing its column names, the predicted class and the original values.\n",
    "    This function is used for error handling and de-bugging as it returns (Lineitems, Predictions, Linevalues) \n",
    "    \"\"\"\n",
    "    # split values for company dataframe according to columns and values\n",
    "    colNames = df.index\n",
    "    colValues = df.values\n",
    "    \n",
    "    # predicting the column groups with accompanying sklearn model\n",
    "    # NOTE: We pre-process with a HashingVectorizer with 1000 features, this action is very model specific\n",
    "    predNames = mdl.predict(HashingVectorizer(strip_accents='unicode', \n",
    "                                              lowercase=True, analyzer='word',\n",
    "                                              n_features=1000, norm='l2').fit_transform(colNames))\n",
    "    print(predNames.size)\n",
    "    print(colNames.size)\n",
    "    print(colValues.size)\n",
    "    retDF = pd.DataFrame({'Original Lineitems': colNames,                       # the original line items\n",
    "                          'Predicted Lineitems': predNames,                     # the predicted line items\n",
    "                          'Line values': colValues.flatten().tolist()})         # the corresponding line values\n",
    "    \n",
    "    return retDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_indicator(pct):\n",
    "    \"\"\"\n",
    "    Determines the level of matching accuracy for a particular firm/year\n",
    "    \"\"\"\n",
    "    def indicator(x):\n",
    "        \n",
    "        if type(x) is float:\n",
    "            y = x\n",
    "        else:\n",
    "            y = min(x)     # from an array determine the minimum relative error\n",
    "        \n",
    "        if y == 0: return 'PERFECT MATCH'\n",
    "        if 0 < y < 0.01: return 'BOUNDED MATCH'\n",
    "        if y >= 0.01: return 'GROSS MISMATCH'\n",
    "        if np.isnan(y): return 'NOT FOUND'\n",
    "    \n",
    "    vFunc = np.vectorize(indicator)      # vectorize function to apply to numpy array\n",
    "    cleanValue = indicator(pct)            # apply vector function\n",
    "    \n",
    "    return cleanValue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_finder(pct):\n",
    "    \"\"\"\n",
    "    Determines the level of matching accuracy for a particular firm/year\n",
    "    \"\"\"\n",
    "    def min_find(x):\n",
    "        return min(x)\n",
    "    \n",
    "    vFunc = np.vectorize(min_find)      # vectorize function to apply to numpy array\n",
    "    cleanValue = min_find(pct)            # apply vector function\n",
    "    \n",
    "    return cleanValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_cl_merge(prediction_df:pd.DataFrame, ttraing_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a merge to overwrite poor model classifications \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_df : pandas.DataFrame\n",
    "        a pandas dataframe highlighting the prediction for a set of line items\n",
    "        according to classification model specifications\n",
    "    \n",
    "    ttraing_df : pandas.DataFrame\n",
    "        a pandas dataframe of manually classified line items used for\n",
    "        testing/training the classification model\n",
    "    \"\"\"\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None  # default='warn' - we ignore for the remapping\n",
    "    \n",
    "    # dictionary mapping lineitems -> classification label\n",
    "    remapping = dict(ttraing_df.values)\n",
    "    \n",
    "    # divide the prediction dataframe into rows that match and don't match the training-testing set\n",
    "    top_half = prediction_df[np.isin(prediction_df.Lineitems, ttraing_df.Lineitems)]\n",
    "    bot_half = prediction_df[~np.isin(prediction_df.Lineitems, ttraing_df.Lineitems)]\n",
    "    \n",
    "    # replace all predicted labels in the top-half with manual classifications\n",
    "    top_half['Labels'] = top_half['Lineitems'].replace(remapping)\n",
    "    \n",
    "    return pd.concat([top_half, bot_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_wrapper(asset_df, liable_df, asset_training, liable_training, hashing_model, \n",
    "                       asset_model, liable_model) -> tuple:\n",
    "    \"\"\"\n",
    "    Re-order the completed DataFrame by ordering the CIK, Name, \n",
    "    Filing Data and Filing Year. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    asset_df : pandas.DataFrame\n",
    "        The asset side balance sheet for a broker-dealer derivied from \n",
    "        PDFs/PNGs\n",
    "        \n",
    "    liable_df : pandas.DataFrame\n",
    "        The liability & equity side balance sheet for a broker-dealer \n",
    "        derivied from PDFs/PNGs\n",
    "        \n",
    "    asset_training : pandas.DataFrame\n",
    "        The classification training set for asset line items \n",
    "        \n",
    "    liable_training : pandas.DataFrame\n",
    "        The classification training set for liability & equity line items \n",
    "        \n",
    "    hashing_model : sklearn.HashingVectorizer\n",
    "        A HashingVectorizer model for converting text/string to numerics\n",
    "        \n",
    "    asset_model : joblib\n",
    "        A log-regression model for predicting asset class items\n",
    "        \n",
    "    liable_model : joblib\n",
    "        A log-regression model for predicting liability & equity class items\n",
    "    \"\"\"\n",
    "    \n",
    "    # the non-prediction columns are stationary (we don't predict anything)\n",
    "    non_prediction_columns = ['CIK', 'Name', 'Filing Date', 'Filing Year']\n",
    "    \n",
    "    # select columns that do not belong to the non-prediction columns list\n",
    "    a_columns = asset_df.columns[~np.isin(asset_df.columns, non_prediction_columns)]\n",
    "    l_columns = liable_df.columns[~np.isin(liable_df.columns, non_prediction_columns)]\n",
    "    \n",
    "    # Use classification model to predict label names for each line item\n",
    "    asset_label_predictions = asset_model.predict(hashing_model.fit_transform(a_columns))\n",
    "    liable_label_predictions = liable_model.predict(hashing_model.fit_transform(l_columns))\n",
    "    \n",
    "    # structured database for asset and liability terms \n",
    "    struct_asset_map = pd.DataFrame([a_columns, asset_label_predictions], \n",
    "                                    index=['Lineitems', 'Labels']).T\n",
    "\n",
    "    struct_liable_map = pd.DataFrame([l_columns, liable_label_predictions], \n",
    "                                     index=['Lineitems', 'Labels']).T\n",
    "    \n",
    "    # assigning variables in accordance with manual classification sets\n",
    "    struct_asset_map = manual_cl_merge(struct_asset_map, asset_training)\n",
    "    struct_liable_map = manual_cl_merge(struct_liable_map, liable_training)\n",
    "    \n",
    "    # construct the line-item prediction classes with corresponding probabilites \n",
    "    a_proba_df = prediction_probabilites(a_columns, asset_model, hashing_model)\n",
    "    l_proba_df = prediction_probabilites(l_columns, liable_model, hashing_model)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # structured database for asset terms \n",
    "    struct_asset_df = structured_data(asset_df, struct_asset_map, non_prediction_columns)\n",
    "    \n",
    "    # we drop ammended releases, preserving unique CIKs with Filing Year (default to first instance)\n",
    "    struct_asset_df = struct_asset_df.drop_duplicates(subset=['CIK', 'Filing Year'], keep='first')\n",
    "    \n",
    "    # extract all line items to reconstruct the appropriate total categories and compute relative differences\n",
    "    asset_lines = struct_asset_df.columns[~np.isin(struct_asset_df.columns,\n",
    "                                                   ['CIK', 'Name', 'Filing Date', 'Filing Year',  'Total assets'])]\n",
    "    struct_asset_df['Reconstructed Total assets'] = struct_asset_df[asset_lines].sum(axis=1)\n",
    "    \n",
    "    # construct absolute relative error, differencing returned Total assets from our reconstructed values\n",
    "    struct_asset_df['Relative Error'] = abs(struct_asset_df['Reconstructed Total assets'] - struct_asset_df['Total assets']) / struct_asset_df['Total assets']\n",
    "\n",
    "    struct_asset_df['Total asset check'] = struct_asset_df['Relative Error'].apply(relative_indicator)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # structured database for liability terms \n",
    "    struct_liable_df = structured_data(liable_df, struct_liable_map, non_prediction_columns)\n",
    "    struct_liable_df = struct_liable_df.drop_duplicates(subset=['CIK', 'Filing Year'], keep='first')\n",
    "    \n",
    "    # extract all line items to reconstruct the appropriate total categories and compute relative differences\n",
    "    liable_lines = struct_liable_df.columns[~np.isin(struct_liable_df.columns, \n",
    "                                            ['CIK', 'Name', 'Filing Date', 'Filing Year',  \n",
    "                                             \"Total liabilities and shareholder's equity\"])]\n",
    "    \n",
    "    # we remove all other premature totals from the reconsturctured\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity\"] = struct_liable_df[liable_lines].sum(axis=1) \n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total liabilites)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df['Total liabilities'].fillna(0)\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total equity)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df[\"Total shareholder's equity\"].fillna(0)\n",
    "    struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total L+E)\"] = struct_liable_df[liable_lines].sum(axis=1) - struct_liable_df['Total liabilities'].fillna(0) - struct_liable_df[\"Total shareholder's equity\"].fillna(0)\n",
    "    \n",
    "    # constructing measures of relative erorrs against each different reconstruction frameworks\n",
    "    struct_liable_df['Relative Error1'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "          \n",
    "    struct_liable_df['Relative Error2'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total liabilites)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "          \n",
    "    struct_liable_df['Relative Error3'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total equity)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "          \n",
    "    struct_liable_df['Relative Error4'] = abs(struct_liable_df[\"Reconstructed Total liabilities and shareholder's equity (less total L+E)\"] - struct_liable_df[\"Total liabilities and shareholder's equity\"]) / struct_liable_df[\"Total liabilities and shareholder's equity\"]\n",
    "\n",
    "    struct_liable_df[\"Total liabilities & shareholder's equity check\"] = struct_liable_df[['Relative Error1', 'Relative Error2', 'Relative Error3', 'Relative Error4']].apply(relative_indicator, axis=1)\n",
    "    struct_liable_df[\"Relative Error\"] = struct_liable_df[['Relative Error1', 'Relative Error2', 'Relative Error3', 'Relative Error4']].apply(relative_finder, axis=1)\n",
    "    \n",
    "    # export all neccessary dataframes constructed\n",
    "    return struct_asset_map, struct_liable_map, a_proba_df, l_proba_df, struct_asset_df, struct_liable_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final structured dataframe has been created.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Amazon Textract client and Sagemaker session\n",
    "    s3 = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    bucket = 'ran-s3-systemic-risk'\n",
    "    out_folder = 'Output/'\n",
    "    \n",
    "    # retrieving the old training-test sets for classification model\n",
    "    s3.download_file(bucket, 'Input/asset_lineitem_training_testing.csv', 'temp.csv')\n",
    "    old_asset_training = pd.read_csv('temp.csv')[['Lineitems', 'Manual Classification']]\n",
    "    s3.download_file(bucket, 'Input/liable_lineitem_training_testing.csv', 'temp.csv')\n",
    "    old_liable_training = pd.read_csv('temp.csv')[['Lineitems', 'Manual Classification']]\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # retrieving the unstructured asset values file from s3 bucket\n",
    "    s3.download_file(bucket, out_folder + 'unstructured_assets.csv', \n",
    "                             'unstructAsset.csv')\n",
    "    s3.download_file(bucket, out_folder + 'unstructured_liable.csv', \n",
    "                             'unstructLiable.csv')\n",
    "    assetDF = pd.read_csv('unstructAsset.csv')\n",
    "    liableDF = pd.read_csv('unstructLiable.csv')\n",
    "    os.remove('unstructAsset.csv')\n",
    "    os.remove('unstructLiable.csv')      \n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # retrieving the asset and liability classification models from s3 bucket\n",
    "    s3.download_file(bucket, 'Input/asset_log_reg_mdl_v2.joblib', 'asset_mdl.joblib')\n",
    "    s3.download_file(bucket, 'Input/liable_log_reg_mdl_v2.joblib', 'liable_mdl.joblib')\n",
    "    assetMDL = load('asset_mdl.joblib')\n",
    "    liableMDL = load('liable_mdl.joblib')\n",
    "    os.remove('asset_mdl.joblib')\n",
    "    os.remove('liable_mdl.joblib')      \n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # text vectorizer to format line items to be accepted in the model \n",
    "    str_mdl = HashingVectorizer(strip_accents='unicode', lowercase=True, analyzer='word', \n",
    "                                n_features=1000, norm='l2')\n",
    "    \n",
    "    # construct the asset/liability mapping alongside prediction probabilites (unpack tuple for data figures)\n",
    "    data_response = structured_wrapper(assetDF, liableDF, old_asset_training, old_liable_training, \n",
    "                                       str_mdl, assetMDL, liableMDL)\n",
    "    struct_asset_map, struct_liable_map, a_proba_df, l_proba_df, struct_asset_df, struct_liable_df = data_response\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Auxillary Database Files \n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    # concat the old and new asset training sets, where new predictions are greater than or equal to 85%    \n",
    "    add_training = a_proba_df[a_proba_df['Max Prediction score'] >= 0.85][['Lineitems', 'Manual Classification']]\n",
    "    joint_training = pd.concat([old_asset_training, \n",
    "                                add_training]).drop_duplicates(subset=['Lineitems'], \n",
    "                                                               keep='first')\n",
    "    \n",
    "    joint_training.to_csv('temp.csv', index=False)\n",
    "    with open('temp.csv', 'rb') as data: s3_pointer.put_object(Bucket=s3_bucket, Key=asset_ttset, Body=data)\n",
    "    \n",
    "    # concat the old and new liability training sets, where new predictions are greater than or equal to 85%    \n",
    "    add_training = l_proba_df[l_proba_df['Max Prediction score'] >= 0.85][['Lineitems', 'Manual Classification']]\n",
    "    joint_training = pd.concat([old_liable_training, \n",
    "                                add_training]).drop_duplicates(subset=['Lineitems'], \n",
    "                                                               keep='first')\n",
    "    \n",
    "    joint_training.to_csv('temp.csv', index=False)\n",
    "    with open('temp.csv', 'rb') as data: s3_pointer.put_object(Bucket=s3_bucket, Key=liable_ttset, Body=data)\n",
    "    \n",
    "    os.remove('temp.csv')\n",
    "    \n",
    "    filename = 'asset_name_map.csv'\n",
    "    struct_asset_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3_pointer.put_object(Bucket=s3_bucket, Key=out_folder + 'asset_name_map.csv', Body=data)\n",
    "    os.remove(filename)\n",
    "    \n",
    "    filename = 'liability_name_map.csv'\n",
    "    struct_liable_map.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3_pointer.put_object(Bucket=s3_bucket, Key=out_folder + 'liability_name_map.csv', Body=data)\n",
    "    os.remove(filename)\n",
    "          \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Database Exportation \n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    filename = 'structured_asset.csv'\n",
    "    struct_asset_df.to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3_pointer.put_object(Bucket=s3_bucket, Key=out_folder + 'structured_asset.csv', Body=data)\n",
    "    os.remove(filename)\n",
    "          \n",
    "    filename = 'structured_liability.csv'\n",
    "    struct_liable_df[struct_liable_df.columns[~np.isin(struct_liable_df.columns, \n",
    "                                                       ['Relative Error1', 'Relative Error2', \n",
    "                                                        'Relative Error3', 'Relative Error4'])]].to_csv(filename, index=False)\n",
    "    with open(filename, 'rb') as data:\n",
    "        s3_pointer.put_object(Bucket=s3_bucket, Key=out_folder + 'structured_liability.csv', Body=data)\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
